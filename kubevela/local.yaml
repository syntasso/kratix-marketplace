---
apiVersion: v1
kind: Namespace
metadata:
  name: vela-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core
  namespace: vela-system
---
apiVersion: v1
data:
  registries: '{ "KubeVela":{ "name": "KubeVela", "helm": { "url": "https://addons.kubevela.net"
    } } }'
kind: ConfigMap
metadata:
  name: vela-addon-registry
  namespace: vela-system
---
apiVersion: v1
data:
  config.yaml: |
    apiVersion: cluster.core.oam.dev/v1alpha1
    kind: ClusterGatewayProxyConfiguration
    spec:
      clientIdentityExchanger:
        rules:
          - name: super-user
            source:
              group: kubevela:ux
            type: PrivilegedIdentityExchanger
kind: ConfigMap
metadata:
  name: kubevela-cluster-gateway-proxy-config
  namespace: vela-system
---
apiVersion: v1
data:
  template: |
    import (
       "vela/op"
    )

    output: {
      op.#Read & {
        value: {
          apiVersion: "core.oam.dev/v1beta1"
            kind:       "ApplicationRevision"
            metadata: {
              name: parameter.name
              namespace: parameter.namespace
            }
        }
      }
    }

    parameter: {
       // +usage=Specify the name of the object
      name: string
      // +usage=Specify the namespace of the object
      namespace: *"default" | string
    }

    status: output.value
kind: ConfigMap
metadata:
  name: application-revision-view
  namespace: vela-system
---
apiVersion: v1
data:
  template: |
    import (
        "vela/ql"
    )
    parameter: {
        appName:    string
        appNs:      string
        name?:      string
        cluster?:   string
        clusterNs?: string
    }
    response: ql.#ListAppliedResources & {
        app: {
            name:      parameter.appName
            namespace: parameter.appNs
            filter: {
                if parameter.cluster != _|_ {
                    cluster: parameter.cluster
                }
                if parameter.clusterNs != _|_ {
                    clusterNamespace: parameter.clusterNs
                }
                if parameter.name != _|_ {
                    components: [parameter.name]
                }
            }
        }
    }
    if response.err == _|_ {
        status: {
            resources: response.list
        }
    }
    if response.err != _|_ {
        status: {
            error: response.err
        }
    }
kind: ConfigMap
metadata:
  name: service-applied-resources-view
  namespace: vela-system
---
apiVersion: v1
data:
  template: |
    import (
      "vela/ql"
    )

    parameter: {
      appName:    string
      appNs:      string
      name?:      string
      cluster?:   string
      clusterNs?: string
    }

    result: ql.#CollectPods & {
      app: {
        name:      parameter.appName
        namespace: parameter.appNs
        filter: {
          if parameter.cluster != _|_ {
            cluster: parameter.cluster
          }
          if parameter.clusterNs != _|_ {
            clusterNamespace: parameter.clusterNs
          }
          if parameter.name != _|_ {
            components: [parameter.name]
          }
        }
      }
    }

    if result.err == _|_ {
      status: {
        podList: [ for pod in result.list if pod.object != _|_ {
          cluster:   pod.cluster
          workload:  pod.workload
          component: pod.component
          metadata: {
            name:         pod.object.metadata.name
            namespace:    pod.object.metadata.namespace
            creationTime: pod.object.metadata.creationTimestamp
            labels:       pod.object.metadata.labels
            version: {
              if pod.publishVersion != _|_ {
                publishVersion: pod.publishVersion
              }
              if pod.deployVersion != _|_ {
                deployVersion: pod.deployVersion
              }
            }
          }
          status: {
            phase: pod.object.status.phase
            // refer to https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase
            if phase != "Pending" && phase != "Unknown" {
              if pod.object.podIP != _|_ {
                podIP: pod.object.status.podIP
              }
              if pod.object.hostIP != _|_ {
                hostIP: pod.object.status.hostIP
              }
              if pod.object.nodeName != _|_ {
                nodeName: pod.object.spec.nodeName
              }
            }
          }
        }]
      }
    }

    if result.err != _|_ {
      status: {
        error: result.err
      }
    }
kind: ConfigMap
metadata:
  name: component-pod-view
  namespace: vela-system
---
apiVersion: v1
data:
  template: |
    import (
    "vela/ql"
    )

    parameter: {
      appName:    string
      appNs:      string
      name?:      string
      cluster?:   string
      clusterNs?: string
    }

    result: ql.#CollectServices & {
      app: {
        name:      parameter.appName
        namespace: parameter.appNs
        filter: {
          if parameter.cluster != _|_ {
            cluster: parameter.cluster
          }
          if parameter.clusterNs != _|_ {
            clusterNamespace: parameter.clusterNs
          }
          if parameter.name != _|_ {
            components: [parameter.name]
          }
        }
      }
    }

    if result.err == _|_ {
      status: {
        services: result.list
      }
    }

    if result.err != _|_ {
      status: {
        error: result.err
      }
    }
kind: ConfigMap
metadata:
  name: component-service-view
  namespace: vela-system
---
apiVersion: v1
data:
  template: |
    import (
        "vela/ql"
    )
    parameter: {
        appName:    string
        appNs:      string
        name?:      string
        cluster?:   string
        clusterNs?: string
    }
    resources: ql.#CollectServiceEndpoints & {
        app: {
            name:      parameter.appName
            namespace: parameter.appNs
            filter: {
                if parameter.cluster != _|_ {
                    cluster: parameter.cluster
                }
                if parameter.clusterNs != _|_ {
                    clusterNamespace: parameter.clusterNs
                }
                if parameter.name != _|_ {
                    components: [parameter.name]
                }
            }
        }
    }
    if resources.err == _|_ {
        status: {
            endpoints: resources.list
        }
    }
    if resources.err != _|_ {
        status: {
            error: resources.err
        }
    }
kind: ConfigMap
metadata:
  name: service-endpoints-view
  namespace: vela-system
---
apiVersion: v1
data:
  template: |
    import (
        "vela/ql"
    )
    parameter: {
        appName:    string
        appNs:      string
        name?:      string
        cluster?:   string
        clusterNs?: string
    }
    response: ql.#GetApplicationTree & {
        app: {
            name:      parameter.appName
            namespace: parameter.appNs
            filter: {
                if parameter.cluster != _|_ {
                    cluster: parameter.cluster
                }
                if parameter.clusterNs != _|_ {
                    clusterNamespace: parameter.clusterNs
                }
                if parameter.name != _|_ {
                    components: [parameter.name]
                }
            }
        }
    }

    if response.err == _|_ {
        status: {
            resources: response.list
        }
    }
    if response.err != _|_ {
        status: {
            error: response.err
        }
    }
kind: ConfigMap
metadata:
  name: application-resource-tree-view
  namespace: vela-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kubevela-vela-core:cluster-gateway:proxy
rules:
- apiGroups:
  - cluster.core.oam.dev
  resources:
  - clustergateways/proxy
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubevela-vela-core:cluster-gateway:proxy
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubevela-vela-core:cluster-gateway:proxy
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: kubevela:client
- kind: ServiceAccount
  name: kubevela-vela-core
  namespace: vela-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubevela-vela-core:manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubevela-vela-core
  namespace: vela-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubevela-vela-core:leader-election-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - configmaps/status
  verbs:
  - get
  - update
  - patch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubevela-vela-core:template-reader-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps/status
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubevela-vela-core:leader-election-rolebinding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubevela-vela-core:leader-election-role
subjects:
- kind: ServiceAccount
  name: kubevela-vela-core
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubevela-vela-core:template-reader-binding
  namespace: vela-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubevela-vela-core:template-reader-role
subjects:
- kind: Group
  name: template-reader
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: vela-core-webhook
  namespace: vela-system
spec:
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 9443
  selector:
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/name: vela-core
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: kubevela-cluster-gateway-service
  namespace: vela-system
spec:
  ports:
  - port: 9443
    protocol: TCP
    targetPort: 9443
  selector:
    app.kubernetes.io/instance: kubevela-cluster-gateway
    app.kubernetes.io/name: vela-core-cluster-gateway
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-cluster-gateway
  namespace: vela-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: kubevela-cluster-gateway
      app.kubernetes.io/name: vela-core-cluster-gateway
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: kubevela-cluster-gateway
        app.kubernetes.io/name: vela-core-cluster-gateway
    spec:
      containers:
      - args:
        - apiserver
        - --secure-port=9443
        - --secret-namespace=vela-system
        - --feature-gates=APIPriorityAndFairness=false,ClientIdentityPenetration=false
        - --cluster-gateway-proxy-config=/etc/proxy-config/config.yaml
        - --tls-cert-file=/etc/k8s-cluster-gateway-certs/tls.crt
        - --tls-private-key-file=/etc/k8s-cluster-gateway-certs/tls.key
        image: oamdev/cluster-gateway:v1.7.0
        imagePullPolicy: IfNotPresent
        name: kubevela-vela-core-cluster-gateway
        ports:
        - containerPort: 9443
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
        securityContext: {}
        volumeMounts:
        - mountPath: /etc/proxy-config
          name: proxy-config
        - mountPath: /etc/k8s-cluster-gateway-certs
          name: tls-cert-vol
          readOnly: true
      securityContext: {}
      serviceAccountName: kubevela-vela-core
      volumes:
      - configMap:
          defaultMode: 420
          name: kubevela-cluster-gateway-proxy-config
        name: proxy-config
      - name: tls-cert-vol
        secret:
          defaultMode: 420
          secretName: kubevela-vela-core-cluster-gateway-tls-v2
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    controller.oam.dev/name: vela-core
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core
  namespace: vela-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: kubevela
      app.kubernetes.io/name: vela-core
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "8080"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/instance: kubevela
        app.kubernetes.io/name: vela-core
    spec:
      containers:
      - args:
        - --metrics-addr=:8080
        - --enable-leader-election
        - --use-webhook=true
        - --webhook-port=9443
        - --webhook-cert-dir=/etc/k8s-webhook-certs
        - --optimize-mark-with-prob=0.1
        - --optimize-disable-component-revision
        - --health-addr=:9440
        - --disable-caps=rollout
        - --system-definition-namespace=vela-system
        - --application-revision-limit=2
        - --definition-revision-limit=2
        - --oam-spec-ver=v0.3
        - --enable-cluster-gateway
        - --application-re-sync-period=5m
        - --concurrent-reconciles=4
        - --kube-api-qps=100
        - --kube-api-burst=200
        - --max-workflow-wait-backoff-time=60
        - --max-workflow-failed-backoff-time=300
        - --max-workflow-step-error-retry-times=10
        - --feature-gates=EnableSuspendOnFailure=false
        - --feature-gates=AuthenticateApplication=false
        - --feature-gates=LegacyComponentRevision=false
        - --feature-gates=GzipResourceTracker=false
        - --feature-gates=ZstdResourceTracker=true
        - --feature-gates=ApplyOnce=false
        - --feature-gates=MultiStageComponentApply=false
        - --feature-gates=GzipApplicationRevision=false
        - --feature-gates=ZstdApplicationRevision=true
        - --feature-gates=PreDispatchDryRun=true
        image: oamdev/vela-core:v1.7.4
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /healthz
            port: healthz
          initialDelaySeconds: 90
          periodSeconds: 5
        name: kubevela
        ports:
        - containerPort: 9443
          name: webhook-server
          protocol: TCP
        - containerPort: 9440
          name: healthz
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /readyz
            port: healthz
          initialDelaySeconds: 30
          periodSeconds: 5
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 50m
            memory: 20Mi
        securityContext: {}
        volumeMounts:
        - mountPath: /etc/k8s-webhook-certs
          name: tls-cert-vol
          readOnly: true
      securityContext: {}
      serviceAccountName: kubevela-vela-core
      volumes:
      - name: tls-cert-vol
        secret:
          defaultMode: 420
          secretName: kubevela-vela-core-admission
---
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  annotations: null
  labels:
    api: cluster-extension-apiserver
    apiserver: "true"
  name: v1alpha1.cluster.core.oam.dev
spec:
  caBundle: Cg==
  group: cluster.core.oam.dev
  groupPriorityMinimum: 2000
  insecureSkipTLSVerify: false
  service:
    name: kubevela-cluster-gateway-service
    namespace: vela-system
    port: 9443
  version: v1alpha1
  versionPriority: 10
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes cron jobs that run code or a script
      to completion.
  name: cron-task
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "output: {\n\tapiVersion: \"batch/v1beta1\"\n\tkind:       \"CronJob\"\n\tspec:
        {\n\t\tschedule:                   parameter.schedule\n\t\tconcurrencyPolicy:
        \         parameter.concurrencyPolicy\n\t\tsuspend:                    parameter.suspend\n\t\tsuccessfulJobsHistoryLimit:
        parameter.successfulJobsHistoryLimit\n\t\tfailedJobsHistoryLimit:     parameter.failedJobsHistoryLimit\n\t\tif
        parameter.startingDeadlineSeconds != _|_ {\n\t\t\tstartingDeadlineSeconds:
        parameter.startingDeadlineSeconds\n\t\t}\n\t\tjobTemplate: {\n\t\t\tmetadata:
        {\n\t\t\t\tlabels: {\n\t\t\t\t\tif parameter.labels != _|_ {\n\t\t\t\t\t\tparameter.labels\n\t\t\t\t\t}\n\t\t\t\t\t\"app.oam.dev/name\":
        \     context.appName\n\t\t\t\t\t\"app.oam.dev/component\": context.name\n\t\t\t\t}\n\t\t\t\tif
        parameter.annotations != _|_ {\n\t\t\t\t\tannotations: parameter.annotations\n\t\t\t\t}\n\t\t\t}\n\t\t\tspec:
        {\n\t\t\t\tparallelism: parameter.count\n\t\t\t\tcompletions: parameter.count\n\t\t\t\tif
        parameter.ttlSecondsAfterFinished != _|_ {\n\t\t\t\t\tttlSecondsAfterFinished:
        parameter.ttlSecondsAfterFinished\n\t\t\t\t}\n\t\t\t\tif parameter.activeDeadlineSeconds
        != _|_ {\n\t\t\t\t\tactiveDeadlineSeconds: parameter.activeDeadlineSeconds\n\t\t\t\t}\n\t\t\t\tbackoffLimit:
        parameter.backoffLimit\n\t\t\t\ttemplate: {\n\t\t\t\t\tmetadata: {\n\t\t\t\t\t\tlabels:
        {\n\t\t\t\t\t\t\tif parameter.labels != _|_ {\n\t\t\t\t\t\t\t\tparameter.labels\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\"app.oam.dev/name\":
        \     context.appName\n\t\t\t\t\t\t\t\"app.oam.dev/component\": context.name\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        parameter.annotations != _|_ {\n\t\t\t\t\t\t\tannotations: parameter.annotations\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tspec:
        {\n\t\t\t\t\t\trestartPolicy: parameter.restart\n\t\t\t\t\t\tcontainers: [{\n\t\t\t\t\t\t\tname:
        \ context.name\n\t\t\t\t\t\t\timage: parameter.image\n\t\t\t\t\t\t\tif parameter[\"imagePullPolicy\"]
        != _|_ {\n\t\t\t\t\t\t\t\timagePullPolicy: parameter.imagePullPolicy\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        parameter[\"cmd\"] != _|_ {\n\t\t\t\t\t\t\t\tcommand: parameter.cmd\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        parameter[\"env\"] != _|_ {\n\t\t\t\t\t\t\t\tenv: parameter.env\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        parameter[\"cpu\"] != _|_ {\n\t\t\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\t\t\tlimits:
        cpu:   parameter.cpu\n\t\t\t\t\t\t\t\t\trequests: cpu: parameter.cpu\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        parameter[\"memory\"] != _|_ {\n\t\t\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\t\t\tlimits:
        memory:   parameter.memory\n\t\t\t\t\t\t\t\t\trequests: memory: parameter.memory\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        parameter[\"volumes\"] != _|_ {\n\t\t\t\t\t\t\t\tvolumeMounts: [ for v in
        parameter.volumes {\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tmountPath: v.mountPath\n\t\t\t\t\t\t\t\t\t\tname:
        \     v.name\n\t\t\t\t\t\t\t\t\t}}]\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}]\n\t\t\t\t\t\tif
        parameter[\"volumes\"] != _|_ {\n\t\t\t\t\t\t\tvolumes: [ for v in parameter.volumes
        {\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tname: v.name\n\t\t\t\t\t\t\t\t\tif
        v.type == \"pvc\" {\n\t\t\t\t\t\t\t\t\t\tpersistentVolumeClaim: claimName:
        v.claimName\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tif v.type == \"configMap\"
        {\n\t\t\t\t\t\t\t\t\t\tconfigMap: {\n\t\t\t\t\t\t\t\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\t\t\t\t\t\t\t\tname:
        \       v.cmName\n\t\t\t\t\t\t\t\t\t\t\tif v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\t\t\titems:
        v.items\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tif
        v.type == \"secret\" {\n\t\t\t\t\t\t\t\t\t\tsecret: {\n\t\t\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\t\t\tsecretName:  v.secretName\n\t\t\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tif
        v.type == \"emptyDir\" {\n\t\t\t\t\t\t\t\t\t\temptyDir: medium: v.medium\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}}]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        parameter[\"imagePullSecrets\"] != _|_ {\n\t\t\t\t\t\t\timagePullSecrets:
        [ for v in parameter.imagePullSecrets {\n\t\t\t\t\t\t\t\tname: v\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        parameter.hostAliases != _|_ {\n\t\t\t\t\t\t\thostAliases: [ for v in parameter.hostAliases
        {\n\t\t\t\t\t\t\t\tip:        v.ip\n\t\t\t\t\t\t\t\thostnames: v.hostnames\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the labels in the workload\n\tlabels?: [string]: string\n\n\t//
        +usage=Specify the annotations in the workload\n\tannotations?: [string]:
        string\n\n\t// +usage=Specify the schedule in Cron format, see https://en.wikipedia.org/wiki/Cron\n\tschedule:
        string\n\n\t// +usage=Specify deadline in seconds for starting the job if
        it misses scheduled\n\tstartingDeadlineSeconds?: int\n\n\t// +usage=suspend
        subsequent executions\n\tsuspend: *false | bool\n\n\t// +usage=Specifies how
        to treat concurrent executions of a Job\n\tconcurrencyPolicy: *\"Allow\" |
        \"Allow\" | \"Forbid\" | \"Replace\"\n\n\t// +usage=The number of successful
        finished jobs to retain\n\tsuccessfulJobsHistoryLimit: *3 | int\n\n\t// +usage=The
        number of failed finished jobs to retain\n\tfailedJobsHistoryLimit: *1 | int\n\n\t//
        +usage=Specify number of tasks to run in parallel\n\t// +short=c\n\tcount:
        *1 | int\n\n\t// +usage=Which image would you like to use for your service\n\t//
        +short=i\n\timage: string\n\n\t// +usage=Specify image pull policy for your
        service\n\timagePullPolicy?: \"Always\" | \"Never\" | \"IfNotPresent\"\n\n\t//
        +usage=Specify image pull secrets for your service\n\timagePullSecrets?: [...string]\n\n\t//
        +usage=Define the job restart policy, the value can only be Never or OnFailure.
        By default, it's Never.\n\trestart: *\"Never\" | string\n\n\t// +usage=Commands
        to run in the container\n\tcmd?: [...string]\n\n\t// +usage=Define arguments
        by using environment variables\n\tenv?: [...{\n\t\t// +usage=Environment variable
        name\n\t\tname: string\n\t\t// +usage=The value of the environment variable\n\t\tvalue?:
        string\n\t\t// +usage=Specifies a source the value of this var should come
        from\n\t\tvalueFrom?: {\n\t\t\t// +usage=Selects a key of a secret in the
        pod's namespace\n\t\t\tsecretKeyRef?: {\n\t\t\t\t// +usage=The name of the
        secret in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the secret to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t\t// +usage=Selects a key of a config map in the pod's
        namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t// +usage=The name of the config
        map in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the config map to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t}\n\t}]\n\n\t// +usage=Number of CPU units for the service,
        like `0.5` (0.5 CPU core), `1` (1 CPU core)\n\tcpu?: string\n\n\t// +usage=Specifies
        the attributes of the memory resource required for the container.\n\tmemory?:
        string\n\n\t// +usage=Declare volumes and volumeMounts\n\tvolumes?: [...{\n\t\tname:
        \     string\n\t\tmountPath: string\n\t\t// +usage=Specify volume type, options:
        \"pvc\",\"configMap\",\"secret\",\"emptyDir\", default to emptyDir\n\t\ttype:
        *\"emptyDir\" | \"pvc\" | \"configMap\" | \"secret\"\n\t\tif type == \"pvc\"
        {\n\t\t\tclaimName: string\n\t\t}\n\t\tif type == \"configMap\" {\n\t\t\tdefaultMode:
        *420 | int\n\t\t\tcmName:      string\n\t\t\titems?: [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath:
        string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif type == \"secret\"
        {\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"emptyDir\" {\n\t\t\tmedium: *\"\" | \"Memory\"\n\t\t}\n\t}]\n\n\t//
        +usage=An optional list of hosts and IPs that will be injected into the pod's
        hosts file\n\thostAliases?: [...{\n\t\tip: string\n\t\thostnames: [...string]\n\t}]\n\n\t//
        +usage=Limits the lifetime of a Job that has finished\n\tttlSecondsAfterFinished?:
        int\n\n\t// +usage=The duration in seconds relative to the startTime that
        the job may be continuously active before the system tries to terminate it\n\tactiveDeadlineSeconds?:
        int\n\n\t// +usage=The number of retries before marking this job failed\n\tbackoffLimit:
        *6 | int\n\n\t// +usage=Instructions for assessing whether the container is
        alive.\n\tlivenessProbe?: #HealthProbe\n\n\t// +usage=Instructions for assessing
        whether the container is in a suitable state to serve traffic.\n\treadinessProbe?:
        #HealthProbe\n}\n#HealthProbe: {\n\n\t// +usage=Instructions for assessing
        container health by executing a command. Either this attribute or the httpGet
        attribute or the tcpSocket attribute MUST be specified. This attribute is
        mutually exclusive with both the httpGet attribute and the tcpSocket attribute.\n\texec?:
        {\n\t\t// +usage=A command to be executed inside the container to assess its
        health. Each space delimited token of the command is a separate array element.
        Commands exiting 0 are considered to be successful probes, whilst all other
        exit codes are considered failures.\n\t\tcommand: [...string]\n\t}\n\n\t//
        +usage=Instructions for assessing container health by executing an HTTP GET
        request. Either this attribute or the exec attribute or the tcpSocket attribute
        MUST be specified. This attribute is mutually exclusive with both the exec
        attribute and the tcpSocket attribute.\n\thttpGet?: {\n\t\t// +usage=The endpoint,
        relative to the port, to which the HTTP GET request should be directed.\n\t\tpath:
        string\n\t\t// +usage=The TCP socket within the container to which the HTTP
        GET request should be directed.\n\t\tport: int\n\t\thttpHeaders?: [...{\n\t\t\tname:
        \ string\n\t\t\tvalue: string\n\t\t}]\n\t}\n\n\t// +usage=Instructions for
        assessing container health by probing a TCP socket. Either this attribute
        or the exec attribute or the httpGet attribute MUST be specified. This attribute
        is mutually exclusive with both the exec attribute and the httpGet attribute.\n\ttcpSocket?:
        {\n\t\t// +usage=The TCP socket within the container that should be probed
        to assess container health.\n\t\tport: int\n\t}\n\n\t// +usage=Number of seconds
        after the container is started before the first probe is initiated.\n\tinitialDelaySeconds:
        *0 | int\n\n\t// +usage=How often, in seconds, to execute the probe.\n\tperiodSeconds:
        *10 | int\n\n\t// +usage=Number of seconds after which the probe times out.\n\ttimeoutSeconds:
        *1 | int\n\n\t// +usage=Minimum consecutive successes for the probe to be
        considered successful after having failed.\n\tsuccessThreshold: *1 | int\n\n\t//
        +usage=Number of consecutive failures required to determine the container
        is not alive (liveness probe) or not ready (readiness probe).\n\tfailureThreshold:
        *3 | int\n}\n"
  workload:
    definition:
      apiVersion: batch/v1beta1
      kind: CronJob
    type: cronjobs.batch
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes daemonset services in Kubernetes.
  name: daemon
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"strconv\"\n)\n\nmountsArray: [\n\tif parameter.volumeMounts
        != _|_ && parameter.volumeMounts.pvc != _|_ for v in parameter.volumeMounts.pvc
        {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif parameter.volumeMounts
        != _|_ && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap
        {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif parameter.volumeMounts
        != _|_ && parameter.volumeMounts.secret != _|_ for v in parameter.volumeMounts.secret
        {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif parameter.volumeMounts
        != _|_ && parameter.volumeMounts.emptyDir != _|_ for v in parameter.volumeMounts.emptyDir
        {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif parameter.volumeMounts
        != _|_ && parameter.volumeMounts.hostPath != _|_ for v in parameter.volumeMounts.hostPath
        {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n]\nvolumesList: [\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in
        parameter.volumeMounts.pvc {\n\t\t{\n\t\t\tname: v.name\n\t\t\tpersistentVolumeClaim:
        claimName: v.claimName\n\t\t}\n\t},\n\n\tif parameter.volumeMounts != _|_
        && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap
        {\n\t\t{\n\t\t\tname: v.name\n\t\t\tconfigMap: {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tname:
        \       v.cmName\n\t\t\t\tif v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for
        v in parameter.volumeMounts.secret {\n\t\t{\n\t\t\tname: v.name\n\t\t\tsecret:
        {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tsecretName:  v.secretName\n\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for
        v in parameter.volumeMounts.emptyDir {\n\t\t{\n\t\t\tname: v.name\n\t\t\temptyDir:
        medium: v.medium\n\t\t}\n\t},\n\n\tif parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath
        != _|_ for v in parameter.volumeMounts.hostPath {\n\t\t{\n\t\t\tname: v.name\n\t\t\thostPath:
        path: v.path\n\t\t}\n\t},\n]\ndeDupVolumesArray: [\n\tfor val in [\n\t\tfor
        i, vi in volumesList {\n\t\t\tfor j, vj in volumesList if j < i && vi.name
        == vj.name {\n\t\t\t\t_ignore: true\n\t\t\t}\n\t\t\tvi\n\t\t},\n\t] if val._ignore
        == _|_ {\n\t\tval\n\t},\n]\noutput: {\n\tapiVersion: \"apps/v1\"\n\tkind:
        \      \"DaemonSet\"\n\tspec: {\n\t\tselector: matchLabels: \"app.oam.dev/component\":
        context.name\n\n\t\ttemplate: {\n\t\t\tmetadata: {\n\t\t\t\tlabels: {\n\t\t\t\t\tif
        parameter.labels != _|_ {\n\t\t\t\t\t\tparameter.labels\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.addRevisionLabel {\n\t\t\t\t\t\t\"app.oam.dev/revision\": context.revision\n\t\t\t\t\t}\n\t\t\t\t\t\"app.oam.dev/name\":
        \     context.appName\n\t\t\t\t\t\"app.oam.dev/component\": context.name\n\t\t\t\t}\n\t\t\t\tif
        parameter.annotations != _|_ {\n\t\t\t\t\tannotations: parameter.annotations\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tspec:
        {\n\t\t\t\tcontainers: [{\n\t\t\t\t\tname:  context.name\n\t\t\t\t\timage:
        parameter.image\n\t\t\t\t\tif parameter[\"port\"] != _|_ && parameter[\"ports\"]
        == _|_ {\n\t\t\t\t\t\tports: [{\n\t\t\t\t\t\t\tcontainerPort: parameter.port\n\t\t\t\t\t\t}]\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter[\"ports\"] != _|_ {\n\t\t\t\t\t\tports: [ for v in parameter.ports
        {\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tcontainerPort: v.port\n\t\t\t\t\t\t\t\tprotocol:
        \     v.protocol\n\t\t\t\t\t\t\t\tif v.name != _|_ {\n\t\t\t\t\t\t\t\t\tname:
        v.name\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tif v.name == _|_ {\n\t\t\t\t\t\t\t\t\tname:
        \"port-\" + strconv.FormatInt(v.port, 10)\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}}]\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"imagePullPolicy\"] != _|_ {\n\t\t\t\t\t\timagePullPolicy: parameter.imagePullPolicy\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cmd\"] != _|_ {\n\t\t\t\t\t\tcommand: parameter.cmd\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"env\"] != _|_ {\n\t\t\t\t\t\tenv: parameter.env\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        context[\"config\"] != _|_ {\n\t\t\t\t\t\tenv: context.config\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cpu\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        cpu:   parameter.cpu\n\t\t\t\t\t\t\trequests: cpu: parameter.cpu\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"memory\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        memory:   parameter.memory\n\t\t\t\t\t\t\trequests: memory: parameter.memory\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumes\"] != _|_ && parameter[\"volumeMounts\"] == _|_ {\n\t\t\t\t\t\tvolumeMounts:
        [ for v in parameter.volumes {\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tmountPath:
        v.mountPath\n\t\t\t\t\t\t\t\tname:      v.name\n\t\t\t\t\t\t\t}}]\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumeMounts\"] != _|_ {\n\t\t\t\t\t\tvolumeMounts: mountsArray\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"livenessProbe\"] != _|_ {\n\t\t\t\t\t\tlivenessProbe: parameter.livenessProbe\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"readinessProbe\"] != _|_ {\n\t\t\t\t\t\treadinessProbe: parameter.readinessProbe\n\t\t\t\t\t}\n\n\t\t\t\t}]\n\n\t\t\t\tif
        parameter[\"hostAliases\"] != _|_ {\n\t\t\t\t\t// +patchKey=ip\n\t\t\t\t\thostAliases:
        parameter.hostAliases\n\t\t\t\t}\n\n\t\t\t\tif parameter[\"imagePullSecrets\"]
        != _|_ {\n\t\t\t\t\timagePullSecrets: [ for v in parameter.imagePullSecrets
        {\n\t\t\t\t\t\tname: v\n\t\t\t\t\t},\n\t\t\t\t\t]\n\t\t\t\t}\n\n\t\t\t\tif
        parameter[\"volumes\"] != _|_ && parameter[\"volumeMounts\"] == _|_ {\n\t\t\t\t\tvolumes:
        [ for v in parameter.volumes {\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tname: v.name\n\t\t\t\t\t\t\tif
        v.type == \"pvc\" {\n\t\t\t\t\t\t\t\tpersistentVolumeClaim: claimName: v.claimName\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"configMap\" {\n\t\t\t\t\t\t\t\tconfigMap: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tname:        v.cmName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"secret\" {\n\t\t\t\t\t\t\t\tsecret: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tsecretName:  v.secretName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"emptyDir\" {\n\t\t\t\t\t\t\t\temptyDir: medium: v.medium\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t\t}\n\n\t\t\t\tif
        parameter[\"volumeMounts\"] != _|_ {\n\t\t\t\t\tvolumes: deDupVolumesArray\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nexposePorts:
        [\n\tif parameter.ports != _|_ for v in parameter.ports if v.expose == true
        {\n\t\tport:       v.port\n\t\ttargetPort: v.port\n\t\tif v.name != _|_ {\n\t\t\tname:
        v.name\n\t\t}\n\t\tif v.name == _|_ {\n\t\t\tname: \"port-\" + strconv.FormatInt(v.port,
        10)\n\t\t}\n\t},\n]\noutputs: {\n\tif len(exposePorts) != 0 {\n\t\twebserviceExpose:
        {\n\t\t\tapiVersion: \"v1\"\n\t\t\tkind:       \"Service\"\n\t\t\tmetadata:
        name: context.name\n\t\t\tspec: {\n\t\t\t\tselector: \"app.oam.dev/component\":
        context.name\n\t\t\t\tports: exposePorts\n\t\t\t\ttype:  parameter.exposeType\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the labels in the workload\n\tlabels?: [string]: string\n\n\t//
        +usage=Specify the annotations in the workload\n\tannotations?: [string]:
        string\n\n\t// +usage=Which image would you like to use for your service\n\t//
        +short=i\n\timage: string\n\n\t// +usage=Specify image pull policy for your
        service\n\timagePullPolicy?: \"Always\" | \"Never\" | \"IfNotPresent\"\n\n\t//
        +usage=Specify image pull secrets for your service\n\timagePullSecrets?: [...string]\n\n\t//
        +ignore\n\t// +usage=Deprecated field, please use ports instead\n\t// +short=p\n\tport?:
        int\n\n\t// +usage=Which ports do you want customer traffic sent to, defaults
        to 80\n\tports?: [...{\n\t\t// +usage=Number of port to expose on the pod's
        IP address\n\t\tport: int\n\t\t// +usage=Name of the port\n\t\tname?: string\n\t\t//
        +usage=Protocol for port. Must be UDP, TCP, or SCTP\n\t\tprotocol: *\"TCP\"
        | \"UDP\" | \"SCTP\"\n\t\t// +usage=Specify if the port should be exposed\n\t\texpose:
        *false | bool\n\t}]\n\n\t// +ignore\n\t// +usage=Specify what kind of Service
        you want. options: \"ClusterIP\", \"NodePort\", \"LoadBalancer\", \"ExternalName\"\n\texposeType:
        *\"ClusterIP\" | \"NodePort\" | \"LoadBalancer\" | \"ExternalName\"\n\n\t//
        +ignore\n\t// +usage=If addRevisionLabel is true, the revision label will
        be added to the underlying pods\n\taddRevisionLabel: *false | bool\n\n\t//
        +usage=Commands to run in the container\n\tcmd?: [...string]\n\n\t// +usage=Define
        arguments by using environment variables\n\tenv?: [...{\n\t\t// +usage=Environment
        variable name\n\t\tname: string\n\t\t// +usage=The value of the environment
        variable\n\t\tvalue?: string\n\t\t// +usage=Specifies a source the value of
        this var should come from\n\t\tvalueFrom?: {\n\t\t\t// +usage=Selects a key
        of a secret in the pod's namespace\n\t\t\tsecretKeyRef?: {\n\t\t\t\t// +usage=The
        name of the secret in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the secret to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t\t// +usage=Selects a key of a config map in the pod's
        namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t// +usage=The name of the config
        map in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the config map to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t}\n\t}]\n\n\t// +usage=Number of CPU units for the service,
        like `0.5` (0.5 CPU core), `1` (1 CPU core)\n\tcpu?: string\n\n\t// +usage=Specifies
        the attributes of the memory resource required for the container.\n\tmemory?:
        string\n\n\tvolumeMounts?: {\n\t\t// +usage=Mount PVC type volume\n\t\tpvc?:
        [...{\n\t\t\tname:      string\n\t\t\tmountPath: string\n\t\t\t// +usage=The
        name of the PVC\n\t\t\tclaimName: string\n\t\t}]\n\t\t// +usage=Mount ConfigMap
        type volume\n\t\tconfigMap?: [...{\n\t\t\tname:        string\n\t\t\tmountPath:
        \  string\n\t\t\tdefaultMode: *420 | int\n\t\t\tcmName:      string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}]\n\t\t//
        +usage=Mount Secret type volume\n\t\tsecret?: [...{\n\t\t\tname:        string\n\t\t\tmountPath:
        \  string\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}]\n\t\t//
        +usage=Mount EmptyDir type volume\n\t\temptyDir?: [...{\n\t\t\tname:      string\n\t\t\tmountPath:
        string\n\t\t\tmedium:    *\"\" | \"Memory\"\n\t\t}]\n\t\t// +usage=Mount HostPath
        type volume\n\t\thostPath?: [...{\n\t\t\tname:              string\n\t\t\tmountPath:
        \        string\n\t\t\tmountPropagation?: \"None\" | \"HostToContainer\" |
        \"Bidirectional\"\n\t\t\tpath:              string\n\t\t\treadOnly?:         bool\n\t\t}]\n\t}\n\n\t//
        +usage=Deprecated field, use volumeMounts instead.\n\tvolumes?: [...{\n\t\tname:
        \     string\n\t\tmountPath: string\n\t\t// +usage=Specify volume type, options:
        \"pvc\",\"configMap\",\"secret\",\"emptyDir\", default to emptyDir\n\t\ttype:
        *\"emptyDir\" | \"pvc\" | \"configMap\" | \"secret\"\n\t\tif type == \"pvc\"
        {\n\t\t\tclaimName: string\n\t\t}\n\t\tif type == \"configMap\" {\n\t\t\tdefaultMode:
        *420 | int\n\t\t\tcmName:      string\n\t\t\titems?: [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath:
        string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif type == \"secret\"
        {\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"emptyDir\" {\n\t\t\tmedium: *\"\" | \"Memory\"\n\t\t}\n\t}]\n\n\t//
        +usage=Instructions for assessing whether the container is alive.\n\tlivenessProbe?:
        #HealthProbe\n\n\t// +usage=Instructions for assessing whether the container
        is in a suitable state to serve traffic.\n\treadinessProbe?: #HealthProbe\n\n\t//
        +usage=Specify the hostAliases to add\n\thostAliases?: [...{\n\t\tip: string\n\t\thostnames:
        [...string]\n\t}]\n}\n#HealthProbe: {\n\n\t// +usage=Instructions for assessing
        container health by executing a command. Either this attribute or the httpGet
        attribute or the tcpSocket attribute MUST be specified. This attribute is
        mutually exclusive with both the httpGet attribute and the tcpSocket attribute.\n\texec?:
        {\n\t\t// +usage=A command to be executed inside the container to assess its
        health. Each space delimited token of the command is a separate array element.
        Commands exiting 0 are considered to be successful probes, whilst all other
        exit codes are considered failures.\n\t\tcommand: [...string]\n\t}\n\n\t//
        +usage=Instructions for assessing container health by executing an HTTP GET
        request. Either this attribute or the exec attribute or the tcpSocket attribute
        MUST be specified. This attribute is mutually exclusive with both the exec
        attribute and the tcpSocket attribute.\n\thttpGet?: {\n\t\t// +usage=The endpoint,
        relative to the port, to which the HTTP GET request should be directed.\n\t\tpath:
        string\n\t\t// +usage=The TCP socket within the container to which the HTTP
        GET request should be directed.\n\t\tport:    int\n\t\thost?:   string\n\t\tscheme?:
        *\"HTTP\" | string\n\t\thttpHeaders?: [...{\n\t\t\tname:  string\n\t\t\tvalue:
        string\n\t\t}]\n\t}\n\n\t// +usage=Instructions for assessing container health
        by probing a TCP socket. Either this attribute or the exec attribute or the
        httpGet attribute MUST be specified. This attribute is mutually exclusive
        with both the exec attribute and the httpGet attribute.\n\ttcpSocket?: {\n\t\t//
        +usage=The TCP socket within the container that should be probed to assess
        container health.\n\t\tport: int\n\t}\n\n\t// +usage=Number of seconds after
        the container is started before the first probe is initiated.\n\tinitialDelaySeconds:
        *0 | int\n\n\t// +usage=How often, in seconds, to execute the probe.\n\tperiodSeconds:
        *10 | int\n\n\t// +usage=Number of seconds after which the probe times out.\n\ttimeoutSeconds:
        *1 | int\n\n\t// +usage=Minimum consecutive successes for the probe to be
        considered successful after having failed.\n\tsuccessThreshold: *1 | int\n\n\t//
        +usage=Number of consecutive failures required to determine the container
        is not alive (liveness probe) or not ready (readiness probe).\n\tfailureThreshold:
        *3 | int\n}\n"
  status:
    customStatus: "ready: {\n\treplicas: *0 | int\n} & {\n\tif context.output.status.numberReady
      != _|_ {\n\t\treplicas: context.output.status.numberReady\n\t}\n}\ndesired:
      {\n\treplicas: *0 | int\n} & {\n\tif context.output.status.desiredNumberScheduled
      != _|_ {\n\t\treplicas: context.output.status.desiredNumberScheduled\n\t}\n}\nmessage:
      \"Ready:\\(ready.replicas)/\\(desired.replicas)\""
    healthPolicy: "ready: {\n\treplicas: *0 | int\n} & {\n\tif context.output.status.numberReady
      != _|_ {\n\t\treplicas: context.output.status.numberReady\n\t}\n}\ndesired:
      {\n\treplicas: *0 | int\n} & {\n\tif context.output.status.desiredNumberScheduled
      != _|_ {\n\t\treplicas: context.output.status.desiredNumberScheduled\n\t}\n}\ncurrent:
      {\n\treplicas: *0 | int\n} & {\n\tif context.output.status.currentNumberScheduled
      != _|_ {\n\t\treplicas: context.output.status.currentNumberScheduled\n\t}\n}\nupdated:
      {\n\treplicas: *0 | int\n} & {\n\tif context.output.status.updatedNumberScheduled
      != _|_ {\n\t\treplicas: context.output.status.updatedNumberScheduled\n\t}\n}\ngeneration:
      {\n\tmetadata: context.output.metadata.generation\n\tobserved: *0 | int\n} &
      {\n\tif context.output.status.observedGeneration != _|_ {\n\t\tobserved: context.output.status.observedGeneration\n\t}\n}\nisHealth:
      (desired.replicas == ready.replicas) && (desired.replicas == updated.replicas)
      && (desired.replicas == current.replicas) && (generation.observed == generation.metadata
      || generation.observed > generation.metadata)"
  workload:
    definition:
      apiVersion: apps/v1
      kind: DaemonSet
    type: daemonsets.apps
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: K8s-objects allow users to specify raw K8s objects
      in properties
  name: k8s-objects
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "output: parameter.objects[0]\noutputs: {\n\tfor i, v in parameter.objects
        {\n\t\tif i > 0 {\n\t\t\t\"objects-\\(i)\": v\n\t\t}\n\t}\n}\nparameter: objects:
        [...{}]\n"
  workload:
    type: autodetects.core.oam.dev
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Raw allow users to specify raw K8s object in properties.
      This definition is DEPRECATED, please use 'k8s-objects' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: raw
  namespace: vela-system
spec:
  schematic:
    cue:
      template: |
        output: parameter
        parameter: {}
  workload:
    type: autodetects.core.oam.dev
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Ref-objects allow users to specify ref objects
      to use. Notice that this component type have special handle logic.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: ref-objects
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#K8sObject: {\n\t// +usage=The resource type for the Kubernetes objects\n\tresource?:
        string\n\t// +usage=The group name for the Kubernetes objects\n\tgroup?: string\n\t//
        +usage=If specified, fetch the Kubernetes objects with the name, exclusive
        to labelSelector\n\tname?: string\n\t// +usage=If specified, fetch the Kubernetes
        objects from the namespace. Otherwise, fetch from the application's namespace.\n\tnamespace?:
        string\n\t// +usage=If specified, fetch the Kubernetes objects from the cluster.
        Otherwise, fetch from the local cluster.\n\tcluster?: string\n\t// +usage=If
        specified, fetch the Kubernetes objects according to the label selector, exclusive
        to name\n\tlabelSelector?: [string]: string\n\t...\n}\noutput: {\n\tif len(parameter.objects)
        > 0 {\n\t\tparameter.objects[0]\n\t}\n\t...\n}\noutputs: {\n\tfor i, v in
        parameter.objects {\n\t\tif i > 0 {\n\t\t\t\"objects-\\(i)\": v\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=If specified, application will fetch native Kubernetes objects
        according to the object description\n\tobjects?: [...#K8sObject]\n\t// +usage=If
        specified, the objects in the urls will be loaded.\n\turls?: [...string]\n}\n"
  status:
    customStatus: "if context.output.apiVersion == \"apps/v1\" && context.output.kind
      == \"Deployment\" {\n\tready: {\n\t\treadyReplicas: *0 | int\n\t} & {\n\t\tif
      context.output.status.readyReplicas != _|_ {\n\t\t\treadyReplicas: context.output.status.readyReplicas\n\t\t}\n\t}\n\tmessage:
      \"Ready:\\(ready.readyReplicas)/\\(context.output.spec.replicas)\"\n}\nif context.output.apiVersion
      != \"apps/v1\" || context.output.kind != \"Deployment\" {\n\tmessage: \"\"\n}"
    healthPolicy: "if context.output.apiVersion == \"apps/v1\" && context.output.kind
      == \"Deployment\" {\n\tready: {\n\t\tupdatedReplicas:    *0 | int\n\t\treadyReplicas:
      \     *0 | int\n\t\treplicas:           *0 | int\n\t\tobservedGeneration: *0
      | int\n\t} & {\n\t\tif context.output.status.updatedReplicas != _|_ {\n\t\t\tupdatedReplicas:
      context.output.status.updatedReplicas\n\t\t}\n\t\tif context.output.status.readyReplicas
      != _|_ {\n\t\t\treadyReplicas: context.output.status.readyReplicas\n\t\t}\n\t\tif
      context.output.status.replicas != _|_ {\n\t\t\treplicas: context.output.status.replicas\n\t\t}\n\t\tif
      context.output.status.observedGeneration != _|_ {\n\t\t\tobservedGeneration:
      context.output.status.observedGeneration\n\t\t}\n\t}\n\tisHealth: (context.output.spec.replicas
      == ready.readyReplicas) && (context.output.spec.replicas == ready.updatedReplicas)
      && (context.output.spec.replicas == ready.replicas) && (ready.observedGeneration
      == context.output.metadata.generation || ready.observedGeneration > context.output.metadata.generation)\n}\nif
      context.output.apiVersion != \"apps/v1\" || context.output.kind != \"Deployment\"
      {\n\tisHealth: true\n}"
  workload:
    type: autodetects.core.oam.dev
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes jobs that run code or a script to completion.
  name: task
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "output: {\n\tapiVersion: \"batch/v1\"\n\tkind:       \"Job\"\n\tspec:
        {\n\t\tparallelism: parameter.count\n\t\tcompletions: parameter.count\n\t\ttemplate:
        {\n\t\t\tmetadata: {\n\t\t\t\tlabels: {\n\t\t\t\t\tif parameter.labels !=
        _|_ {\n\t\t\t\t\t\tparameter.labels\n\t\t\t\t\t}\n\t\t\t\t\t\"app.oam.dev/name\":
        \     context.appName\n\t\t\t\t\t\"app.oam.dev/component\": context.name\n\t\t\t\t}\n\t\t\t\tif
        parameter.annotations != _|_ {\n\t\t\t\t\tannotations: parameter.annotations\n\t\t\t\t}\n\t\t\t}\n\t\t\tspec:
        {\n\t\t\t\trestartPolicy: parameter.restart\n\t\t\t\tcontainers: [{\n\t\t\t\t\tname:
        \ context.name\n\t\t\t\t\timage: parameter.image\n\n\t\t\t\t\tif parameter[\"imagePullPolicy\"]
        != _|_ {\n\t\t\t\t\t\timagePullPolicy: parameter.imagePullPolicy\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cmd\"] != _|_ {\n\t\t\t\t\t\tcommand: parameter.cmd\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"env\"] != _|_ {\n\t\t\t\t\t\tenv: parameter.env\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cpu\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        cpu:   parameter.cpu\n\t\t\t\t\t\t\trequests: cpu: parameter.cpu\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"memory\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        memory:   parameter.memory\n\t\t\t\t\t\t\trequests: memory: parameter.memory\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumes\"] != _|_ {\n\t\t\t\t\t\tvolumeMounts: [ for v in parameter.volumes
        {\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tmountPath: v.mountPath\n\t\t\t\t\t\t\t\tname:
        \     v.name\n\t\t\t\t\t\t\t}}]\n\t\t\t\t\t}\n\t\t\t\t}]\n\n\t\t\t\tif parameter[\"volumes\"]
        != _|_ {\n\t\t\t\t\tvolumes: [ for v in parameter.volumes {\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tname:
        v.name\n\t\t\t\t\t\t\tif v.type == \"pvc\" {\n\t\t\t\t\t\t\t\tpersistentVolumeClaim:
        claimName: v.claimName\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif v.type == \"configMap\"
        {\n\t\t\t\t\t\t\t\tconfigMap: {\n\t\t\t\t\t\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\t\t\t\t\t\tname:
        \       v.cmName\n\t\t\t\t\t\t\t\t\tif v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems:
        v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"secret\" {\n\t\t\t\t\t\t\t\tsecret: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tsecretName:  v.secretName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"emptyDir\" {\n\t\t\t\t\t\t\t\temptyDir: medium: v.medium\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}}]\n\t\t\t\t}\n\n\t\t\t\tif
        parameter[\"imagePullSecrets\"] != _|_ {\n\t\t\t\t\timagePullSecrets: [ for
        v in parameter.imagePullSecrets {\n\t\t\t\t\t\tname: v\n\t\t\t\t\t},\n\t\t\t\t\t]\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the labels in the workload\n\tlabels?: [string]: string\n\n\t//
        +usage=Specify the annotations in the workload\n\tannotations?: [string]:
        string\n\n\t// +usage=Specify number of tasks to run in parallel\n\t// +short=c\n\tcount:
        *1 | int\n\n\t// +usage=Which image would you like to use for your service\n\t//
        +short=i\n\timage: string\n\n\t// +usage=Specify image pull policy for your
        service\n\timagePullPolicy?: \"Always\" | \"Never\" | \"IfNotPresent\"\n\n\t//
        +usage=Specify image pull secrets for your service\n\timagePullSecrets?: [...string]\n\n\t//
        +usage=Define the job restart policy, the value can only be Never or OnFailure.
        By default, it's Never.\n\trestart: *\"Never\" | string\n\n\t// +usage=Commands
        to run in the container\n\tcmd?: [...string]\n\n\t// +usage=Define arguments
        by using environment variables\n\tenv?: [...{\n\t\t// +usage=Environment variable
        name\n\t\tname: string\n\t\t// +usage=The value of the environment variable\n\t\tvalue?:
        string\n\t\t// +usage=Specifies a source the value of this var should come
        from\n\t\tvalueFrom?: {\n\t\t\t// +usage=Selects a key of a secret in the
        pod's namespace\n\t\t\tsecretKeyRef?: {\n\t\t\t\t// +usage=The name of the
        secret in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the secret to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t\t// +usage=Selects a key of a config map in the pod's
        namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t// +usage=The name of the config
        map in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the config map to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t}\n\t}]\n\n\t// +usage=Number of CPU units for the service,
        like `0.5` (0.5 CPU core), `1` (1 CPU core)\n\tcpu?: string\n\n\t// +usage=Specifies
        the attributes of the memory resource required for the container.\n\tmemory?:
        string\n\n\t// +usage=Declare volumes and volumeMounts\n\tvolumes?: [...{\n\t\tname:
        \     string\n\t\tmountPath: string\n\t\t// +usage=Specify volume type, options:
        \"pvc\",\"configMap\",\"secret\",\"emptyDir\", default to emptyDir\n\t\ttype:
        *\"emptyDir\" | \"pvc\" | \"configMap\" | \"secret\"\n\t\tif type == \"pvc\"
        {\n\t\t\tclaimName: string\n\t\t}\n\t\tif type == \"configMap\" {\n\t\t\tdefaultMode:
        *420 | int\n\t\t\tcmName:      string\n\t\t\titems?: [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath:
        string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif type == \"secret\"
        {\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"emptyDir\" {\n\t\t\tmedium: *\"\" | \"Memory\"\n\t\t}\n\t}]\n\n\t//
        +usage=Instructions for assessing whether the container is alive.\n\tlivenessProbe?:
        #HealthProbe\n\n\t// +usage=Instructions for assessing whether the container
        is in a suitable state to serve traffic.\n\treadinessProbe?: #HealthProbe\n}\n#HealthProbe:
        {\n\n\t// +usage=Instructions for assessing container health by executing
        a command. Either this attribute or the httpGet attribute or the tcpSocket
        attribute MUST be specified. This attribute is mutually exclusive with both
        the httpGet attribute and the tcpSocket attribute.\n\texec?: {\n\t\t// +usage=A
        command to be executed inside the container to assess its health. Each space
        delimited token of the command is a separate array element. Commands exiting
        0 are considered to be successful probes, whilst all other exit codes are
        considered failures.\n\t\tcommand: [...string]\n\t}\n\n\t// +usage=Instructions
        for assessing container health by executing an HTTP GET request. Either this
        attribute or the exec attribute or the tcpSocket attribute MUST be specified.
        This attribute is mutually exclusive with both the exec attribute and the
        tcpSocket attribute.\n\thttpGet?: {\n\t\t// +usage=The endpoint, relative
        to the port, to which the HTTP GET request should be directed.\n\t\tpath:
        string\n\t\t// +usage=The TCP socket within the container to which the HTTP
        GET request should be directed.\n\t\tport: int\n\t\thttpHeaders?: [...{\n\t\t\tname:
        \ string\n\t\t\tvalue: string\n\t\t}]\n\t}\n\n\t// +usage=Instructions for
        assessing container health by probing a TCP socket. Either this attribute
        or the exec attribute or the httpGet attribute MUST be specified. This attribute
        is mutually exclusive with both the exec attribute and the httpGet attribute.\n\ttcpSocket?:
        {\n\t\t// +usage=The TCP socket within the container that should be probed
        to assess container health.\n\t\tport: int\n\t}\n\n\t// +usage=Number of seconds
        after the container is started before the first probe is initiated.\n\tinitialDelaySeconds:
        *0 | int\n\n\t// +usage=How often, in seconds, to execute the probe.\n\tperiodSeconds:
        *10 | int\n\n\t// +usage=Number of seconds after which the probe times out.\n\ttimeoutSeconds:
        *1 | int\n\n\t// +usage=Minimum consecutive successes for the probe to be
        considered successful after having failed.\n\tsuccessThreshold: *1 | int\n\n\t//
        +usage=Number of consecutive failures required to determine the container
        is not alive (liveness probe) or not ready (readiness probe).\n\tfailureThreshold:
        *3 | int\n}\n"
  status:
    customStatus: "status: {\n\tactive:    *0 | int\n\tfailed:    *0 | int\n\tsucceeded:
      *0 | int\n} & {\n\tif context.output.status.active != _|_ {\n\t\tactive: context.output.status.active\n\t}\n\tif
      context.output.status.failed != _|_ {\n\t\tfailed: context.output.status.failed\n\t}\n\tif
      context.output.status.succeeded != _|_ {\n\t\tsucceeded: context.output.status.succeeded\n\t}\n}\nmessage:
      \"Active/Failed/Succeeded:\\(status.active)/\\(status.failed)/\\(status.succeeded)\""
    healthPolicy: "succeeded: *0 | int\nif context.output.status.succeeded != _|_
      {\n\tsucceeded: context.output.status.succeeded\n}\nisHealth: succeeded == context.output.spec.parallelism"
  workload:
    definition:
      apiVersion: batch/v1
      kind: Job
    type: jobs.batch
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes long-running, scalable, containerized
      services that have a stable network endpoint to receive external network traffic
      from customers.
  name: webservice
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"strconv\"\n\t\"strings\"\n)\n\nmountsArray: [\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_ for v in
        parameter.volumeMounts.pvc {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for
        v in parameter.volumeMounts.configMap {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for
        v in parameter.volumeMounts.secret {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for
        v in parameter.volumeMounts.emptyDir {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for
        v in parameter.volumeMounts.hostPath {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n]\nvolumesList:
        [\n\tif parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_
        for v in parameter.volumeMounts.pvc {\n\t\t{\n\t\t\tname: v.name\n\t\t\tpersistentVolumeClaim:
        claimName: v.claimName\n\t\t}\n\t},\n\n\tif parameter.volumeMounts != _|_
        && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap
        {\n\t\t{\n\t\t\tname: v.name\n\t\t\tconfigMap: {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tname:
        \       v.cmName\n\t\t\t\tif v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for
        v in parameter.volumeMounts.secret {\n\t\t{\n\t\t\tname: v.name\n\t\t\tsecret:
        {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tsecretName:  v.secretName\n\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for
        v in parameter.volumeMounts.emptyDir {\n\t\t{\n\t\t\tname: v.name\n\t\t\temptyDir:
        medium: v.medium\n\t\t}\n\t},\n\n\tif parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath
        != _|_ for v in parameter.volumeMounts.hostPath {\n\t\t{\n\t\t\tname: v.name\n\t\t\thostPath:
        path: v.path\n\t\t}\n\t},\n]\ndeDupVolumesArray: [\n\tfor val in [\n\t\tfor
        i, vi in volumesList {\n\t\t\tfor j, vj in volumesList if j < i && vi.name
        == vj.name {\n\t\t\t\t_ignore: true\n\t\t\t}\n\t\t\tvi\n\t\t},\n\t] if val._ignore
        == _|_ {\n\t\tval\n\t},\n]\noutput: {\n\tapiVersion: \"apps/v1\"\n\tkind:
        \      \"Deployment\"\n\tspec: {\n\t\tselector: matchLabels: \"app.oam.dev/component\":
        context.name\n\n\t\ttemplate: {\n\t\t\tmetadata: {\n\t\t\t\tlabels: {\n\t\t\t\t\tif
        parameter.labels != _|_ {\n\t\t\t\t\t\tparameter.labels\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.addRevisionLabel {\n\t\t\t\t\t\t\"app.oam.dev/revision\": context.revision\n\t\t\t\t\t}\n\t\t\t\t\t\"app.oam.dev/name\":
        \     context.appName\n\t\t\t\t\t\"app.oam.dev/component\": context.name\n\t\t\t\t}\n\t\t\t\tif
        parameter.annotations != _|_ {\n\t\t\t\t\tannotations: parameter.annotations\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tspec:
        {\n\t\t\t\tcontainers: [{\n\t\t\t\t\tname:  context.name\n\t\t\t\t\timage:
        parameter.image\n\t\t\t\t\tif parameter[\"port\"] != _|_ && parameter[\"ports\"]
        == _|_ {\n\t\t\t\t\t\tports: [{\n\t\t\t\t\t\t\tcontainerPort: parameter.port\n\t\t\t\t\t\t}]\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter[\"ports\"] != _|_ {\n\t\t\t\t\t\tports: [ for v in parameter.ports
        {\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tcontainerPort: v.port\n\t\t\t\t\t\t\t\tprotocol:
        \     v.protocol\n\t\t\t\t\t\t\t\tif v.name != _|_ {\n\t\t\t\t\t\t\t\t\tname:
        v.name\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tif v.name == _|_ {\n\t\t\t\t\t\t\t\t\t_name:
        \"port-\" + strconv.FormatInt(v.port, 10)\n\t\t\t\t\t\t\t\t\tname:  *_name
        | string\n\t\t\t\t\t\t\t\t\tif v.protocol != \"TCP\" {\n\t\t\t\t\t\t\t\t\t\tname:
        _name + \"-\" + strings.ToLower(v.protocol)\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}}]\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"imagePullPolicy\"] != _|_ {\n\t\t\t\t\t\timagePullPolicy: parameter.imagePullPolicy\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cmd\"] != _|_ {\n\t\t\t\t\t\tcommand: parameter.cmd\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"args\"] != _|_ {\n\t\t\t\t\t\targs: parameter.args\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"env\"] != _|_ {\n\t\t\t\t\t\tenv: parameter.env\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        context[\"config\"] != _|_ {\n\t\t\t\t\t\tenv: context.config\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cpu\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        cpu:   parameter.cpu\n\t\t\t\t\t\t\trequests: cpu: parameter.cpu\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"memory\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        memory:   parameter.memory\n\t\t\t\t\t\t\trequests: memory: parameter.memory\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumes\"] != _|_ && parameter[\"volumeMounts\"] == _|_ {\n\t\t\t\t\t\tvolumeMounts:
        [ for v in parameter.volumes {\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tmountPath:
        v.mountPath\n\t\t\t\t\t\t\t\tname:      v.name\n\t\t\t\t\t\t\t}}]\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumeMounts\"] != _|_ {\n\t\t\t\t\t\tvolumeMounts: mountsArray\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"livenessProbe\"] != _|_ {\n\t\t\t\t\t\tlivenessProbe: parameter.livenessProbe\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"readinessProbe\"] != _|_ {\n\t\t\t\t\t\treadinessProbe: parameter.readinessProbe\n\t\t\t\t\t}\n\n\t\t\t\t}]\n\n\t\t\t\tif
        parameter[\"hostAliases\"] != _|_ {\n\t\t\t\t\t// +patchKey=ip\n\t\t\t\t\thostAliases:
        parameter.hostAliases\n\t\t\t\t}\n\n\t\t\t\tif parameter[\"imagePullSecrets\"]
        != _|_ {\n\t\t\t\t\timagePullSecrets: [ for v in parameter.imagePullSecrets
        {\n\t\t\t\t\t\tname: v\n\t\t\t\t\t},\n\t\t\t\t\t]\n\t\t\t\t}\n\n\t\t\t\tif
        parameter[\"volumes\"] != _|_ && parameter[\"volumeMounts\"] == _|_ {\n\t\t\t\t\tvolumes:
        [ for v in parameter.volumes {\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tname: v.name\n\t\t\t\t\t\t\tif
        v.type == \"pvc\" {\n\t\t\t\t\t\t\t\tpersistentVolumeClaim: claimName: v.claimName\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"configMap\" {\n\t\t\t\t\t\t\t\tconfigMap: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tname:        v.cmName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"secret\" {\n\t\t\t\t\t\t\t\tsecret: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tsecretName:  v.secretName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"emptyDir\" {\n\t\t\t\t\t\t\t\temptyDir: medium: v.medium\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t\t}\n\n\t\t\t\tif
        parameter[\"volumeMounts\"] != _|_ {\n\t\t\t\t\tvolumes: deDupVolumesArray\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nexposePorts:
        [\n\tif parameter.ports != _|_ for v in parameter.ports if v.expose == true
        {\n\t\tport:       v.port\n\t\ttargetPort: v.port\n\t\tif v.name != _|_ {\n\t\t\tname:
        v.name\n\t\t}\n\t\tif v.name == _|_ {\n\t\t\t_name: \"port-\" + strconv.FormatInt(v.port,
        10)\n\t\t\tname:  *_name | string\n\t\t\tif v.protocol != \"TCP\" {\n\t\t\t\tname:
        _name + \"-\" + strings.ToLower(v.protocol)\n\t\t\t}\n\t\t}\n\t\tif v.nodePort
        != _|_ && parameter.exposeType == \"NodePort\" {\n\t\t\tnodePort: v.nodePort\n\t\t}\n\t\tif
        v.protocol != _|_ {\n\t\t\tprotocol: v.protocol\n\t\t}\n\t},\n]\noutputs:
        {\n\tif len(exposePorts) != 0 {\n\t\twebserviceExpose: {\n\t\t\tapiVersion:
        \"v1\"\n\t\t\tkind:       \"Service\"\n\t\t\tmetadata: name: context.name\n\t\t\tspec:
        {\n\t\t\t\tselector: \"app.oam.dev/component\": context.name\n\t\t\t\tports:
        exposePorts\n\t\t\t\ttype:  parameter.exposeType\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the labels in the workload\n\tlabels?: [string]: string\n\n\t//
        +usage=Specify the annotations in the workload\n\tannotations?: [string]:
        string\n\n\t// +usage=Which image would you like to use for your service\n\t//
        +short=i\n\timage: string\n\n\t// +usage=Specify image pull policy for your
        service\n\timagePullPolicy?: \"Always\" | \"Never\" | \"IfNotPresent\"\n\n\t//
        +usage=Specify image pull secrets for your service\n\timagePullSecrets?: [...string]\n\n\t//
        +ignore\n\t// +usage=Deprecated field, please use ports instead\n\t// +short=p\n\tport?:
        int\n\n\t// +usage=Which ports do you want customer traffic sent to, defaults
        to 80\n\tports?: [...{\n\t\t// +usage=Number of port to expose on the pod's
        IP address\n\t\tport: int\n\t\t// +usage=Name of the port\n\t\tname?: string\n\t\t//
        +usage=Protocol for port. Must be UDP, TCP, or SCTP\n\t\tprotocol: *\"TCP\"
        | \"UDP\" | \"SCTP\"\n\t\t// +usage=Specify if the port should be exposed\n\t\texpose:
        *false | bool\n\t\t// +usage=exposed node port. Only Valid when exposeType
        is NodePort\n\t\tnodePort?: int\n\t}]\n\n\t// +ignore\n\t// +usage=Specify
        what kind of Service you want. options: \"ClusterIP\", \"NodePort\", \"LoadBalancer\"\n\texposeType:
        *\"ClusterIP\" | \"NodePort\" | \"LoadBalancer\"\n\n\t// +ignore\n\t// +usage=If
        addRevisionLabel is true, the revision label will be added to the underlying
        pods\n\taddRevisionLabel: *false | bool\n\n\t// +usage=Commands to run in
        the container\n\tcmd?: [...string]\n\n\t// +usage=Arguments to the entrypoint\n\targs?:
        [...string]\n\n\t// +usage=Define arguments by using environment variables\n\tenv?:
        [...{\n\t\t// +usage=Environment variable name\n\t\tname: string\n\t\t// +usage=The
        value of the environment variable\n\t\tvalue?: string\n\t\t// +usage=Specifies
        a source the value of this var should come from\n\t\tvalueFrom?: {\n\t\t\t//
        +usage=Selects a key of a secret in the pod's namespace\n\t\t\tsecretKeyRef?:
        {\n\t\t\t\t// +usage=The name of the secret in the pod's namespace to select
        from\n\t\t\t\tname: string\n\t\t\t\t// +usage=The key of the secret to select
        from. Must be a valid secret key\n\t\t\t\tkey: string\n\t\t\t}\n\t\t\t// +usage=Selects
        a key of a config map in the pod's namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t//
        +usage=The name of the config map in the pod's namespace to select from\n\t\t\t\tname:
        string\n\t\t\t\t// +usage=The key of the config map to select from. Must be
        a valid secret key\n\t\t\t\tkey: string\n\t\t\t}\n\t\t}\n\t}]\n\n\t// +usage=Number
        of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)\n\tcpu?:
        string\n\n\t// +usage=Specifies the attributes of the memory resource required
        for the container.\n\tmemory?: string\n\n\tvolumeMounts?: {\n\t\t// +usage=Mount
        PVC type volume\n\t\tpvc?: [...{\n\t\t\tname:      string\n\t\t\tmountPath:
        string\n\t\t\tsubPath?:  string\n\t\t\t// +usage=The name of the PVC\n\t\t\tclaimName:
        string\n\t\t}]\n\t\t// +usage=Mount ConfigMap type volume\n\t\tconfigMap?:
        [...{\n\t\t\tname:        string\n\t\t\tmountPath:   string\n\t\t\tsubPath?:
        \   string\n\t\t\tdefaultMode: *420 | int\n\t\t\tcmName:      string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}]\n\t\t//
        +usage=Mount Secret type volume\n\t\tsecret?: [...{\n\t\t\tname:        string\n\t\t\tmountPath:
        \  string\n\t\t\tsubPath?:    string\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:
        \ string\n\t\t\titems?: [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode:
        *511 | int\n\t\t\t}]\n\t\t}]\n\t\t// +usage=Mount EmptyDir type volume\n\t\temptyDir?:
        [...{\n\t\t\tname:      string\n\t\t\tmountPath: string\n\t\t\tsubPath?:  string\n\t\t\tmedium:
        \   *\"\" | \"Memory\"\n\t\t}]\n\t\t// +usage=Mount HostPath type volume\n\t\thostPath?:
        [...{\n\t\t\tname:      string\n\t\t\tmountPath: string\n\t\t\tsubPath?:  string\n\t\t\tpath:
        \     string\n\t\t}]\n\t}\n\n\t// +usage=Deprecated field, use volumeMounts
        instead.\n\tvolumes?: [...{\n\t\tname:      string\n\t\tmountPath: string\n\t\t//
        +usage=Specify volume type, options: \"pvc\",\"configMap\",\"secret\",\"emptyDir\",
        default to emptyDir\n\t\ttype: *\"emptyDir\" | \"pvc\" | \"configMap\" | \"secret\"\n\t\tif
        type == \"pvc\" {\n\t\t\tclaimName: string\n\t\t}\n\t\tif type == \"configMap\"
        {\n\t\t\tdefaultMode: *420 | int\n\t\t\tcmName:      string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"secret\" {\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"emptyDir\" {\n\t\t\tmedium: *\"\" | \"Memory\"\n\t\t}\n\t}]\n\n\t//
        +usage=Instructions for assessing whether the container is alive.\n\tlivenessProbe?:
        #HealthProbe\n\n\t// +usage=Instructions for assessing whether the container
        is in a suitable state to serve traffic.\n\treadinessProbe?: #HealthProbe\n\n\t//
        +usage=Specify the hostAliases to add\n\thostAliases?: [...{\n\t\tip: string\n\t\thostnames:
        [...string]\n\t}]\n}\n#HealthProbe: {\n\n\t// +usage=Instructions for assessing
        container health by executing a command. Either this attribute or the httpGet
        attribute or the tcpSocket attribute MUST be specified. This attribute is
        mutually exclusive with both the httpGet attribute and the tcpSocket attribute.\n\texec?:
        {\n\t\t// +usage=A command to be executed inside the container to assess its
        health. Each space delimited token of the command is a separate array element.
        Commands exiting 0 are considered to be successful probes, whilst all other
        exit codes are considered failures.\n\t\tcommand: [...string]\n\t}\n\n\t//
        +usage=Instructions for assessing container health by executing an HTTP GET
        request. Either this attribute or the exec attribute or the tcpSocket attribute
        MUST be specified. This attribute is mutually exclusive with both the exec
        attribute and the tcpSocket attribute.\n\thttpGet?: {\n\t\t// +usage=The endpoint,
        relative to the port, to which the HTTP GET request should be directed.\n\t\tpath:
        string\n\t\t// +usage=The TCP socket within the container to which the HTTP
        GET request should be directed.\n\t\tport:    int\n\t\thost?:   string\n\t\tscheme?:
        *\"HTTP\" | string\n\t\thttpHeaders?: [...{\n\t\t\tname:  string\n\t\t\tvalue:
        string\n\t\t}]\n\t}\n\n\t// +usage=Instructions for assessing container health
        by probing a TCP socket. Either this attribute or the exec attribute or the
        httpGet attribute MUST be specified. This attribute is mutually exclusive
        with both the exec attribute and the httpGet attribute.\n\ttcpSocket?: {\n\t\t//
        +usage=The TCP socket within the container that should be probed to assess
        container health.\n\t\tport: int\n\t}\n\n\t// +usage=Number of seconds after
        the container is started before the first probe is initiated.\n\tinitialDelaySeconds:
        *0 | int\n\n\t// +usage=How often, in seconds, to execute the probe.\n\tperiodSeconds:
        *10 | int\n\n\t// +usage=Number of seconds after which the probe times out.\n\ttimeoutSeconds:
        *1 | int\n\n\t// +usage=Minimum consecutive successes for the probe to be
        considered successful after having failed.\n\tsuccessThreshold: *1 | int\n\n\t//
        +usage=Number of consecutive failures required to determine the container
        is not alive (liveness probe) or not ready (readiness probe).\n\tfailureThreshold:
        *3 | int\n}\n"
  status:
    customStatus: "ready: {\n\treadyReplicas: *0 | int\n} & {\n\tif context.output.status.readyReplicas
      != _|_ {\n\t\treadyReplicas: context.output.status.readyReplicas\n\t}\n}\nmessage:
      \"Ready:\\(ready.readyReplicas)/\\(context.output.spec.replicas)\""
    healthPolicy: "ready: {\n\tupdatedReplicas:    *0 | int\n\treadyReplicas:      *0
      | int\n\treplicas:           *0 | int\n\tobservedGeneration: *0 | int\n} & {\n\tif
      context.output.status.updatedReplicas != _|_ {\n\t\tupdatedReplicas: context.output.status.updatedReplicas\n\t}\n\tif
      context.output.status.readyReplicas != _|_ {\n\t\treadyReplicas: context.output.status.readyReplicas\n\t}\n\tif
      context.output.status.replicas != _|_ {\n\t\treplicas: context.output.status.replicas\n\t}\n\tif
      context.output.status.observedGeneration != _|_ {\n\t\tobservedGeneration: context.output.status.observedGeneration\n\t}\n}\nisHealth:
      (context.output.spec.replicas == ready.readyReplicas) && (context.output.spec.replicas
      == ready.updatedReplicas) && (context.output.spec.replicas == ready.replicas)
      && (ready.observedGeneration == context.output.metadata.generation || ready.observedGeneration
      > context.output.metadata.generation)"
  workload:
    definition:
      apiVersion: apps/v1
      kind: Deployment
    type: deployments.apps
---
apiVersion: core.oam.dev/v1beta1
kind: ComponentDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describes long-running, scalable, containerized
      services that running at backend. They do NOT have network endpoint to receive
      external network traffic.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: worker
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "mountsArray: [\n\tif parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc
        != _|_ for v in parameter.volumeMounts.pvc {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.configMap != _|_ for
        v in parameter.volumeMounts.configMap {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for
        v in parameter.volumeMounts.secret {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for
        v in parameter.volumeMounts.emptyDir {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath != _|_ for
        v in parameter.volumeMounts.hostPath {\n\t\t{\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t\tname: v.name\n\t\t}\n\t},\n]\nvolumesList:
        [\n\tif parameter.volumeMounts != _|_ && parameter.volumeMounts.pvc != _|_
        for v in parameter.volumeMounts.pvc {\n\t\t{\n\t\t\tname: v.name\n\t\t\tpersistentVolumeClaim:
        claimName: v.claimName\n\t\t}\n\t},\n\n\tif parameter.volumeMounts != _|_
        && parameter.volumeMounts.configMap != _|_ for v in parameter.volumeMounts.configMap
        {\n\t\t{\n\t\t\tname: v.name\n\t\t\tconfigMap: {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tname:
        \       v.cmName\n\t\t\t\tif v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.secret != _|_ for
        v in parameter.volumeMounts.secret {\n\t\t{\n\t\t\tname: v.name\n\t\t\tsecret:
        {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tsecretName:  v.secretName\n\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\n\tif
        parameter.volumeMounts != _|_ && parameter.volumeMounts.emptyDir != _|_ for
        v in parameter.volumeMounts.emptyDir {\n\t\t{\n\t\t\tname: v.name\n\t\t\temptyDir:
        medium: v.medium\n\t\t}\n\t},\n\n\tif parameter.volumeMounts != _|_ && parameter.volumeMounts.hostPath
        != _|_ for v in parameter.volumeMounts.hostPath {\n\t\t{\n\t\t\tname: v.name\n\t\t\thostPath:
        path: v.path\n\t\t}\n\t},\n]\ndeDupVolumesArray: [\n\tfor val in [\n\t\tfor
        i, vi in volumesList {\n\t\t\tfor j, vj in volumesList if j < i && vi.name
        == vj.name {\n\t\t\t\t_ignore: true\n\t\t\t}\n\t\t\tvi\n\t\t},\n\t] if val._ignore
        == _|_ {\n\t\tval\n\t},\n]\noutput: {\n\tapiVersion: \"apps/v1\"\n\tkind:
        \      \"Deployment\"\n\tspec: {\n\t\tselector: matchLabels: \"app.oam.dev/component\":
        context.name\n\n\t\ttemplate: {\n\t\t\tmetadata: labels: {\n\t\t\t\t\"app.oam.dev/name\":
        \     context.appName\n\t\t\t\t\"app.oam.dev/component\": context.name\n\t\t\t}\n\n\t\t\tspec:
        {\n\t\t\t\tcontainers: [{\n\t\t\t\t\tname:  context.name\n\t\t\t\t\timage:
        parameter.image\n\n\t\t\t\t\tif parameter[\"imagePullPolicy\"] != _|_ {\n\t\t\t\t\t\timagePullPolicy:
        parameter.imagePullPolicy\n\t\t\t\t\t}\n\n\t\t\t\t\tif parameter[\"cmd\"]
        != _|_ {\n\t\t\t\t\t\tcommand: parameter.cmd\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"env\"] != _|_ {\n\t\t\t\t\t\tenv: parameter.env\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"cpu\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        cpu:   parameter.cpu\n\t\t\t\t\t\t\trequests: cpu: parameter.cpu\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"memory\"] != _|_ {\n\t\t\t\t\t\tresources: {\n\t\t\t\t\t\t\tlimits:
        memory:   parameter.memory\n\t\t\t\t\t\t\trequests: memory: parameter.memory\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumes\"] != _|_ && parameter[\"volumeMounts\"] == _|_ {\n\t\t\t\t\t\tvolumeMounts:
        [ for v in parameter.volumes {\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tmountPath:
        v.mountPath\n\t\t\t\t\t\t\t\tname:      v.name\n\t\t\t\t\t\t\t}}]\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"volumeMounts\"] != _|_ {\n\t\t\t\t\t\tvolumeMounts: mountsArray\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"livenessProbe\"] != _|_ {\n\t\t\t\t\t\tlivenessProbe: parameter.livenessProbe\n\t\t\t\t\t}\n\n\t\t\t\t\tif
        parameter[\"readinessProbe\"] != _|_ {\n\t\t\t\t\t\treadinessProbe: parameter.readinessProbe\n\t\t\t\t\t}\n\n\t\t\t\t}]\n\n\t\t\t\tif
        parameter[\"imagePullSecrets\"] != _|_ {\n\t\t\t\t\timagePullSecrets: [ for
        v in parameter.imagePullSecrets {\n\t\t\t\t\t\tname: v\n\t\t\t\t\t},\n\t\t\t\t\t]\n\t\t\t\t}\n\n\t\t\t\tif
        parameter[\"volumes\"] != _|_ && parameter[\"volumeMounts\"] == _|_ {\n\t\t\t\t\tvolumes:
        [ for v in parameter.volumes {\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tname: v.name\n\t\t\t\t\t\t\tif
        v.type == \"pvc\" {\n\t\t\t\t\t\t\t\tpersistentVolumeClaim: claimName: v.claimName\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"configMap\" {\n\t\t\t\t\t\t\t\tconfigMap: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tname:        v.cmName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"secret\" {\n\t\t\t\t\t\t\t\tsecret: {\n\t\t\t\t\t\t\t\t\tdefaultMode:
        v.defaultMode\n\t\t\t\t\t\t\t\t\tsecretName:  v.secretName\n\t\t\t\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        v.type == \"emptyDir\" {\n\t\t\t\t\t\t\t\temptyDir: medium: v.medium\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t\tif
        parameter[\"volumeMounts\"] != _|_ {\n\t\t\t\t\tvolumes: deDupVolumesArray\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Which image would you like to use for your service\n\t// +short=i\n\timage:
        string\n\n\t// +usage=Specify image pull policy for your service\n\timagePullPolicy?:
        string\n\n\t// +usage=Specify image pull secrets for your service\n\timagePullSecrets?:
        [...string]\n\n\t// +usage=Commands to run in the container\n\tcmd?: [...string]\n\n\t//
        +usage=Define arguments by using environment variables\n\tenv?: [...{\n\t\t//
        +usage=Environment variable name\n\t\tname: string\n\t\t// +usage=The value
        of the environment variable\n\t\tvalue?: string\n\t\t// +usage=Specifies a
        source the value of this var should come from\n\t\tvalueFrom?: {\n\t\t\t//
        +usage=Selects a key of a secret in the pod's namespace\n\t\t\tsecretKeyRef?:
        {\n\t\t\t\t// +usage=The name of the secret in the pod's namespace to select
        from\n\t\t\t\tname: string\n\t\t\t\t// +usage=The key of the secret to select
        from. Must be a valid secret key\n\t\t\t\tkey: string\n\t\t\t}\n\t\t\t// +usage=Selects
        a key of a config map in the pod's namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t//
        +usage=The name of the config map in the pod's namespace to select from\n\t\t\t\tname:
        string\n\t\t\t\t// +usage=The key of the config map to select from. Must be
        a valid secret key\n\t\t\t\tkey: string\n\t\t\t}\n\t\t}\n\t}]\n\n\t// +usage=Number
        of CPU units for the service, like `0.5` (0.5 CPU core), `1` (1 CPU core)\n\tcpu?:
        string\n\n\t// +usage=Specifies the attributes of the memory resource required
        for the container.\n\tmemory?: string\n\n\tvolumeMounts?: {\n\t\t// +usage=Mount
        PVC type volume\n\t\tpvc?: [...{\n\t\t\tname:      string\n\t\t\tmountPath:
        string\n\t\t\t// +usage=The name of the PVC\n\t\t\tclaimName: string\n\t\t}]\n\t\t//
        +usage=Mount ConfigMap type volume\n\t\tconfigMap?: [...{\n\t\t\tname:        string\n\t\t\tmountPath:
        \  string\n\t\t\tdefaultMode: *420 | int\n\t\t\tcmName:      string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}]\n\t\t//
        +usage=Mount Secret type volume\n\t\tsecret?: [...{\n\t\t\tname:        string\n\t\t\tmountPath:
        \  string\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}]\n\t\t//
        +usage=Mount EmptyDir type volume\n\t\temptyDir?: [...{\n\t\t\tname:      string\n\t\t\tmountPath:
        string\n\t\t\tmedium:    *\"\" | \"Memory\"\n\t\t}]\n\t\t// +usage=Mount HostPath
        type volume\n\t\thostPath?: [...{\n\t\t\tname:      string\n\t\t\tmountPath:
        string\n\t\t\tpath:      string\n\t\t}]\n\t}\n\n\t// +usage=Deprecated field,
        use volumeMounts instead.\n\tvolumes?: [...{\n\t\tname:      string\n\t\tmountPath:
        string\n\t\t// +usage=Specify volume type, options: \"pvc\",\"configMap\",\"secret\",\"emptyDir\",
        default to emptyDir\n\t\ttype: *\"emptyDir\" | \"pvc\" | \"configMap\" | \"secret\"\n\t\tif
        type == \"pvc\" {\n\t\t\tclaimName: string\n\t\t}\n\t\tif type == \"configMap\"
        {\n\t\t\tdefaultMode: *420 | int\n\t\t\tcmName:      string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"secret\" {\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"emptyDir\" {\n\t\t\tmedium: *\"\" | \"Memory\"\n\t\t}\n\t}]\n\n\t//
        +usage=Instructions for assessing whether the container is alive.\n\tlivenessProbe?:
        #HealthProbe\n\n\t// +usage=Instructions for assessing whether the container
        is in a suitable state to serve traffic.\n\treadinessProbe?: #HealthProbe\n}\n#HealthProbe:
        {\n\n\t// +usage=Instructions for assessing container health by executing
        a command. Either this attribute or the httpGet attribute or the tcpSocket
        attribute MUST be specified. This attribute is mutually exclusive with both
        the httpGet attribute and the tcpSocket attribute.\n\texec?: {\n\t\t// +usage=A
        command to be executed inside the container to assess its health. Each space
        delimited token of the command is a separate array element. Commands exiting
        0 are considered to be successful probes, whilst all other exit codes are
        considered failures.\n\t\tcommand: [...string]\n\t}\n\n\t// +usage=Instructions
        for assessing container health by executing an HTTP GET request. Either this
        attribute or the exec attribute or the tcpSocket attribute MUST be specified.
        This attribute is mutually exclusive with both the exec attribute and the
        tcpSocket attribute.\n\thttpGet?: {\n\t\t// +usage=The endpoint, relative
        to the port, to which the HTTP GET request should be directed.\n\t\tpath:
        string\n\t\t// +usage=The TCP socket within the container to which the HTTP
        GET request should be directed.\n\t\tport: int\n\t\thttpHeaders?: [...{\n\t\t\tname:
        \ string\n\t\t\tvalue: string\n\t\t}]\n\t}\n\n\t// +usage=Instructions for
        assessing container health by probing a TCP socket. Either this attribute
        or the exec attribute or the httpGet attribute MUST be specified. This attribute
        is mutually exclusive with both the exec attribute and the httpGet attribute.\n\ttcpSocket?:
        {\n\t\t// +usage=The TCP socket within the container that should be probed
        to assess container health.\n\t\tport: int\n\t}\n\n\t// +usage=Number of seconds
        after the container is started before the first probe is initiated.\n\tinitialDelaySeconds:
        *0 | int\n\n\t// +usage=How often, in seconds, to execute the probe.\n\tperiodSeconds:
        *10 | int\n\n\t// +usage=Number of seconds after which the probe times out.\n\ttimeoutSeconds:
        *1 | int\n\n\t// +usage=Minimum consecutive successes for the probe to be
        considered successful after having failed.\n\tsuccessThreshold: *1 | int\n\n\t//
        +usage=Number of consecutive failures required to determine the container
        is not alive (liveness probe) or not ready (readiness probe).\n\tfailureThreshold:
        *3 | int\n}\n"
  status:
    customStatus: "ready: {\n\treadyReplicas: *0 | int\n} & {\n\tif context.output.status.readyReplicas
      != _|_ {\n\t\treadyReplicas: context.output.status.readyReplicas\n\t}\n}\nmessage:
      \"Ready:\\(ready.readyReplicas)/\\(context.output.spec.replicas)\""
    healthPolicy: "ready: {\n\tupdatedReplicas:    *0 | int\n\treadyReplicas:      *0
      | int\n\treplicas:           *0 | int\n\tobservedGeneration: *0 | int\n} & {\n\tif
      context.output.status.updatedReplicas != _|_ {\n\t\tupdatedReplicas: context.output.status.updatedReplicas\n\t}\n\tif
      context.output.status.readyReplicas != _|_ {\n\t\treadyReplicas: context.output.status.readyReplicas\n\t}\n\tif
      context.output.status.replicas != _|_ {\n\t\treplicas: context.output.status.replicas\n\t}\n\tif
      context.output.status.observedGeneration != _|_ {\n\t\tobservedGeneration: context.output.status.observedGeneration\n\t}\n}\nisHealth:
      (context.output.spec.replicas == ready.readyReplicas) && (context.output.spec.replicas
      == ready.updatedReplicas) && (context.output.spec.replicas == ready.replicas)
      && (ready.observedGeneration == context.output.metadata.generation || ready.observedGeneration
      > context.output.metadata.generation)"
  workload:
    definition:
      apiVersion: apps/v1
      kind: Deployment
    type: deployments.apps
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: kubevela-vela-core-admission
  namespace: vela-system
webhooks:
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /mutating-core-oam-dev-v1beta1-approllout
  failurePolicy: Ignore
  name: mutating.core.oam.dev.v1beta1.approllouts
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    resources:
    - approllouts
    scope: Namespaced
  sideEffects: None
  timeoutSeconds: 5
- admissionReviewVersions:
  - v1beta1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /mutate-standard-oam-dev-v1alpha1-podspecworkload
  failurePolicy: Ignore
  name: mcontainerized.kb.io
  rules:
  - apiGroups:
    - standard.oam.dev
    apiVersions:
    - v1alpha1
    operations:
    - CREATE
    - UPDATE
    resources:
    - podspecworkloads
  sideEffects: None
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /mutating-core-oam-dev-v1beta1-applications
  failurePolicy: Ignore
  name: mutating.core.oam.dev.v1beta1.applications
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    resources:
    - applications
  sideEffects: None
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /mutating-core-oam-dev-v1beta1-componentdefinitions
  failurePolicy: Ignore
  name: mutating.core.oam-dev.v1beta1.componentdefinitions
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    resources:
    - componentdefinitions
  sideEffects: None
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Allow configuration drift for applied resources,
      delivery the resource without continuously reconciliation.
  name: apply-once
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#ApplyOnceStrategy: {\n\t// +usage=When the strategy takes effect,e.g.
        onUpdateonStateKeep\n\taffect?: string\n\t// +usage=Specify the path of the
        resource that allow configuration drift\n\tpath: [...string]\n}\n#ApplyOncePolicyRule:
        {\n\t// +usage=Specify how to select the targets of the rule\n\tselector?:
        #ResourcePolicyRuleSelector\n\t// +usage=Specify the strategy for configuring
        the resource level configuration drift behaviour\n\tstrategy: #ApplyOnceStrategy\n}\n#ResourcePolicyRuleSelector:
        {\n\t// +usage=Select resources by component names\n\tcomponentNames?: [...string]\n\t//
        +usage=Select resources by component types\n\tcomponentTypes?: [...string]\n\t//
        +usage=Select resources by oamTypes (COMPONENT or TRAIT)\n\toamTypes?: [...string]\n\t//
        +usage=Select resources by trait types\n\ttraitTypes?: [...string]\n\t// +usage=Select
        resources by resource types (like Deployment)\n\tresourceTypes?: [...string]\n\t//
        +usage=Select resources by their names\n\tresourceNames?: [...string]\n}\nparameter:
        {\n\t// +usage=Whether to enable apply-once for the whole application\n\tenable:
        *false | bool\n\t// +usage=Specify the rules for configuring apply-once policy
        in resource level\n\trules?: [...#ApplyOncePolicyRule]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Determining the destination where components should
      be deployed to, and support override configuration
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: envbinding
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#PatchParams: {\n\t// +usage=Specify the name of the patch component,
        if empty, all components will be merged\n\tname?: string\n\t// +usage=Specify
        the type of the patch component.\n\ttype?: string\n\tproperties?: {...}\n\ttraits?:
        [...{\n\t\ttype: string\n\t\tproperties?: {...}\n\t\t// +usage=Specify if
        the trait should be remove, default false\n\t\tdisable: *false | bool\n\t}]\n}\nparameter:
        envs: [...{\n\tname: string\n\tplacement?: {\n\t\tclusterSelector?: {\n\t\t\t//
        +usage=Specify cluster name, defualt local\n\t\t\tname: *\"local\" | string\n\t\t\tlabels?:
        [string]: string\n\t\t}\n\t\tnamespaceSelector?: {\n\t\t\t// +usage=Specify
        namespace name.\n\t\t\tname?: string\n\t\t\tlabels?: [string]: string\n\t\t}\n\t}\n\tselector?:
        components: [...string]\n\tpatch?: components: [...#PatchParams]\n}]\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the garbage collect behaviour for the
      application.
  name: garbage-collect
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#GarbageCollectPolicyRule: {\n\t// +usage=Specify how to select the
        targets of the rule\n\tselector: [...#ResourcePolicyRuleSelector]\n\t// +usage=Specify
        the strategy for target resource to recycle\n\tstrategy: *\"onAppUpdate\"
        | \"onAppDelete\" | \"never\"\n}\n#ResourcePolicyRuleSelector: {\n\t// +usage=Select
        resources by component names\n\tcomponentNames?: [...string]\n\t// +usage=Select
        resources by component types\n\tcomponentTypes?: [...string]\n\t// +usage=Select
        resources by oamTypes (COMPONENT or TRAIT)\n\toamTypes?: [...string]\n\t//
        +usage=Select resources by trait types\n\ttraitTypes?: [...string]\n\t// +usage=Select
        resources by resource types (like Deployment)\n\tresourceTypes?: [...string]\n\t//
        +usage=Select resources by their names\n\tresourceNames?: [...string]\n}\nparameter:
        {\n\t// +usage=If is set, outdated versioned resourcetracker will not be recycled
        automatically, outdated resources will be kept until resourcetracker be deleted
        manually\n\tkeepLegacyResource: *false | bool\n\t// +usage=Specify the list
        of rules to control gc strategy at resource level, if one resource is controlled
        by multiple rules, first rule will be used\n\trules?: [...#GarbageCollectPolicyRule]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply periodical health checking to the application.
  name: health
  namespace: vela-system
spec:
  manageHealthCheck: true
  schematic:
    cue:
      template: "output: {\n\tapiVersion: \"core.oam.dev/v1alpha2\"\n\tkind:       \"HealthScope\"\n\tspec:
        {\n\t\t\"probe-timeout\":  parameter.probeTimeout\n\t\t\"probe-interval\":
        parameter.probeInterval\n\t\tappReferences: [{\n\t\t\tappName: context.appName\n\t\t}]\n\t\tworkloadRefs:
        []\n\t\tmanageHealthCheck: true\n\t}\n}\nparameter: {\n\t// +usage=Specify
        health checking timeout(seconds), default 10s\n\tprobeTimeout: *10 | int\n\t//
        +usage=Specify health checking interval(seconds), default 30s\n\tprobeInterval:
        *30 | int\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describe the configuration to override when deploying
      resources, it only works with specified `deploy` step in workflow.
  name: override
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#PatchParams: {\n\t// +usage=Specify the name of the patch component,
        if empty, all components will be merged\n\tname?: string\n\t// +usage=Specify
        the type of the patch component.\n\ttype?: string\n\t// +usage=Specify the
        properties to override.\n\tproperties?: {...}\n\t// +usage=Specify the traits
        to override.\n\ttraits?: [...{\n\t\t// +usage=Specify the type of the trait
        to be patched.\n\t\ttype: string\n\t\t// +usage=Specify the properties to
        override.\n\t\tproperties?: {...}\n\t\t// +usage=Specify if the trait should
        be remove, default false\n\t\tdisable: *false | bool\n\t}]\n}\nparameter:
        {\n\t// +usage=Specify the overridden component configuration.\n\tcomponents:
        [...#PatchParams]\n\t// +usage=Specify a list of component names to use, if
        empty, all components will be selected.\n\tselector?: [...string]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the resources to be read-only in the
      application (no update / state-keep).
  name: read-only
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#PolicyRule: {\n\t// +usage=Specify how to select the targets of
        the rule\n\tselector: #RuleSelector\n}\n#RuleSelector: {\n\t// +usage=Select
        resources by component names\n\tcomponentNames?: [...string]\n\t// +usage=Select
        resources by component types\n\tcomponentTypes?: [...string]\n\t// +usage=Select
        resources by oamTypes (COMPONENT or TRAIT)\n\toamTypes?: [...string]\n\t//
        +usage=Select resources by trait types\n\ttraitTypes?: [...string]\n\t// +usage=Select
        resources by resource types (like Deployment)\n\tresourceTypes?: [...string]\n\t//
        +usage=Select resources by their names\n\tresourceNames?: [...string]\n}\nparameter:
        {\n\t// +usage=Specify the list of rules to control read only strategy at
        resource level.\n\t// The selected resource will be read-only to the current
        application. If the target resource does\n\t// not exist, error will be raised.\n\trules?:
        [...#PolicyRule]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describe the configuration to replicate components
      when deploying resources, it only works with specified `deploy` step in workflow.
  name: replication
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "parameter: {\n\t// +usage=Spicify the keys of replication. Every
        key coresponds to a replication components\n\tkeys: [...string]\n\t// +usage=Specify
        the components which will be replicated.\n\tselector?: [...string]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the resources to be sharable across
      applications.
  name: shared-resource
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#SharedResourcePolicyRule: {\n\t// +usage=Specify how to select the
        targets of the rule\n\tselector: [...#ResourcePolicyRuleSelector]\n}\n#ResourcePolicyRuleSelector:
        {\n\t// +usage=Select resources by component names\n\tcomponentNames?: [...string]\n\t//
        +usage=Select resources by component types\n\tcomponentTypes?: [...string]\n\t//
        +usage=Select resources by oamTypes (COMPONENT or TRAIT)\n\toamTypes?: [...string]\n\t//
        +usage=Select resources by trait types\n\ttraitTypes?: [...string]\n\t// +usage=Select
        resources by resource types (like Deployment)\n\tresourceTypes?: [...string]\n\t//
        +usage=Select resources by their names\n\tresourceNames?: [...string]\n}\nparameter:
        {\n\t// +usage=Specify the list of rules to control shared-resource strategy
        at resource level.\n\t// The selected resource will be sharable across applications.
        (That means multiple applications\n\t// can all read it without conflict,
        but only the first one can write it)\n\trules?: [...#SharedResourcePolicyRule]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure the resources to be able to take over
      when it belongs to no application.
  name: take-over
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "#PolicyRule: {\n\t// +usage=Specify how to select the targets of
        the rule\n\tselector: [...#RuleSelector]\n}\n#RuleSelector: {\n\t// +usage=Select
        resources by component names\n\tcomponentNames?: [...string]\n\t// +usage=Select
        resources by component types\n\tcomponentTypes?: [...string]\n\t// +usage=Select
        resources by oamTypes (COMPONENT or TRAIT)\n\toamTypes?: [...string]\n\t//
        +usage=Select resources by trait types\n\ttraitTypes?: [...string]\n\t// +usage=Select
        resources by resource types (like Deployment)\n\tresourceTypes?: [...string]\n\t//
        +usage=Select resources by their names\n\tresourceNames?: [...string]\n}\nparameter:
        {\n\t// +usage=Specify the list of rules to control take over strategy at
        resource level.\n\t// The selected resource will be able to be taken over
        by the current application when the resource belongs to no\n\t// one.\n\trules?:
        [...#PolicyRule]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: PolicyDefinition
metadata:
  annotations:
    definition.oam.dev/description: Describe the destination where components should
      be deployed to.
  name: topology
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "parameter: {\n\t// +usage=Specify the names of the clusters to select.\n\tclusters?:
        [...string]\n\t// +usage=Specify the label selector for clusters\n\tclusterLabelSelector?:
        [string]: string\n\t// +usage=Ignore empty cluster error\n\tallowEmpty?: bool\n\t//
        +usage=Deprecated: Use clusterLabelSelector instead.\n\tclusterSelector?:
        [string]: string\n\t// +usage=Specify the target namespace to deploy in the
        selected clusters, default inherit the original namespace.\n\tnamespace?:
        string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: ScopeDefinition
metadata:
  name: healthscopes.core.oam.dev
  namespace: vela-system
spec:
  allowComponentOverlap: true
  definitionRef:
    name: healthscopes.core.oam.dev
  workloadRefsPath: spec.workloadRefs
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Affinity specifies affinity and toleration K8s
      pod for your workload which follows the pod spec in path 'spec.template'.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: affinity
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: {\n\tif parameter.podAffinity != _|_
        {\n\t\taffinity: podAffinity: {\n\t\t\tif parameter.podAffinity.required !=
        _|_ {\n\t\t\t\trequiredDuringSchedulingIgnoredDuringExecution: [\n\t\t\t\t\tfor
        k in parameter.podAffinity.required {\n\t\t\t\t\t\tif k.labelSelector != _|_
        {\n\t\t\t\t\t\t\tlabelSelector: k.labelSelector\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        k.namespace != _|_ {\n\t\t\t\t\t\t\tnamespace: k.namespace\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttopologyKey:
        k.topologyKey\n\t\t\t\t\t\tif k.namespaceSelector != _|_ {\n\t\t\t\t\t\t\tnamespaceSelector:
        k.namespaceSelector\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t}\n\t\t\tif parameter.podAffinity.preferred
        != _|_ {\n\t\t\t\tpreferredDuringSchedulingIgnoredDuringExecution: [\n\t\t\t\t\tfor
        k in parameter.podAffinity.preferred {\n\t\t\t\t\t\tweight:          k.weight\n\t\t\t\t\t\tpodAffinityTerm:
        k.podAffinityTerm\n\t\t\t\t\t}]\n\t\t\t}\n\t\t}\n\t}\n\tif parameter.podAntiAffinity
        != _|_ {\n\t\taffinity: podAntiAffinity: {\n\t\t\tif parameter.podAntiAffinity.required
        != _|_ {\n\t\t\t\trequiredDuringSchedulingIgnoredDuringExecution: [\n\t\t\t\t\tfor
        k in parameter.podAntiAffinity.required {\n\t\t\t\t\t\tif k.labelSelector
        != _|_ {\n\t\t\t\t\t\t\tlabelSelector: k.labelSelector\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        k.namespace != _|_ {\n\t\t\t\t\t\t\tnamespace: k.namespace\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttopologyKey:
        k.topologyKey\n\t\t\t\t\t\tif k.namespaceSelector != _|_ {\n\t\t\t\t\t\t\tnamespaceSelector:
        k.namespaceSelector\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t}\n\t\t\tif parameter.podAntiAffinity.preferred
        != _|_ {\n\t\t\t\tpreferredDuringSchedulingIgnoredDuringExecution: [\n\t\t\t\t\tfor
        k in parameter.podAntiAffinity.preferred {\n\t\t\t\t\t\tweight:          k.weight\n\t\t\t\t\t\tpodAffinityTerm:
        k.podAffinityTerm\n\t\t\t\t\t}]\n\t\t\t}\n\t\t}\n\t}\n\tif parameter.nodeAffinity
        != _|_ {\n\t\taffinity: nodeAffinity: {\n\t\t\tif parameter.nodeAffinity.required
        != _|_ {\n\t\t\t\trequiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms:
        [\n\t\t\t\t\tfor k in parameter.nodeAffinity.required.nodeSelectorTerms {\n\t\t\t\t\t\tif
        k.matchExpressions != _|_ {\n\t\t\t\t\t\t\tmatchExpressions: k.matchExpressions\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        k.matchFields != _|_ {\n\t\t\t\t\t\t\tmatchFields: k.matchFields\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t}\n\t\t\tif
        parameter.nodeAffinity.preferred != _|_ {\n\t\t\t\tpreferredDuringSchedulingIgnoredDuringExecution:
        [\n\t\t\t\t\tfor k in parameter.nodeAffinity.preferred {\n\t\t\t\t\t\tweight:
        \    k.weight\n\t\t\t\t\t\tpreference: k.preference\n\t\t\t\t\t}]\n\t\t\t}\n\t\t}\n\t}\n\tif
        parameter.tolerations != _|_ {\n\t\ttolerations: [\n\t\t\tfor k in parameter.tolerations
        {\n\t\t\t\tif k.key != _|_ {\n\t\t\t\t\tkey: k.key\n\t\t\t\t}\n\t\t\t\tif
        k.effect != _|_ {\n\t\t\t\t\teffect: k.effect\n\t\t\t\t}\n\t\t\t\tif k.value
        != _|_ {\n\t\t\t\t\tvalue: k.value\n\t\t\t\t}\n\t\t\t\toperator: k.operator\n\t\t\t\tif
        k.tolerationSeconds != _|_ {\n\t\t\t\t\ttolerationSeconds: k.tolerationSeconds\n\t\t\t\t}\n\t\t\t}]\n\t}\n}\n#labelSelector:
        {\n\tmatchLabels?: [string]: string\n\tmatchExpressions?: [...{\n\t\tkey:
        \     string\n\t\toperator: *\"In\" | \"NotIn\" | \"Exists\" | \"DoesNotExist\"\n\t\tvalues?:
        [...string]\n\t}]\n}\n#podAffinityTerm: {\n\tlabelSelector?: #labelSelector\n\tnamespaces?:
        [...string]\n\ttopologyKey:        string\n\tnamespaceSelector?: #labelSelector\n}\n#nodeSelecor:
        {\n\tkey:      string\n\toperator: *\"In\" | \"NotIn\" | \"Exists\" | \"DoesNotExist\"
        | \"Gt\" | \"Lt\"\n\tvalues?: [...string]\n}\n#nodeSelectorTerm: {\n\tmatchExpressions?:
        [...#nodeSelecor]\n\tmatchFields?: [...#nodeSelecor]\n}\nparameter: {\n\t//
        +usage=Specify the pod affinity scheduling rules\n\tpodAffinity?: {\n\t\t//
        +usage=Specify the required during scheduling ignored during execution\n\t\trequired?:
        [...#podAffinityTerm]\n\t\t// +usage=Specify the preferred during scheduling
        ignored during execution\n\t\tpreferred?: [...{\n\t\t\t// +usage=Specify weight
        associated with matching the corresponding podAffinityTerm\n\t\t\tweight:
        int & >=1 & <=100\n\t\t\t// +usage=Specify a set of pods\n\t\t\tpodAffinityTerm:
        #podAffinityTerm\n\t\t}]\n\t}\n\t// +usage=Specify the pod anti-affinity scheduling
        rules\n\tpodAntiAffinity?: {\n\t\t// +usage=Specify the required during scheduling
        ignored during execution\n\t\trequired?: [...#podAffinityTerm]\n\t\t// +usage=Specify
        the preferred during scheduling ignored during execution\n\t\tpreferred?:
        [...{\n\t\t\t// +usage=Specify weight associated with matching the corresponding
        podAffinityTerm\n\t\t\tweight: int & >=1 & <=100\n\t\t\t// +usage=Specify
        a set of pods\n\t\t\tpodAffinityTerm: #podAffinityTerm\n\t\t}]\n\t}\n\t//
        +usage=Specify the node affinity scheduling rules for the pod\n\tnodeAffinity?:
        {\n\t\t// +usage=Specify the required during scheduling ignored during execution\n\t\trequired?:
        {\n\t\t\t// +usage=Specify a list of node selector\n\t\t\tnodeSelectorTerms:
        [...#nodeSelectorTerm]\n\t\t}\n\t\t// +usage=Specify the preferred during
        scheduling ignored during execution\n\t\tpreferred?: [...{\n\t\t\t// +usage=Specify
        weight associated with matching the corresponding nodeSelector\n\t\t\tweight:
        int & >=1 & <=100\n\t\t\t// +usage=Specify a node selector\n\t\t\tpreference:
        #nodeSelectorTerm\n\t\t}]\n\t}\n\t// +usage=Specify tolerant taint\n\ttolerations?:
        [...{\n\t\tkey?:     string\n\t\toperator: *\"Equal\" | \"Exists\"\n\t\tvalue?:
        \  string\n\t\teffect?:  \"NoSchedule\" | \"PreferNoSchedule\" | \"NoExecute\"\n\t\t//
        +usage=Specify the period of time the toleration\n\t\ttolerationSeconds?:
        int\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add annotations on your workload. if it generates
      pod, add same annotations for generated pods.
  name: annotations
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  podDisruptive: true
  schematic:
    cue:
      template: "// +patchStrategy=jsonMergePatch\npatch: {\n\tmetadata: annotations:
        {\n\t\tfor k, v in parameter {\n\t\t\t(k): v\n\t\t}\n\t}\n\tif context.output.spec
        != _|_ && context.output.spec.template != _|_ {\n\t\tspec: template: metadata:
        annotations: {\n\t\t\tfor k, v in parameter {\n\t\t\t\t(k): v\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        [string]: string | null\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add command on K8s pod for your workload which
      follows the pod spec in path 'spec.template'
  name: command
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  schematic:
    cue:
      template: "#PatchParams: {\n\t// +usage=Specify the name of the target container,
        if not set, use the component name\n\tcontainerName: *\"\" | string\n\t//
        +usage=Specify the command to use in the target container, if not set, it
        will not be changed\n\tcommand: *null | [...string]\n\t// +usage=Specify the
        args to use in the target container, if set, it will override existing args\n\targs:
        *null | [...string]\n\t// +usage=Specify the args to add in the target container,
        existing args will be kept, cannot be used with `args`\n\taddArgs: *null |
        [...string]\n\t// +usage=Specify the existing args to delete in the target
        container, cannot be used with `args`\n\tdelArgs: *null | [...string]\n}\nPatchContainer:
        {\n\t_params:         #PatchParams\n\tname:            _params.containerName\n\t_baseContainers:
        context.output.spec.template.spec.containers\n\t_matchContainers_: [ for _container_
        in _baseContainers if _container_.name == name {_container_}]\n\t_baseContainer:
        *_|_ | {...}\n\tif len(_matchContainers_) == 0 {\n\t\terr: \"container \\(name)
        not found\"\n\t}\n\tif len(_matchContainers_) > 0 {\n\t\t_baseContainer: _matchContainers_[0]\n\t\tif
        _params.command != null {\n\t\t\t// +patchStrategy=replace\n\t\t\tcommand:
        _params.command\n\t\t}\n\t\tif (_params.addArgs != null || _params.delArgs
        != null) && _params.args != null {\n\t\t\terr: \"cannot set addArgs/delArgs
        and args at the same time\"\n\t\t}\n\t\t_delArgs: {...}\n\t\tif _params.delArgs
        != null {\n\t\t\t_delArgs: {for k in _params.delArgs {(k): \"\"}}\n\t\t}\n\t\tif
        _params.delArgs == null {\n\t\t\t_delArgs: {}\n\t\t}\n\t\t_args: [...string]\n\t\tif
        _params.args != null {\n\t\t\t_args: _params.args\n\t\t}\n\t\tif _params.args
        == null && _baseContainer.args != _|_ {\n\t\t\t_args: _baseContainer.args\n\t\t}\n\t\tif
        _params.args == null && _baseContainer.args == _|_ {\n\t\t\t_args: []\n\t\t}\n\t\t_argsMap:
        {for a in _args {(a): \"\"}}\n\t\t_addArgs: [...string]\n\t\tif _params.addArgs
        != null {\n\t\t\t_addArgs: _params.addArgs\n\t\t}\n\t\tif _params.addArgs
        == null {\n\t\t\t_addArgs: []\n\t\t}\n\n\t\t// +patchStrategy=replace\n\t\targs:
        [ for a in _args if _delArgs[a] == _|_ {a}] + [ for a in _addArgs if _delArgs[a]
        == _|_ && _argsMap[a] == _|_ {a}]\n\t}\n}\n// +patchStrategy=open\npatch:
        spec: template: spec: {\n\tif parameter.containers == _|_ {\n\t\t// +patchKey=name\n\t\tcontainers:
        [{\n\t\t\tPatchContainer & {_params: {\n\t\t\t\tif parameter.containerName
        == \"\" {\n\t\t\t\t\tcontainerName: context.name\n\t\t\t\t}\n\t\t\t\tif parameter.containerName
        != \"\" {\n\t\t\t\t\tcontainerName: parameter.containerName\n\t\t\t\t}\n\t\t\t\tcommand:
        parameter.command\n\t\t\t\targs:    parameter.args\n\t\t\t\taddArgs: parameter.addArgs\n\t\t\t\tdelArgs:
        parameter.delArgs\n\t\t\t}}\n\t\t}]\n\t}\n\tif parameter.containers != _|_
        {\n\t\t// +patchKey=name\n\t\tcontainers: [ for c in parameter.containers
        {\n\t\t\tif c.containerName == \"\" {\n\t\t\t\terr: \"container name must
        be set for containers\"\n\t\t\t}\n\t\t\tif c.containerName != \"\" {\n\t\t\t\tPatchContainer
        & {_params: c}\n\t\t\t}\n\t\t}]\n\t}\n}\nparameter: *#PatchParams | close({\n\t//
        +usage=Specify the commands for multiple containers\n\tcontainers: [...#PatchParams]\n})\nerrs:
        [ for c in patch.spec.template.spec.containers if c.err != _|_ {c.err}]\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Create/Attach configmaps on K8s pod for your workload
      which follows the pod spec in path 'spec.template'. This definition is DEPRECATED,
      please specify configmap in 'storage' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: configmap
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: {\n\tcontainers: [{\n\t\t// +patchKey=name\n\t\tvolumeMounts:
        [\n\t\t\tfor v in parameter.volumes {\n\t\t\t\t{\n\t\t\t\t\tname:      \"volume-\\(v.name)\"\n\t\t\t\t\tmountPath:
        v.mountPath\n\t\t\t\t\treadOnly:  v.readOnly\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t},
        ...]\n\t// +patchKey=name\n\tvolumes: [\n\t\tfor v in parameter.volumes {\n\t\t\t{\n\t\t\t\tname:
        \"volume-\\(v.name)\"\n\t\t\t\tconfigMap: name: v.name\n\t\t\t}\n\t\t},\n\t]\n}\noutputs:
        {\n\tfor v in parameter.volumes {\n\t\tif v.data != _|_ {\n\t\t\t(v.name):
        {\n\t\t\t\tapiVersion: \"v1\"\n\t\t\t\tkind:       \"ConfigMap\"\n\t\t\t\tmetadata:
        name: v.name\n\t\t\t\tdata: v.data\n\t\t\t}\n\t\t}\n\t}\n}\nparameter: {\n\t//
        +usage=Specify mounted configmap names and their mount paths in the container\n\tvolumes:
        [...{\n\t\tname:      string\n\t\tmountPath: string\n\t\treadOnly:  *false
        | bool\n\t\tdata?: [string]: string\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Set the image of the container.
  name: container-image
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "#PatchParams: {\n\t// +usage=Specify the name of the target container,
        if not set, use the component name\n\tcontainerName: *\"\" | string\n\t//
        +usage=Specify the image of the container\n\timage: string\n\t// +usage=Specify
        the image pull policy of the container\n\timagePullPolicy: *\"\" | \"IfNotPresent\"
        | \"Always\" | \"Never\"\n}\nPatchContainer: {\n\t_params:         #PatchParams\n\tname:
        \           _params.containerName\n\t_baseContainers: context.output.spec.template.spec.containers\n\t_matchContainers_:
        [ for _container_ in _baseContainers if _container_.name == name {_container_}]\n\t_baseContainer:
        *_|_ | {...}\n\tif len(_matchContainers_) == 0 {\n\t\terr: \"container \\(name)
        not found\"\n\t}\n\tif len(_matchContainers_) > 0 {\n\t\t// +patchStrategy=retainKeys\n\t\timage:
        _params.image\n\n\t\tif _params.imagePullPolicy != \"\" {\n\t\t\t// +patchStrategy=retainKeys\n\t\t\timagePullPolicy:
        _params.imagePullPolicy\n\t\t}\n\t}\n}\npatch: spec: template: spec: {\n\tif
        parameter.containers == _|_ {\n\t\t// +patchKey=name\n\t\tcontainers: [{\n\t\t\tPatchContainer
        & {_params: {\n\t\t\t\tif parameter.containerName == \"\" {\n\t\t\t\t\tcontainerName:
        context.name\n\t\t\t\t}\n\t\t\t\tif parameter.containerName != \"\" {\n\t\t\t\t\tcontainerName:
        parameter.containerName\n\t\t\t\t}\n\t\t\t\timage:           parameter.image\n\t\t\t\timagePullPolicy:
        parameter.imagePullPolicy\n\t\t\t}}\n\t\t}]\n\t}\n\tif parameter.containers
        != _|_ {\n\t\t// +patchKey=name\n\t\tcontainers: [ for c in parameter.containers
        {\n\t\t\tif c.containerName == \"\" {\n\t\t\t\terr: \"containerName must be
        set for containers\"\n\t\t\t}\n\t\t\tif c.containerName != \"\" {\n\t\t\t\tPatchContainer
        & {_params: c}\n\t\t\t}\n\t\t}]\n\t}\n}\nparameter: #PatchParams | close({\n\t//
        +usage=Specify the container image for multiple containers\n\tcontainers:
        [...#PatchParams]\n})\nerrs: [ for c in patch.spec.template.spec.containers
        if c.err != _|_ {c.err}]\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Automatically scale the component based on CPU
      usage.
  name: cpuscaler
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  schematic:
    cue:
      template: "outputs: cpuscaler: {\n\tapiVersion: \"autoscaling/v1\"\n\tkind:
        \      \"HorizontalPodAutoscaler\"\n\tmetadata: name: context.name\n\tspec:
        {\n\t\tscaleTargetRef: {\n\t\t\tapiVersion: parameter.targetAPIVersion\n\t\t\tkind:
        \      parameter.targetKind\n\t\t\tname:       context.name\n\t\t}\n\t\tminReplicas:
        \                   parameter.min\n\t\tmaxReplicas:                    parameter.max\n\t\ttargetCPUUtilizationPercentage:
        parameter.cpuUtil\n\t}\n}\nparameter: {\n\t// +usage=Specify the minimal number
        of replicas to which the autoscaler can scale down\n\tmin: *1 | int\n\t//
        +usage=Specify the maximum number of of replicas to which the autoscaler can
        scale up\n\tmax: *10 | int\n\t// +usage=Specify the average CPU utilization,
        for example, 50 means the CPU usage is 50%\n\tcpuUtil: *50 | int\n\t// +usage=Specify
        the apiVersion of scale target\n\ttargetAPIVersion: *\"apps/v1\" | string\n\t//
        +usage=Specify the kind of scale target\n\ttargetKind: *\"Deployment\" | string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add env on K8s pod for your workload which follows
      the pod spec in path 'spec.template'
  name: env
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  schematic:
    cue:
      template: "#PatchParams: {\n\t// +usage=Specify the name of the target container,
        if not set, use the component name\n\tcontainerName: *\"\" | string\n\t//
        +usage=Specify if replacing the whole environment settings for the container\n\treplace:
        *false | bool\n\t// +usage=Specify the  environment variables to merge, if
        key already existing, override its value\n\tenv: [string]: string\n\t// +usage=Specify
        which existing environment variables to unset\n\tunset: *[] | [...string]\n}\nPatchContainer:
        {\n\t_params: #PatchParams\n\tname:    _params.containerName\n\t_delKeys:
        {for k in _params.unset {(k): \"\"}}\n\t_baseContainers: context.output.spec.template.spec.containers\n\t_matchContainers_:
        [ for _container_ in _baseContainers if _container_.name == name {_container_}]\n\t_baseContainer:
        *_|_ | {...}\n\tif len(_matchContainers_) == 0 {\n\t\terr: \"container \\(name)
        not found\"\n\t}\n\tif len(_matchContainers_) > 0 {\n\t\t_baseContainer: _matchContainers_[0]\n\t\t_baseEnv:
        \      _baseContainer.env\n\t\tif _baseEnv == _|_ {\n\t\t\t// +patchStrategy=replace\n\t\t\tenv:
        [ for k, v in _params.env if _delKeys[k] == _|_ {\n\t\t\t\tname:  k\n\t\t\t\tvalue:
        v\n\t\t\t}]\n\t\t}\n\t\tif _baseEnv != _|_ {\n\t\t\t_baseEnvMap: {for envVar
        in _baseEnv {(envVar.name): envVar}}\n\t\t\t// +patchStrategy=replace\n\t\t\tenv:
        [ for envVar in _baseEnv if _delKeys[envVar.name] == _|_ && !_params.replace
        {\n\t\t\t\tname: envVar.name\n\t\t\t\tif _params.env[envVar.name] != _|_ {\n\t\t\t\t\tvalue:
        _params.env[envVar.name]\n\t\t\t\t}\n\t\t\t\tif _params.env[envVar.name] ==
        _|_ {\n\t\t\t\t\tif envVar.value != _|_ {\n\t\t\t\t\t\tvalue: envVar.value\n\t\t\t\t\t}\n\t\t\t\t\tif
        envVar.valueFrom != _|_ {\n\t\t\t\t\t\tvalueFrom: envVar.valueFrom\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}]
        + [ for k, v in _params.env if _delKeys[k] == _|_ && (_params.replace || _baseEnvMap[k]
        == _|_) {\n\t\t\t\tname:  k\n\t\t\t\tvalue: v\n\t\t\t}]\n\t\t}\n\t}\n}\npatch:
        spec: template: spec: {\n\tif parameter.containers == _|_ {\n\t\t// +patchKey=name\n\t\tcontainers:
        [{\n\t\t\tPatchContainer & {_params: {\n\t\t\t\tif parameter.containerName
        == \"\" {\n\t\t\t\t\tcontainerName: context.name\n\t\t\t\t}\n\t\t\t\tif parameter.containerName
        != \"\" {\n\t\t\t\t\tcontainerName: parameter.containerName\n\t\t\t\t}\n\t\t\t\treplace:
        parameter.replace\n\t\t\t\tenv:     parameter.env\n\t\t\t\tunset:   parameter.unset\n\t\t\t}}\n\t\t}]\n\t}\n\tif
        parameter.containers != _|_ {\n\t\t// +patchKey=name\n\t\tcontainers: [ for
        c in parameter.containers {\n\t\t\tif c.containerName == \"\" {\n\t\t\t\terr:
        \"containerName must be set for containers\"\n\t\t\t}\n\t\t\tif c.containerName
        != \"\" {\n\t\t\t\tPatchContainer & {_params: c}\n\t\t\t}\n\t\t}]\n\t}\n}\nparameter:
        *#PatchParams | close({\n\t// +usage=Specify the environment variables for
        multiple containers\n\tcontainers: [...#PatchParams]\n})\nerrs: [ for c in
        patch.spec.template.spec.containers if c.err != _|_ {c.err}]\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Expose port to enable web traffic for your component.
  name: expose
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: "import (\n\t\"strconv\"\n)\n\noutputs: service: {\n\tapiVersion:
        \"v1\"\n\tkind:       \"Service\"\n\tmetadata: name:        context.name\n\tmetadata:
        annotations: parameter.annotations\n\tspec: {\n\t\tselector: \"app.oam.dev/component\":
        context.name\n\t\tports: [\n\t\t\tfor p in parameter.port {\n\t\t\t\tname:
        \      \"port-\" + strconv.FormatInt(p, 10)\n\t\t\t\tport:       p\n\t\t\t\ttargetPort:
        p\n\t\t\t},\n\t\t]\n\t\ttype: parameter.type\n\t}\n}\nparameter: {\n\t// +usage=Specify
        the exposion ports\n\tport: [...int]\n\t// +usage=Specify the annotaions of
        the exposed service\n\tannotations: [string]: string\n\t// +usage=Specify
        what kind of Service you want. options: \"ClusterIP\",\"NodePort\",\"LoadBalancer\",\"ExternalName\"\n\ttype:
        *\"ClusterIP\" | \"NodePort\" | \"LoadBalancer\" | \"ExternalName\"\n}\n"
  status:
    customStatus: "message: *\"\" | string\nservice: context.outputs.service\nif service.spec.type
      == \"ClusterIP\" {\n\tmessage: \"ClusterIP: \\(service.spec.clusterIP)\"\n}\nif
      service.spec.type == \"LoadBalancer\" {\n\tstatus: service.status\n\tisHealth:
      *false | bool\n\tif status != _|_ if status.loadBalancer != _|_ if status.loadBalancer.ingress
      != _|_ if len(status.loadBalancer.ingress) > 0 if status.loadBalancer.ingress[0].ip
      != _|_ {\n\t\tisHealth: true\n\t}\n\tif !isHealth {\n\t\tmessage: \"ExternalIP:
      Pending\"\n\t}\n\tif isHealth {\n\t\tmessage: \"ExternalIP: \\(status.loadBalancer.ingress[0].ip)\"\n\t}\n}"
    healthPolicy: "service: context.outputs.service\nif service.spec.type == \"LoadBalancer\"
      {\n\tstatus: service.status\n\tisHealth: *false | bool\n\tif status != _|_ if
      status.loadBalancer != _|_ if status.loadBalancer.ingress != _|_ if len(status.loadBalancer.ingress)
      > 0 if status.loadBalancer.ingress[0].ip != _|_ {\n\t\tisHealth: true\n\t}\n}\nif
      service.spec.type != \"LoadBalancer\" {\n\tisHealth: true\n}"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component, the
      ingress API matches K8s v1.20+.
  name: gateway
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: "// trait template can have multiple outputs in one trait\noutputs:
        service: {\n\tapiVersion: \"v1\"\n\tkind:       \"Service\"\n\tmetadata: name:
        context.name\n\tspec: {\n\t\tselector: \"app.oam.dev/component\": context.name\n\t\tports:
        [\n\t\t\tfor k, v in parameter.http {\n\t\t\t\tport:       v\n\t\t\t\ttargetPort:
        v\n\t\t\t},\n\t\t]\n\t}\n}\nlegacyAPI: context.clusterVersion.minor < 19\noutputs:
        ingress: {\n\tif legacyAPI {\n\t\tapiVersion: \"networking.k8s.io/v1beta1\"\n\t}\n\tif
        !legacyAPI {\n\t\tapiVersion: \"networking.k8s.io/v1\"\n\t}\n\tkind: \"Ingress\"\n\tmetadata:
        {\n\t\tname: context.name\n\t\tannotations: {\n\t\t\tif !parameter.classInSpec
        {\n\t\t\t\t\"kubernetes.io/ingress.class\": parameter.class\n\t\t\t}\n\t\t\tif
        parameter.gatewayHost != _|_ {\n\t\t\t\t\"ingress.controller/host\": parameter.gatewayHost\n\t\t\t}\n\t\t}\n\t}\n\tspec:
        {\n\t\tif parameter.classInSpec {\n\t\t\tingressClassName: parameter.class\n\t\t}\n\t\tif
        parameter.secretName != _|_ {\n\t\t\ttls: [{\n\t\t\t\thosts: [\n\t\t\t\t\tparameter.domain,\n\t\t\t\t]\n\t\t\t\tsecretName:
        parameter.secretName\n\t\t\t}]\n\t\t}\n\t\trules: [{\n\t\t\tif parameter.domain
        != _|_ {\n\t\t\t\thost: parameter.domain\n\t\t\t}\n\t\t\thttp: paths: [\n\t\t\t\tfor
        k, v in parameter.http {\n\t\t\t\t\tpath:     k\n\t\t\t\t\tpathType: \"ImplementationSpecific\"\n\t\t\t\t\tbackend:
        {\n\t\t\t\t\t\tif legacyAPI {\n\t\t\t\t\t\t\tserviceName: context.name\n\t\t\t\t\t\t\tservicePort:
        v\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif !legacyAPI {\n\t\t\t\t\t\t\tservice: {\n\t\t\t\t\t\t\t\tname:
        context.name\n\t\t\t\t\t\t\t\tport: number: v\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t]\n\t\t}]\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the domain you want to expose\n\tdomain?: string\n\n\t//
        +usage=Specify the mapping relationship between the http path and the workload
        port\n\thttp: [string]: int\n\n\t// +usage=Specify the class of ingress to
        use\n\tclass: *\"nginx\" | string\n\n\t// +usage=Set ingress class in '.spec.ingressClassName'
        instead of 'kubernetes.io/ingress.class' annotation.\n\tclassInSpec: *false
        | bool\n\n\t// +usage=Specify the secret name you want to quote to use tls.\n\tsecretName?:
        string\n\n\t// +usage=Specify the host of the ingress gateway, which is used
        to generate the endpoints when the host is empty.\n\tgatewayHost?: string\n}\n"
  status:
    customStatus: "if context.outputs.ingress.status.loadBalancer.ingress == _|_ {\n
      \ message: \"No loadBalancer found, visiting by using 'vela port-forward \"
      + context.appName + \"'\\n\"\n}\nif context.outputs.ingress.status.loadBalancer.ingress
      != _|_ {\n\tlet igs = context.outputs.ingress.status.loadBalancer.ingress\n
      \ if igs[0].ip != _|_ {\n  \tif igs[0].host != _|_ {\n\t    message: \"Visiting
      URL: \" + context.outputs.ingress.spec.rules[0].host + \", IP: \" + igs[0].ip\n
      \ \t}\n  \tif igs[0].host == _|_ {\n\t    message: \"Host not specified, visit
      the cluster or load balancer in front of the cluster with IP: \" + igs[0].ip\n
      \ \t}\n  }\n  if igs[0].ip == _|_ {\n  \tif igs[0].host != _|_ {\n\t\t  message:
      \"Visiting URL: \" + context.outputs.ingress.spec.rules[0].host\n\t\t}\n  \tif
      igs[0].host != _|_ {\n\t    message: \"Host not specified, visit the cluster
      or load balancer in front of the cluster\"\n\t\t}\n  }\n}"
    healthPolicy: 'isHealth: len(context.outputs.service.spec.clusterIP) > 0'
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add host aliases on K8s pod for your workload
      which follows the pod spec in path 'spec.template'.
  name: hostalias
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: false
  schematic:
    cue:
      template: "patch: {\n\t// +patchKey=ip\n\tspec: template: spec: hostAliases:
        parameter.hostAliases\n}\nparameter: {\n\t// +usage=Specify the hostAliases
        to add\n\thostAliases: [...{\n\t\tip: string\n\t\thostnames: [...string]\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Configure k8s HPA for Deployment or Statefulsets
  name: hpa
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: "outputs: hpa: {\n\tif context.clusterVersion.minor < 23 {\n\t\tapiVersion:
        \"autoscaling/v2beta2\"\n\t}\n\tif context.clusterVersion.minor >= 23 {\n\t\tapiVersion:
        \"autoscaling/v2\"\n\t}\n\tkind: \"HorizontalPodAutoscaler\"\n\tmetadata:
        name: context.name\n\tspec: {\n\t\tscaleTargetRef: {\n\t\t\tapiVersion: parameter.targetAPIVersion\n\t\t\tkind:
        \      parameter.targetKind\n\t\t\tname:       context.name\n\t\t}\n\t\tminReplicas:
        parameter.min\n\t\tmaxReplicas: parameter.max\n\t\tmetrics: [\n\t\t\t{\n\t\t\t\ttype:
        \"Resource\"\n\t\t\t\tresource: {\n\t\t\t\t\tname: \"cpu\"\n\t\t\t\t\ttarget:
        {\n\t\t\t\t\t\ttype: parameter.cpu.type\n\t\t\t\t\t\tif parameter.cpu.type
        == \"Utilization\" {\n\t\t\t\t\t\t\taverageUtilization: parameter.cpu.value\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif
        parameter.cpu.type == \"AverageValue\" {\n\t\t\t\t\t\t\taverageValue: parameter.cpu.value\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\tif
        parameter.mem != _|_ {\n\t\t\t\t{\n\t\t\t\t\ttype: \"Resource\"\n\t\t\t\t\tresource:
        {\n\t\t\t\t\t\tname: \"memory\"\n\t\t\t\t\t\ttarget: {\n\t\t\t\t\t\t\ttype:
        parameter.mem.type\n\t\t\t\t\t\t\tif parameter.cpu.type == \"Utilization\"
        {\n\t\t\t\t\t\t\t\taverageUtilization: parameter.cpu.value\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif
        parameter.cpu.type == \"AverageValue\" {\n\t\t\t\t\t\t\t\taverageValue: parameter.cpu.value\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\tif
        parameter.podCustomMetrics != _|_ for m in parameter.podCustomMetrics {\n\t\t\t\ttype:
        \"Pods\"\n\t\t\t\tpods: {\n\t\t\t\t\tmetric: name: m.name\n\t\t\t\t\ttarget:
        {\n\t\t\t\t\t\ttype:         \"AverageValue\"\n\t\t\t\t\t\taverageValue: m.value\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the minimal number of replicas to which the autoscaler
        can scale down\n\tmin: *1 | int\n\t// +usage=Specify the maximum number of
        of replicas to which the autoscaler can scale up\n\tmax: *10 | int\n\t// +usage=Specify
        the apiVersion of scale target\n\ttargetAPIVersion: *\"apps/v1\" | string\n\t//
        +usage=Specify the kind of scale target\n\ttargetKind: *\"Deployment\" | string\n\tcpu:
        {\n\t\t// +usage=Specify resource metrics in terms of percentage(\"Utilization\")
        or direct value(\"AverageValue\")\n\t\ttype: *\"Utilization\" | \"AverageValue\"\n\t\t//
        +usage=Specify the value of CPU utilization or averageValue\n\t\tvalue: *50
        | int\n\t}\n\tmem?: {\n\t\t// +usage=Specify resource metrics in terms of
        percentage(\"Utilization\") or direct value(\"AverageValue\")\n\t\ttype: *\"Utilization\"
        | \"AverageValue\"\n\t\t// +usage=Specify  the value of MEM utilization or
        averageValue\n\t\tvalue: *50 | int\n\t}\n\t// +usage=Specify custom metrics
        of pod type\n\tpodCustomMetrics?: [...{\n\t\t// +usage=Specify name of custom
        metrics\n\t\tname: string\n\t\t// +usage=Specify target value of custom metrics\n\t\tvalue:
        string\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component, the
      ingress API matches K8s v1.20+.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: ingress-1-20
  namespace: vela-system
spec:
  podDisruptive: false
  schematic:
    cue:
      template: "// trait template can have multiple outputs in one trait\noutputs:
        service: {\n\tapiVersion: \"v1\"\n\tkind:       \"Service\"\n\tmetadata: name:
        context.name\n\tspec: {\n\t\tselector: \"app.oam.dev/component\": context.name\n\t\tports:
        [\n\t\t\tfor k, v in parameter.http {\n\t\t\t\tport:       v\n\t\t\t\ttargetPort:
        v\n\t\t\t},\n\t\t]\n\t}\n}\noutputs: ingress: {\n\tapiVersion: \"networking.k8s.io/v1\"\n\tkind:
        \      \"Ingress\"\n\tmetadata: {\n\t\tname: context.name\n\t\tannotations:
        \"kubernetes.io/ingress.class\": parameter.class\n\t}\n\tspec: rules: [{\n\t\thost:
        parameter.domain\n\t\thttp: paths: [\n\t\t\tfor k, v in parameter.http {\n\t\t\t\tpath:
        \    k\n\t\t\t\tpathType: \"ImplementationSpecific\"\n\t\t\t\tbackend: service:
        {\n\t\t\t\t\tname: context.name\n\t\t\t\t\tport: number: v\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t}]\n}\nparameter:
        {\n\t// +usage=Specify the domain you want to expose\n\tdomain: string\n\n\t//
        +usage=Specify the mapping relationship between the http path and the workload
        port\n\thttp: [string]: int\n\n\t// +usage=Specify the class of ingress to
        use\n\tclass: *\"nginx\" | string\n}\n"
  status:
    customStatus: "let igs = context.outputs.ingress.status.loadBalancer.ingress\nif
      igs == _|_ {\n  message: \"No loadBalancer found, visiting by using 'vela port-forward
      \" + context.appName + \"'\\n\"\n}\nif len(igs) > 0 {\n  if igs[0].ip != _|_
      {\n\t  message: \"Visiting URL: \" + context.outputs.ingress.spec.rules[0].host
      + \", IP: \" + igs[0].ip\n  }\n  if igs[0].ip == _|_ {\n\t  message: \"Visiting
      URL: \" + context.outputs.ingress.spec.rules[0].host\n  }\n}"
    healthPolicy: 'isHealth: len(context.outputs.service.spec.clusterIP) > 0'
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: ingress
  namespace: vela-system
spec:
  podDisruptive: false
  schematic:
    cue:
      template: "// trait template can have multiple outputs in one trait\noutputs:
        service: {\n\tapiVersion: \"v1\"\n\tkind:       \"Service\"\n\tmetadata: name:
        context.name\n\tspec: {\n\t\tselector: \"app.oam.dev/component\": context.name\n\t\tports:
        [\n\t\t\tfor k, v in parameter.http {\n\t\t\t\tport:       v\n\t\t\t\ttargetPort:
        v\n\t\t\t},\n\t\t]\n\t}\n}\noutputs: ingress: {\n\tapiVersion: \"networking.k8s.io/v1beta1\"\n\tkind:
        \      \"Ingress\"\n\tmetadata: name: context.name\n\tspec: rules: [{\n\t\thost:
        parameter.domain\n\t\thttp: paths: [\n\t\t\tfor k, v in parameter.http {\n\t\t\t\tpath:
        k\n\t\t\t\tbackend: {\n\t\t\t\t\tserviceName: context.name\n\t\t\t\t\tservicePort:
        v\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t}]\n}\nparameter: {\n\t// +usage=Specify
        the domain you want to expose\n\tdomain: string\n\n\t// +usage=Specify the
        mapping relationship between the http path and the workload port\n\thttp:
        [string]: int\n}\n"
  status:
    customStatus: "let igs = context.outputs.ingress.status.loadBalancer.ingress\nif
      igs == _|_ {\n\tmessage: \"No loadBalancer found, visiting by using 'vela port-forward
      \" + context.appName + \"'\\n\"\n}\nif len(igs) > 0 {\n\tif igs[0].ip != _|_
      {\n\t\tmessage: \"Visiting URL: \" + context.outputs.ingress.spec.rules[0].host
      + \", IP: \" + igs[0].ip\n\t}\n\tif igs[0].ip == _|_ {\n\t\tmessage: \"Visiting
      URL: \" + context.outputs.ingress.spec.rules[0].host\n\t}\n}"
    healthPolicy: 'isHealth: len(context.outputs.service.spec.clusterIP) > 0'
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: add an init container and use shared volume with
      pod
  name: init-container
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: {\n\t// +patchKey=name\n\tcontainers:
        [{\n\t\tname: context.name\n\t\t// +patchKey=name\n\t\tvolumeMounts: [{\n\t\t\tname:
        \     parameter.mountName\n\t\t\tmountPath: parameter.appMountPath\n\t\t}]\n\t}]\n\tinitContainers:
        [{\n\t\tname:            parameter.name\n\t\timage:           parameter.image\n\t\timagePullPolicy:
        parameter.imagePullPolicy\n\t\tif parameter.cmd != _|_ {\n\t\t\tcommand: parameter.cmd\n\t\t}\n\t\tif
        parameter.args != _|_ {\n\t\t\targs: parameter.args\n\t\t}\n\t\tif parameter[\"env\"]
        != _|_ {\n\t\t\tenv: parameter.env\n\t\t}\n\n\t\t// +patchKey=name\n\t\tvolumeMounts:
        [{\n\t\t\tname:      parameter.mountName\n\t\t\tmountPath: parameter.initMountPath\n\t\t}]
        + parameter.extraVolumeMounts\n\t}]\n\t// +patchKey=name\n\tvolumes: [{\n\t\tname:
        parameter.mountName\n\t\temptyDir: {}\n\t}]\n}\nparameter: {\n\t// +usage=Specify
        the name of init container\n\tname: string\n\n\t// +usage=Specify the image
        of init container\n\timage: string\n\n\t// +usage=Specify image pull policy
        for your service\n\timagePullPolicy: *\"IfNotPresent\" | \"Always\" | \"Never\"\n\n\t//
        +usage=Specify the commands run in the init container\n\tcmd?: [...string]\n\n\t//
        +usage=Specify the args run in the init container\n\targs?: [...string]\n\n\t//
        +usage=Specify the env run in the init container\n\tenv?: [...{\n\t\t// +usage=Environment
        variable name\n\t\tname: string\n\t\t// +usage=The value of the environment
        variable\n\t\tvalue?: string\n\t\t// +usage=Specifies a source the value of
        this var should come from\n\t\tvalueFrom?: {\n\t\t\t// +usage=Selects a key
        of a secret in the pod's namespace\n\t\t\tsecretKeyRef?: {\n\t\t\t\t// +usage=The
        name of the secret in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the secret to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t\t// +usage=Selects a key of a config map in the pod's
        namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t// +usage=The name of the config
        map in the pod's namespace to select from\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=The key of the config map to select from. Must be a valid secret key\n\t\t\t\tkey:
        string\n\t\t\t}\n\t\t}\n\t}]\n\n\t// +usage=Specify the mount name of shared
        volume\n\tmountName: *\"workdir\" | string\n\n\t// +usage=Specify the mount
        path of app container\n\tappMountPath: string\n\n\t// +usage=Specify the mount
        path of init container\n\tinitMountPath: string\n\n\t// +usage=Specify the
        extra volume mounts for the init container\n\textraVolumeMounts: [...{\n\t\t//
        +usage=The name of the volume to be mounted\n\t\tname: string\n\t\t// +usage=The
        mountPath for mount in the init container\n\t\tmountPath: string\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Patch the output following Json Merge Patch strategy,
      following RFC 7396.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: json-merge-patch
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        parameter: {...}
        // +patchStrategy=jsonMergePatch
        patch: parameter
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Patch the output following Json Patch strategy,
      following RFC 6902.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: json-patch
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  podDisruptive: true
  schematic:
    cue:
      template: |
        parameter: operations: [...{...}]
        // +patchStrategy=jsonPatch
        patch: parameter
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Set k8s update strategy for Deployment/DaemonSet/StatefulSet
  name: k8s-update-strategy
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  conflictsWith: []
  podDisruptive: false
  schematic:
    cue:
      template: "patch: spec: {\n\tif parameter.targetKind == \"Deployment\" && parameter.strategy.type
        != \"OnDelete\" {\n\t\t// +patchStrategy=retainKeys\n\t\tstrategy: {\n\t\t\ttype:
        parameter.strategy.type\n\t\t\tif parameter.strategy.type == \"RollingUpdate\"
        {\n\t\t\t\trollingUpdate: {\n\t\t\t\t\tmaxSurge:       parameter.strategy.rollingStrategy.maxSurge\n\t\t\t\t\tmaxUnavailable:
        parameter.strategy.rollingStrategy.maxUnavailable\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif
        parameter.targetKind == \"StatefulSet\" && parameter.strategy.type != \"Recreate\"
        {\n\t\t// +patchStrategy=retainKeys\n\t\tupdateStrategy: {\n\t\t\ttype: parameter.strategy.type\n\t\t\tif
        parameter.strategy.type == \"RollingUpdate\" {\n\t\t\t\trollingUpdate: partition:
        parameter.strategy.rollingStrategy.partition\n\t\t\t}\n\t\t}\n\t}\n\n\tif
        parameter.targetKind == \"DaemonSet\" && parameter.strategy.type != \"Recreate\"
        {\n\t\t// +patchStrategy=retainKeys\n\t\tupdateStrategy: {\n\t\t\ttype: parameter.strategy.type\n\t\t\tif
        parameter.strategy.type == \"RollingUpdate\" {\n\t\t\t\trollingUpdate: {\n\t\t\t\t\tmaxSurge:
        \      parameter.strategy.rollingStrategy.maxSurge\n\t\t\t\t\tmaxUnavailable:
        parameter.strategy.rollingStrategy.maxUnavailable\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n}\nparameter:
        {\n\t// +usage=Specify the apiVersion of target\n\ttargetAPIVersion: *\"apps/v1\"
        | string\n\t// +usage=Specify the kind of target\n\ttargetKind: *\"Deployment\"
        | \"StatefulSet\" | \"DaemonSet\"\n\t// +usage=Specify the strategy of update\n\tstrategy:
        {\n\t\t// +usage=Specify the strategy type\n\t\ttype: *\"RollingUpdate\" |
        \"Recreate\" | \"OnDelete\"\n\t\t// +usage=Specify the parameters of rollong
        update strategy\n\t\trollingStrategy?: {\n\t\t\tmaxSurge:       *\"25%\" |
        string\n\t\t\tmaxUnavailable: *\"25%\" | string\n\t\t\tpartition:      *0
        | int\n\t\t}\n\t}\n}\n"
  workloadRefPath: ""
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add labels on your workload. if it generates pod,
      add same label for generated pods.
  name: labels
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  podDisruptive: true
  schematic:
    cue:
      template: "// +patchStrategy=jsonMergePatch\npatch: {\n\tmetadata: labels: {\n\t\tfor
        k, v in parameter {\n\t\t\t(k): v\n\t\t}\n\t}\n\tif context.output.spec !=
        _|_ && context.output.spec.template != _|_ {\n\t\tspec: template: metadata:
        labels: {\n\t\t\tfor k, v in parameter {\n\t\t\t\t(k): v\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        [string]: string | null\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add lifecycle hooks for every container of K8s
      pod for your workload which follows the pod spec in path 'spec.template'.
  name: lifecycle
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: containers: [...{\n\tlifecycle: {\n\t\tif
        parameter.postStart != _|_ {\n\t\t\tpostStart: parameter.postStart\n\t\t}\n\t\tif
        parameter.preStop != _|_ {\n\t\t\tpreStop: parameter.preStop\n\t\t}\n\t}\n}]\nparameter:
        {\n\tpostStart?: #LifeCycleHandler\n\tpreStop?:   #LifeCycleHandler\n}\n#Port:
        int & >=1 & <=65535\n#LifeCycleHandler: {\n\texec?: command: [...string]\n\thttpGet?:
        {\n\t\tpath?:  string\n\t\tport:   #Port\n\t\thost?:  string\n\t\tscheme:
        *\"HTTP\" | \"HTTPS\"\n\t\thttpHeaders?: [...{\n\t\t\tname:  string\n\t\t\tvalue:
        string\n\t\t}]\n\t}\n\ttcpSocket?: {\n\t\tport:  #Port\n\t\thost?: string\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: nocalhost develop configuration.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: nocalhost
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "import (\n\t\"encoding/json\"\n)\n\noutputs: nocalhostService: {\n\tapiVersion:
        \"v1\"\n\tkind:       \"Service\"\n\tmetadata: name: context.name\n\tspec:
        {\n\t\tselector: \"app.oam.dev/component\": context.name\n\t\tports: [\n\t\t\t{\n\t\t\t\tport:
        \      parameter.port\n\t\t\t\ttargetPort: parameter.port\n\t\t\t},\n\t\t]\n\t\ttype:
        \"ClusterIP\"\n\t}\n}\npatch: metadata: annotations: {\n\t\"dev.nocalhost/application-name\":
        \     context.appName\n\t\"dev.nocalhost/application-namespace\": context.namespace\n\t\"dev.nocalhost\":
        \                      json.Marshal({\n\t\tname:        context.name\n\t\tserviceType:
        parameter.serviceType\n\t\tcontainers: [\n\t\t\t{\n\t\t\t\tname: context.name\n\t\t\t\tdev:
        {\n\t\t\t\t\tif parameter.gitUrl != _|_ {\n\t\t\t\t\t\tgitUrl: parameter.gitUrl\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.image == \"go\" {\n\t\t\t\t\t\timage: \"nocalhost-docker.pkg.coding.net/nocalhost/dev-images/golang:latest\"\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.image == \"java\" {\n\t\t\t\t\t\timage: \"nocalhost-docker.pkg.coding.net/nocalhost/dev-images/java:latest\"\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.image == \"python\" {\n\t\t\t\t\t\timage: \"nocalhost-docker.pkg.coding.net/nocalhost/dev-images/python:latest\"\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.image == \"node\" {\n\t\t\t\t\t\timage: \"nocalhost-docker.pkg.coding.net/nocalhost/dev-images/node:latest\"\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.image == \"ruby\" {\n\t\t\t\t\t\timage: \"nocalhost-docker.pkg.coding.net/nocalhost/dev-images/ruby:latest\"\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.image != \"go\" && parameter.image != \"java\" && parameter.image
        != \"python\" && parameter.image != \"node\" && parameter.image != \"ruby\"
        {\n\t\t\t\t\t\timage: parameter.image\n\t\t\t\t\t}\n\t\t\t\t\tshell:   parameter.shell\n\t\t\t\t\tworkDir:
        parameter.workDir\n\t\t\t\t\tif parameter.storageClass != _|_ {\n\t\t\t\t\t\tstorageClass:
        parameter.storageClass\n\t\t\t\t\t}\n\t\t\t\t\tresources: {\n\t\t\t\t\t\tlimits:
        \  parameter.resources.limits\n\t\t\t\t\t\trequests: parameter.resources.requests\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.persistentVolumeDirs != _|_ {\n\t\t\t\t\t\tpersistentVolumeDirs:
        [\n\t\t\t\t\t\t\tfor v in parameter.persistentVolumeDirs {\n\t\t\t\t\t\t\t\tpath:
        \    v.path\n\t\t\t\t\t\t\t\tcapacity: v.capacity\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.command != _|_ {\n\t\t\t\t\t\tcommand: parameter.command\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.debug != _|_ {\n\t\t\t\t\t\tdebug: parameter.debug\n\t\t\t\t\t}\n\t\t\t\t\thotReload:
        parameter.hotReload\n\t\t\t\t\tif parameter.sync != _|_ {\n\t\t\t\t\t\tsync:
        parameter.sync\n\t\t\t\t\t}\n\t\t\t\t\tif parameter.env != _|_ {\n\t\t\t\t\t\tenv:
        [\n\t\t\t\t\t\t\tfor v in parameter.env {\n\t\t\t\t\t\t\t\tname:  v.name\n\t\t\t\t\t\t\t\tvalue:
        v.value\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t\tif parameter.portForward
        != _|_ {\n\t\t\t\t\t\tportForward: parameter.portForward\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.portForward == _|_ {\n\t\t\t\t\t\tportForward: [\"\\(parameter.port):\\(parameter.port)\"]\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t})\n}\nlanguage:
        \"go\" | \"java\" | \"python\" | \"node\" | \"ruby\"\nparameter: {\n\tport:
        \         int\n\tserviceType:   *\"deployment\" | string\n\tgitUrl?:       string\n\timage:
        \        language | string\n\tshell:         *\"bash\" | string\n\tworkDir:
        \      *\"/home/nocalhost-dev\" | string\n\tstorageClass?: string\n\tcommand:
        {\n\t\trun:   *[\"sh\", \"run.sh\"] | [...string]\n\t\tdebug: *[\"sh\", \"debug.sh\"]
        | [...string]\n\t}\n\tdebug?: remoteDebugPort?: int\n\thotReload: *true |
        bool\n\tsync: {\n\t\ttype:              *\"send\" | string\n\t\tfilePattern:
        \      *[\"./\"] | [...string]\n\t\tignoreFilePattern: *[\".git\", \".vscode\",
        \".idea\", \".gradle\", \"build\"] | [...string]\n\t}\n\tenv?: [...{\n\t\tname:
        \ string\n\t\tvalue: string\n\t}]\n\tportForward?: [...string]\n\tpersistentVolumeDirs?:
        [...{\n\t\tpath:     string\n\t\tcapacity: string\n\t}]\n\tresources: {\n\t\tlimits:
        {\n\t\t\tmemory: *\"2Gi\" | string\n\t\t\tcpu:    *\"2\" | string\n\t\t}\n\t\trequests:
        {\n\t\t\tmemory: *\"512Mi\" | string\n\t\t\tcpu:    *\"0.5\" | string\n\t\t}\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: affinity specify node affinity and toleration
      on K8s pod for your workload which follows the pod spec in path 'spec.template'.
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/ui-hidden: "true"
  name: node-affinity
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: {\n\tif parameter.affinity != _|_ {\n\t\taffinity:
        nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms:
        [{\n\t\t\tmatchExpressions: [\n\t\t\t\tfor k, v in parameter.affinity {\n\t\t\t\t\tkey:
        \     k\n\t\t\t\t\toperator: \"In\"\n\t\t\t\t\tvalues:   v\n\t\t\t\t},\n\t\t\t]}]\n\t}\n\tif
        parameter.tolerations != _|_ {\n\t\ttolerations: [\n\t\t\tfor k, v in parameter.tolerations
        {\n\t\t\t\teffect:   \"NoSchedule\"\n\t\t\t\tkey:      k\n\t\t\t\toperator:
        \"Equal\"\n\t\t\t\tvalue:    v\n\t\t\t}]\n\t}\n}\nparameter: {\n\taffinity?:
        [string]: [...string]\n\ttolerations?: [string]: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Enable public web traffic for the component without
      creating a Service.
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/ui-hidden: "true"
  name: pure-ingress
  namespace: vela-system
spec:
  appliesToWorkloads:
  - '*'
  conflictsWith: []
  podDisruptive: false
  schematic:
    cue:
      template: "outputs: ingress: {\n\tapiVersion: \"networking.k8s.io/v1beta1\"\n\tkind:
        \      \"Ingress\"\n\tmetadata: name: context.name\n\tspec: rules: [{\n\t\thost:
        parameter.domain\n\t\thttp: paths: [\n\t\t\tfor k, v in parameter.http {\n\t\t\t\tpath:
        k\n\t\t\t\tbackend: {\n\t\t\t\t\tserviceName: context.name\n\t\t\t\t\tservicePort:
        v\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t}]\n}\nparameter: {\n\t// +usage=Specify
        the domain you want to expose\n\tdomain: string\n\n\t// +usage=Specify the
        mapping relationship between the http path and the workload port\n\thttp:
        [string]: int\n}\n"
  status:
    customStatus: "let igs = context.outputs.ingress.status.loadBalancer.ingress\nif
      igs == _|_ {\n\tmessage: \"No loadBalancer found, visiting by using 'vela port-forward
      \" + context.appName + \" --route'\\n\"\n}\nif len(igs) > 0 {\n\tif igs[0].ip
      != _|_ {\n\t\tmessage: \"Visiting URL: \" + context.outputs.ingress.spec.rules[0].host
      + \", IP: \" + igs[0].ip\n\t}\n\tif igs[0].ip == _|_ {\n\t\tmessage: \"Visiting
      URL: \" + context.outputs.ingress.spec.rules[0].host\n\t}\n}"
  workloadRefPath: ""
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Create a Persistent Volume Claim and mount the
      PVC as volume to the  first container in the pod. This definition is DEPRECATED,
      please specify pvc in 'storage' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: pvc
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: {\n\tcontainers: [{\n\t\tif parameter.volumeMode
        == \"Block\" {\n\t\t\t// +patchKey=name\n\t\t\tvolumeDevices: [\n\t\t\t\tfor
        v in parameter.volumesToMount {\n\t\t\t\t\t{\n\t\t\t\t\t\tname:       v.name\n\t\t\t\t\t\tdevicePath:
        v.devicePath\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t]\n\t\t}\n\t\tif parameter.volumeMode
        == \"Filesystem\" {\n\t\t\t// +patchKey=name\n\t\t\tvolumeMounts: [\n\t\t\t\tfor
        v in parameter.volumesToMount {\n\t\t\t\t\t{\n\t\t\t\t\t\tname:      v.name\n\t\t\t\t\t\tmountPath:
        v.mountPath\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t]\n\t\t}\n\t}]\n\n\t// +patchKey=name\n\tvolumes:
        [\n\t\tfor v in parameter.volumesToMount {\n\t\t\t{\n\t\t\t\tname: v.name\n\t\t\t\tpersistentVolumeClaim:
        claimName: parameter.claimName\n\t\t\t}\n\t\t},\n\t]\n}\noutputs: claim: {\n\tapiVersion:
        \"v1\"\n\tkind:       \"PersistentVolumeClaim\"\n\tmetadata: name: parameter.claimName\n\tspec:
        {\n\t\taccessModes: parameter.accessModes\n\t\tvolumeMode:  parameter.volumeMode\n\t\tif
        parameter.volumeName != _|_ {\n\t\t\tvolumeName: parameter.volumeName\n\t\t}\n\n\t\tif
        parameter.storageClassName != _|_ {\n\t\t\tstorageClassName: parameter.storageClassName\n\t\t}\n\t\tresources:
        requests: storage: parameter.resources.requests.storage\n\t\tif parameter.resources.limits
        != _|_ {\n\t\t\tresources: limits: storage: parameter.resources.limits.storage\n\t\t}\n\t\tif
        parameter.dataSourceRef != _|_ {\n\t\t\tdataSourceRef: parameter.dataSourceRef\n\t\t}\n\t\tif
        parameter.dataSource != _|_ {\n\t\t\tdataSource: parameter.dataSource\n\t\t}\n\t\tif
        parameter.selector != _|_ {\n\t\t\tdataSource: parameter.selector\n\t\t}\n\t}\n}\nparameter:
        {\n\tclaimName:   string\n\tvolumeMode:  *\"Filesystem\" | string\n\tvolumeName?:
        string\n\taccessModes: [...string]\n\tstorageClassName?: string\n\tresources:
        {\n\t\trequests: storage: =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t\tlimits?:
        storage:  =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t}\n\tdataSourceRef?:
        {\n\t\tname:     string\n\t\tkind:     string\n\t\tapiGroup: string\n\t}\n\tdataSource?:
        {\n\t\tname:     string\n\t\tkind:     string\n\t\tapiGroup: string\n\t}\n\tselector?:
        {\n\t\tmatchLabels?: [string]: string\n\t\tmatchExpressions?: {\n\t\t\tkey:
        string\n\t\t\tvalues: [...string]\n\t\t\toperator: string\n\t\t}\n\t}\n\tvolumesToMount:
        [...{\n\t\tname: string\n\t\tif volumeMode == \"Block\" {\n\t\t\tdevicePath:
        string\n\t\t}\n\t\tif volumeMode == \"Filesystem\" {\n\t\t\tmountPath: string\n\t\t}\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add resource requests and limits on K8s pod for
      your workload which follows the pod spec in path 'spec.template.'
  name: resource
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "patch: spec: template: spec: containers: [...{\n\tresources: {\n\t\tif
        parameter.cpu != _|_ && parameter.memory != _|_ && parameter.requests == _|_
        && parameter.limits == _|_ {\n\t\t\trequests: {\n\t\t\t\tcpu:    parameter.cpu\n\t\t\t\tmemory:
        parameter.memory\n\t\t\t}\n\t\t\tlimits: {\n\t\t\t\tcpu:    parameter.cpu\n\t\t\t\tmemory:
        parameter.memory\n\t\t\t}\n\t\t}\n\n\t\tif parameter.requests != _|_ {\n\t\t\trequests:
        {\n\t\t\t\tcpu:    parameter.requests.cpu\n\t\t\t\tmemory: parameter.requests.memory\n\t\t\t}\n\t\t}\n\t\tif
        parameter.limits != _|_ {\n\t\t\tlimits: {\n\t\t\t\tcpu:    parameter.limits.cpu\n\t\t\t\tmemory:
        parameter.limits.memory\n\t\t\t}\n\t\t}\n\t}\n}]\nparameter: {\n\t// +usage=Specify
        the amount of cpu for requests and limits\n\tcpu?: *1 | number\n\t// +usage=Specify
        the amount of memory for requests and limits\n\tmemory?: *\"2048Mi\" | =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t//
        +usage=Specify the resources in requests\n\trequests?: {\n\t\t// +usage=Specify
        the amount of cpu for requests\n\t\tcpu: *1 | number\n\t\t// +usage=Specify
        the amount of memory for requests\n\t\tmemory: *\"2048Mi\" | =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t}\n\t//
        +usage=Specify the resources in limits\n\tlimits?: {\n\t\t// +usage=Specify
        the amount of cpu for limits\n\t\tcpu: *1 | number\n\t\t// +usage=Specify
        the amount of memory for limits\n\t\tmemory: *\"2048Mi\" | =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Manually scale K8s pod for your workload which
      follows the pod spec in path 'spec.template'.
  name: scaler
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  podDisruptive: false
  schematic:
    cue:
      template: "parameter: {\n\t// +usage=Specify the number of workload\n\treplicas:
        *1 | int\n}\n// +patchStrategy=retainKeys\npatch: spec: replicas: parameter.replicas\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Specify serviceAccount for your workload which
      follows the pod spec in path 'spec.template'.
  name: service-account
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: false
  schematic:
    cue:
      template: "#Privileges: {\n\t// +usage=Specify the verbs to be allowed for the
        resource\n\tverbs: [...string]\n\t// +usage=Specify the apiGroups of the resource\n\tapiGroups?:
        [...string]\n\t// +usage=Specify the resources to be allowed\n\tresources?:
        [...string]\n\t// +usage=Specify the resourceNames to be allowed\n\tresourceNames?:
        [...string]\n\t// +usage=Specify the resource url to be allowed\n\tnonResourceURLs?:
        [...string]\n\t// +usage=Specify the scope of the privileges, default to be
        namespace scope\n\tscope: *\"namespace\" | \"cluster\"\n}\nparameter: {\n\t//
        +usage=Specify the name of ServiceAccount\n\tname: string\n\t// +usage=Specify
        whether to create new ServiceAccount or not\n\tcreate: *false | bool\n\t//
        +usage=Specify the privileges of the ServiceAccount, if not empty, RoleBindings(ClusterRoleBindings)
        will be created\n\tprivileges?: [...#Privileges]\n}\n// +patchStrategy=retainKeys\npatch:
        spec: template: spec: serviceAccountName: parameter.name\n_clusterPrivileges:
        [ if parameter.privileges != _|_ for p in parameter.privileges if p.scope
        == \"cluster\" {p}]\n_namespacePrivileges: [ if parameter.privileges != _|_
        for p in parameter.privileges if p.scope == \"namespace\" {p}]\noutputs: {\n\tif
        parameter.create {\n\t\t\"service-account\": {\n\t\t\tapiVersion: \"v1\"\n\t\t\tkind:
        \      \"ServiceAccount\"\n\t\t\tmetadata: name: parameter.name\n\t\t}\n\t}\n\tif
        parameter.privileges != _|_ {\n\t\tif len(_clusterPrivileges) > 0 {\n\t\t\t\"cluster-role\":
        {\n\t\t\t\tapiVersion: \"rbac.authorization.k8s.io/v1\"\n\t\t\t\tkind:       \"ClusterRole\"\n\t\t\t\tmetadata:
        name: \"\\(context.namespace):\\(parameter.name)\"\n\t\t\t\trules: [ for p
        in _clusterPrivileges {\n\t\t\t\t\tverbs: p.verbs\n\t\t\t\t\tif p.apiGroups
        != _|_ {\n\t\t\t\t\t\tapiGroups: p.apiGroups\n\t\t\t\t\t}\n\t\t\t\t\tif p.resources
        != _|_ {\n\t\t\t\t\t\tresources: p.resources\n\t\t\t\t\t}\n\t\t\t\t\tif p.resourceNames
        != _|_ {\n\t\t\t\t\t\tresourceNames: p.resourceNames\n\t\t\t\t\t}\n\t\t\t\t\tif
        p.nonResourceURLs != _|_ {\n\t\t\t\t\t\tnonResourceURLs: p.nonResourceURLs\n\t\t\t\t\t}\n\t\t\t\t}]\n\t\t\t}\n\t\t\t\"cluster-role-binding\":
        {\n\t\t\t\tapiVersion: \"rbac.authorization.k8s.io/v1\"\n\t\t\t\tkind:       \"ClusterRoleBinding\"\n\t\t\t\tmetadata:
        name: \"\\(context.namespace):\\(parameter.name)\"\n\t\t\t\troleRef: {\n\t\t\t\t\tapiGroup:
        \"rbac.authorization.k8s.io\"\n\t\t\t\t\tkind:     \"ClusterRole\"\n\t\t\t\t\tname:
        \    \"\\(context.namespace):\\(parameter.name)\"\n\t\t\t\t}\n\t\t\t\tsubjects:
        [{\n\t\t\t\t\tkind:      \"ServiceAccount\"\n\t\t\t\t\tname:      parameter.name\n\t\t\t\t\tnamespace:
        (context.namespace)\n\t\t\t\t}]\n\t\t\t}\n\t\t}\n\t\tif len(_namespacePrivileges)
        > 0 {\n\t\t\trole: {\n\t\t\t\tapiVersion: \"rbac.authorization.k8s.io/v1\"\n\t\t\t\tkind:
        \      \"Role\"\n\t\t\t\tmetadata: name: parameter.name\n\t\t\t\trules: [
        for p in _namespacePrivileges {\n\t\t\t\t\tverbs: p.verbs\n\t\t\t\t\tif p.apiGroups
        != _|_ {\n\t\t\t\t\t\tapiGroups: p.apiGroups\n\t\t\t\t\t}\n\t\t\t\t\tif p.resources
        != _|_ {\n\t\t\t\t\t\tresources: p.resources\n\t\t\t\t\t}\n\t\t\t\t\tif p.resourceNames
        != _|_ {\n\t\t\t\t\t\tresourceNames: p.resourceNames\n\t\t\t\t\t}\n\t\t\t\t\tif
        p.nonResourceURLs != _|_ {\n\t\t\t\t\t\tnonResourceURLs: p.nonResourceURLs\n\t\t\t\t\t}\n\t\t\t\t}]\n\t\t\t}\n\t\t\t\"role-binding\":
        {\n\t\t\t\tapiVersion: \"rbac.authorization.k8s.io/v1\"\n\t\t\t\tkind:       \"RoleBinding\"\n\t\t\t\tmetadata:
        name: parameter.name\n\t\t\t\troleRef: {\n\t\t\t\t\tapiGroup: \"rbac.authorization.k8s.io\"\n\t\t\t\t\tkind:
        \    \"Role\"\n\t\t\t\t\tname:     parameter.name\n\t\t\t\t}\n\t\t\t\tsubjects:
        [{\n\t\t\t\t\tkind: \"ServiceAccount\"\n\t\t\t\t\tname: parameter.name\n\t\t\t\t}]\n\t\t\t}\n\t\t}\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Binding secrets of cloud resources to component
      env. This definition is DEPRECATED, please use 'storage' instead.
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: service-binding
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  schematic:
    cue:
      template: "patch: spec: template: spec: {\n\t// +patchKey=name\n\tcontainers:
        [{\n\t\tname: context.name\n\t\t// +patchKey=name\n\t\tenv: [\n\t\t\tfor envName,
        v in parameter.envMappings {\n\t\t\t\tname: envName\n\t\t\t\tvalueFrom: secretKeyRef:
        {\n\t\t\t\t\tname: v.secret\n\t\t\t\t\tif v[\"key\"] != _|_ {\n\t\t\t\t\t\tkey:
        v.key\n\t\t\t\t\t}\n\t\t\t\t\tif v[\"key\"] == _|_ {\n\t\t\t\t\t\tkey: envName\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t]\n\t}]\n}\nparameter:
        {\n\t// +usage=The mapping of environment variables to secret\n\tenvMappings:
        [string]: #KeySecret\n}\n#KeySecret: {\n\tkey?:   string\n\tsecret: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Inject a sidecar container to K8s pod for your
      workload which follows the pod spec in path 'spec.template'.
  name: sidecar
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "patch: {\n\t// +patchKey=name\n\tspec: template: spec: containers:
        [{\n\t\tname:  parameter.name\n\t\timage: parameter.image\n\t\tif parameter.cmd
        != _|_ {\n\t\t\tcommand: parameter.cmd\n\t\t}\n\t\tif parameter.args != _|_
        {\n\t\t\targs: parameter.args\n\t\t}\n\t\tif parameter[\"env\"] != _|_ {\n\t\t\tenv:
        parameter.env\n\t\t}\n\t\tif parameter[\"volumes\"] != _|_ {\n\t\t\tvolumeMounts:
        [ for v in parameter.volumes {\n\t\t\t\t{\n\t\t\t\t\tmountPath: v.path\n\t\t\t\t\tname:
        \     v.name\n\t\t\t\t}\n\t\t\t}]\n\t\t}\n\t\tif parameter[\"livenessProbe\"]
        != _|_ {\n\t\t\tlivenessProbe: parameter.livenessProbe\n\t\t}\n\n\t\tif parameter[\"readinessProbe\"]
        != _|_ {\n\t\t\treadinessProbe: parameter.readinessProbe\n\t\t}\n\t}]\n}\nparameter:
        {\n\t// +usage=Specify the name of sidecar container\n\tname: string\n\n\t//
        +usage=Specify the image of sidecar container\n\timage: string\n\n\t// +usage=Specify
        the commands run in the sidecar\n\tcmd?: [...string]\n\n\t// +usage=Specify
        the args in the sidecar\n\targs?: [...string]\n\n\t// +usage=Specify the env
        in the sidecar\n\tenv?: [...{\n\t\t// +usage=Environment variable name\n\t\tname:
        string\n\t\t// +usage=The value of the environment variable\n\t\tvalue?: string\n\t\t//
        +usage=Specifies a source the value of this var should come from\n\t\tvalueFrom?:
        {\n\t\t\t// +usage=Selects a key of a secret in the pod's namespace\n\t\t\tsecretKeyRef?:
        {\n\t\t\t\t// +usage=The name of the secret in the pod's namespace to select
        from\n\t\t\t\tname: string\n\t\t\t\t// +usage=The key of the secret to select
        from. Must be a valid secret key\n\t\t\t\tkey: string\n\t\t\t}\n\t\t\t// +usage=Selects
        a key of a config map in the pod's namespace\n\t\t\tconfigMapKeyRef?: {\n\t\t\t\t//
        +usage=The name of the config map in the pod's namespace to select from\n\t\t\t\tname:
        string\n\t\t\t\t// +usage=The key of the config map to select from. Must be
        a valid secret key\n\t\t\t\tkey: string\n\t\t\t}\n\t\t\t// +usage=Specify
        the field reference for env\n\t\t\tfieldRef?: {\n\t\t\t\t// +usage=Specify
        the field path for env\n\t\t\t\tfieldPath: string\n\t\t\t}\n\t\t}\n\t}]\n\n\t//
        +usage=Specify the shared volume path\n\tvolumes?: [...{\n\t\tname: string\n\t\tpath:
        string\n\t}]\n\n\t// +usage=Instructions for assessing whether the container
        is alive.\n\tlivenessProbe?: #HealthProbe\n\n\t// +usage=Instructions for
        assessing whether the container is in a suitable state to serve traffic.\n\treadinessProbe?:
        #HealthProbe\n}\n#HealthProbe: {\n\n\t// +usage=Instructions for assessing
        container health by executing a command. Either this attribute or the httpGet
        attribute or the tcpSocket attribute MUST be specified. This attribute is
        mutually exclusive with both the httpGet attribute and the tcpSocket attribute.\n\texec?:
        {\n\t\t// +usage=A command to be executed inside the container to assess its
        health. Each space delimited token of the command is a separate array element.
        Commands exiting 0 are considered to be successful probes, whilst all other
        exit codes are considered failures.\n\t\tcommand: [...string]\n\t}\n\n\t//
        +usage=Instructions for assessing container health by executing an HTTP GET
        request. Either this attribute or the exec attribute or the tcpSocket attribute
        MUST be specified. This attribute is mutually exclusive with both the exec
        attribute and the tcpSocket attribute.\n\thttpGet?: {\n\t\t// +usage=The endpoint,
        relative to the port, to which the HTTP GET request should be directed.\n\t\tpath:
        string\n\t\t// +usage=The TCP socket within the container to which the HTTP
        GET request should be directed.\n\t\tport: int\n\t\thttpHeaders?: [...{\n\t\t\tname:
        \ string\n\t\t\tvalue: string\n\t\t}]\n\t}\n\n\t// +usage=Instructions for
        assessing container health by probing a TCP socket. Either this attribute
        or the exec attribute or the httpGet attribute MUST be specified. This attribute
        is mutually exclusive with both the exec attribute and the httpGet attribute.\n\ttcpSocket?:
        {\n\t\t// +usage=The TCP socket within the container that should be probed
        to assess container health.\n\t\tport: int\n\t}\n\n\t// +usage=Number of seconds
        after the container is started before the first probe is initiated.\n\tinitialDelaySeconds:
        *0 | int\n\n\t// +usage=How often, in seconds, to execute the probe.\n\tperiodSeconds:
        *10 | int\n\n\t// +usage=Number of seconds after which the probe times out.\n\ttimeoutSeconds:
        *1 | int\n\n\t// +usage=Minimum consecutive successes for the probe to be
        considered successful after having failed.\n\tsuccessThreshold: *1 | int\n\n\t//
        +usage=Number of consecutive failures required to determine the container
        is not alive (liveness probe) or not ready (readiness probe).\n\tfailureThreshold:
        *3 | int\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add startup probe hooks for the specified container
      of K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: startup-probe
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "#StartupProbeParams: {\n\t// +usage=Specify the name of the target
        container, if not set, use the component name\n\tcontainerName: *\"\" | string\n\t//
        +usage=Number of seconds after the container has started before liveness probes
        are initiated. Minimum value is 0.\n\tinitialDelaySeconds: *0 | int\n\t//
        +usage=How often, in seconds, to execute the probe. Minimum value is 1.\n\tperiodSeconds:
        *10 | int\n\t// +usage=Number of seconds after which the probe times out.
        Minimum value is 1.\n\ttimeoutSeconds: *1 | int\n\t// +usage=Minimum consecutive
        successes for the probe to be considered successful after having failed.  Minimum
        value is 1.\n\tsuccessThreshold: *1 | int\n\t// +usage=Minimum consecutive
        failures for the probe to be considered failed after having succeeded. Minimum
        value is 1.\n\tfailureThreshold: *3 | int\n\t// +usage=Optional duration in
        seconds the pod needs to terminate gracefully upon probe failure. Set this
        value longer than the expected cleanup time for your process.\n\tterminationGracePeriodSeconds?:
        int\n\t// +usage=Instructions for assessing container startup status by executing
        a command. Either this attribute or the httpGet attribute or the grpc attribute
        or the tcpSocket attribute MUST be specified. This attribute is mutually exclusive
        with the httpGet attribute and the tcpSocket attribute and the gRPC attribute.\n\texec?:
        {\n\t\t// +usage=A command to be executed inside the container to assess its
        health. Each space delimited token of the command is a separate array element.
        Commands exiting 0 are considered to be successful probes, whilst all other
        exit codes are considered failures.\n\t\tcommand: [...string]\n\t}\n\t// +usage=Instructions
        for assessing container startup status by executing an HTTP GET request. Either
        this attribute or the exec attribute or the grpc attribute or the tcpSocket
        attribute MUST be specified. This attribute is mutually exclusive with the
        exec attribute and the tcpSocket attribute and the gRPC attribute.\n\thttpGet?:
        {\n\t\t// +usage=The endpoint, relative to the port, to which the HTTP GET
        request should be directed.\n\t\tpath?: string\n\t\t// +usage=The port numer
        to access on the host or container.\n\t\tport: int\n\t\t// +usage=The hostname
        to connect to, defaults to the pod IP. You probably want to set \"Host\" in
        httpHeaders instead.\n\t\thost?: string\n\t\t// +usage=The Scheme to use for
        connecting to the host.\n\t\tscheme?: *\"HTTP\" | \"HTTPS\"\n\t\t// +usage=Custom
        headers to set in the request. HTTP allows repeated headers.\n\t\thttpHeaders?:
        [...{\n\t\t\t// +usage=The header field name\n\t\t\tname: string\n\t\t\t//+usage=The
        header field value\n\t\t\tvalue: string\n\t\t}]\n\t}\n\t// +usage=Instructions
        for assessing container startup status by probing a gRPC service. Either this
        attribute or the exec attribute or the grpc attribute or the httpGet attribute
        MUST be specified. This attribute is mutually exclusive with the exec attribute
        and the httpGet attribute and the tcpSocket attribute.\n\tgrpc?: {\n\t\t//
        +usage=The port number of the gRPC service.\n\t\tport: int\n\t\t// +usage=The
        name of the service to place in the gRPC HealthCheckRequest\n\t\tservice?:
        string\n\t}\n\t// +usage=Instructions for assessing container startup status
        by probing a TCP socket. Either this attribute or the exec attribute or the
        tcpSocket attribute or the httpGet attribute MUST be specified. This attribute
        is mutually exclusive with the exec attribute and the httpGet attribute and
        the gRPC attribute.\n\ttcpSocket?: {\n\t\t// +usage=Number or name of the
        port to access on the container.\n\t\tport: string\n\t\t// +usage=Host name
        to connect to, defaults to the pod IP.\n\t\thost?: string\n\t}\n}\nPatchContainer:
        {\n\t_params:         #StartupProbeParams\n\tname:            _params.containerName\n\t_baseContainers:
        context.output.spec.template.spec.containers\n\t_matchContainers_: [ for _container_
        in _baseContainers if _container_.name == name {_container_}]\n\tif len(_matchContainers_)
        == 0 {\n\t\terr: \"container \\(name) not found\"\n\t}\n\tif len(_matchContainers_)
        > 0 {\n\t\tstartupProbe: {\n\t\t\tif _params.exec != _|_ {\n\t\t\t\texec:
        _params.exec\n\t\t\t}\n\t\t\tif _params.httpGet != _|_ {\n\t\t\t\thttpGet:
        _params.httpGet\n\t\t\t}\n\t\t\tif _params.grpc != _|_ {\n\t\t\t\tgrpc: _params.grpc\n\t\t\t}\n\t\t\tif
        _params.tcpSocket != _|_ {\n\t\t\t\ttcpSocket: _params.tcpSocket\n\t\t\t}\n\t\t\tif
        _params.initialDelaySeconds != _|_ {\n\t\t\t\tinitialDelaySeconds: _params.initialDelaySeconds\n\t\t\t}\n\t\t\tif
        _params.periodSeconds != _|_ {\n\t\t\t\tperiodSeconds: _params.periodSeconds\n\t\t\t}\n\t\t\tif
        _params.tcpSocket != _|_ {\n\t\t\t\ttcpSocket: _params.tcpSocket\n\t\t\t}\n\t\t\tif
        _params.timeoutSeconds != _|_ {\n\t\t\t\ttimeoutSeconds: _params.timeoutSeconds\n\t\t\t}\n\t\t\tif
        _params.successThreshold != _|_ {\n\t\t\t\tsuccessThreshold: _params.successThreshold\n\t\t\t}\n\t\t\tif
        _params.failureThreshold != _|_ {\n\t\t\t\tfailureThreshold: _params.failureThreshold\n\t\t\t}\n\t\t\tif
        _params.terminationGracePeriodSeconds != _|_ {\n\t\t\t\tterminationGracePeriodSeconds:
        _params.terminationGracePeriodSeconds\n\t\t\t}\n\t\t}\n\t}\n}\npatch: spec:
        template: spec: {\n\tif parameter.probes == _|_ {\n\t\t// +patchKey=name\n\t\tcontainers:
        [{\n\t\t\tPatchContainer & {_params: {\n\t\t\t\tif parameter.containerName
        == \"\" {\n\t\t\t\t\tcontainerName: context.name\n\t\t\t\t}\n\t\t\t\tif parameter.containerName
        != \"\" {\n\t\t\t\t\tcontainerName: parameter.containerName\n\t\t\t\t}\n\t\t\t\tperiodSeconds:
        \                parameter.periodSeconds\n\t\t\t\tinitialDelaySeconds:           parameter.initialDelaySeconds\n\t\t\t\ttimeoutSeconds:
        \               parameter.timeoutSeconds\n\t\t\t\tsuccessThreshold:              parameter.successThreshold\n\t\t\t\tfailureThreshold:
        \             parameter.failureThreshold\n\t\t\t\tterminationGracePeriodSeconds:
        parameter.terminationGracePeriodSeconds\n\t\t\t\tif parameter.exec != _|_
        {\n\t\t\t\t\texec: parameter.exec\n\t\t\t\t}\n\t\t\t\tif parameter.httpGet
        != _|_ {\n\t\t\t\t\thttpGet: parameter.httpGet\n\t\t\t\t}\n\t\t\t\tif parameter.grpc
        != _|_ {\n\t\t\t\t\tgrpc: parameter.grpc\n\t\t\t\t}\n\t\t\t\tif parameter.tcpSocket
        != _|_ {\n\t\t\t\t\ttcpSocket: parameter.grtcpSocketpc\n\t\t\t\t}\n\t\t\t}}\n\t\t}]\n\t}\n\tif
        parameter.probes != _|_ {\n\t\t// +patchKey=name\n\t\tcontainers: [ for c
        in parameter.probes {\n\t\t\tif c.name == \"\" {\n\t\t\t\terr: \"containerName
        must be set when specifying startup probe for multiple containers\"\n\t\t\t}\n\t\t\tif
        c.name != \"\" {\n\t\t\t\tPatchContainer & {_params: c}\n\t\t\t}\n\t\t}]\n\t}\n}\nparameter:
        *#StartupProbeParams | close({\n\t// +usage=Specify the startup probe for
        multiple containers\n\tprobes: [...#StartupProbeParams]\n})\nerrs: [ for c
        in patch.spec.template.spec.containers if c.err != _|_ {c.err}]\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add storages on K8s pod for your workload which
      follows the pod spec in path 'spec.template'.
  name: storage
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "volumesList: [\n\tif parameter.pvc != _|_ for v in parameter.pvc
        {\n\t\t{\n\t\t\tname: \"pvc-\" + v.name\n\t\t\tpersistentVolumeClaim: claimName:
        v.name\n\t\t}\n\t},\n\tif parameter.configMap != _|_ for v in parameter.configMap
        if v.mountPath != _|_ {\n\t\t{\n\t\t\tname: \"configmap-\" + v.name\n\t\t\tconfigMap:
        {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tname:        v.name\n\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.secret != _|_ for v in parameter.secret if v.mountPath != _|_ {\n\t\t{\n\t\t\tname:
        \"secret-\" + v.name\n\t\t\tsecret: {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tsecretName:
        \ v.name\n\t\t\t\tif v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.emptyDir != _|_ for v in parameter.emptyDir {\n\t\t{\n\t\t\tname:
        \"emptydir-\" + v.name\n\t\t\temptyDir: medium: v.medium\n\t\t}\n\t},\n\tif
        parameter.hostPath != _|_ for v in parameter.hostPath {\n\t\t{\n\t\t\tname:
        \"hostpath-\" + v.name\n\t\t\tpath: v.path\n\t\t}\n\t},\n]\nvolumeMountsList:
        [\n\tif parameter.pvc != _|_ for v in parameter.pvc {\n\t\tif v.volumeMode
        == \"Filesystem\" {\n\t\t\t{\n\t\t\t\tname:      \"pvc-\" + v.name\n\t\t\t\tmountPath:
        v.mountPath\n\t\t\t\tif v.subPath != _|_ {\n\t\t\t\t\tsubPath: v.subPath\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.configMap != _|_ for v in parameter.configMap if v.mountPath !=
        _|_ {\n\t\t{\n\t\t\tname:      \"configmap-\" + v.name\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.secret != _|_ for v in parameter.secret if v.mountPath != _|_ {\n\t\t{\n\t\t\tname:
        \     \"secret-\" + v.name\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath
        != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t}\n\t},\n\tif parameter.emptyDir
        != _|_ for v in parameter.emptyDir {\n\t\t{\n\t\t\tname:      \"emptydir-\"
        + v.name\n\t\t\tmountPath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t}\n\t},\n\tif parameter.hostPath != _|_ for v in parameter.hostPath
        {\n\t\t{\n\t\t\tname:      \"hostpath-\" + v.name\n\t\t\tmountPath: v.mountPath\n\t\t}\n\t},\n]\nenvList:
        [\n\tif parameter.configMap != _|_ for v in parameter.configMap if v.mountToEnv
        != _|_ {\n\t\t{\n\t\t\tname: v.mountToEnv.envName\n\t\t\tvalueFrom: configMapKeyRef:
        {\n\t\t\t\tname: v.name\n\t\t\t\tkey:  v.mountToEnv.configMapKey\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.configMap != _|_ for v in parameter.configMap if v.mountToEnvs !=
        _|_ for k in v.mountToEnvs {\n\t\t{\n\t\t\tname: k.envName\n\t\t\tvalueFrom:
        configMapKeyRef: {\n\t\t\t\tname: v.name\n\t\t\t\tkey:  k.configMapKey\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.secret != _|_ for v in parameter.secret if v.mountToEnv != _|_ {\n\t\t{\n\t\t\tname:
        v.mountToEnv.envName\n\t\t\tvalueFrom: secretKeyRef: {\n\t\t\t\tname: v.name\n\t\t\t\tkey:
        \ v.mountToEnv.secretKey\n\t\t\t}\n\t\t}\n\t},\n\tif parameter.secret != _|_
        for v in parameter.secret if v.mountToEnvs != _|_ for k in v.mountToEnvs {\n\t\t{\n\t\t\tname:
        k.envName\n\t\t\tvalueFrom: secretKeyRef: {\n\t\t\t\tname: v.name\n\t\t\t\tkey:
        \ k.secretKey\n\t\t\t}\n\t\t}\n\t},\n]\nvolumeDevicesList: *[\n\t\t\tfor v
        in parameter.pvc if v.volumeMode == \"Block\" {\n\t\t{\n\t\t\tname:       \"pvc-\"
        + v.name\n\t\t\tdevicePath: v.mountPath\n\t\t\tif v.subPath != _|_ {\n\t\t\t\tsubPath:
        v.subPath\n\t\t\t}\n\t\t}\n\t},\n] | []\ndeDupVolumesArray: [\n\tfor val in
        [\n\t\tfor i, vi in volumesList {\n\t\t\tfor j, vj in volumesList if j < i
        && vi.name == vj.name {\n\t\t\t\t_ignore: true\n\t\t\t}\n\t\t\tvi\n\t\t},\n\t]
        if val._ignore == _|_ {\n\t\tval\n\t},\n]\npatch: spec: template: spec: {\n\t//
        +patchKey=name\n\tvolumes: deDupVolumesArray\n\n\tcontainers: [{\n\t\t// +patchKey=name\n\t\tenv:
        envList\n\t\t// +patchKey=name\n\t\tvolumeDevices: volumeDevicesList\n\t\t//
        +patchKey=name\n\t\tvolumeMounts: volumeMountsList\n\t}, ...]\n\n}\noutputs:
        {\n\tfor v in parameter.pvc {\n\t\tif v.mountOnly == false {\n\t\t\t\"pvc-\\(v.name)\":
        {\n\t\t\t\tapiVersion: \"v1\"\n\t\t\t\tkind:       \"PersistentVolumeClaim\"\n\t\t\t\tmetadata:
        name: v.name\n\t\t\t\tspec: {\n\t\t\t\t\taccessModes: v.accessModes\n\t\t\t\t\tvolumeMode:
        \ v.volumeMode\n\t\t\t\t\tif v.volumeName != _|_ {\n\t\t\t\t\t\tvolumeName:
        v.volumeName\n\t\t\t\t\t}\n\t\t\t\t\tif v.storageClassName != _|_ {\n\t\t\t\t\t\tstorageClassName:
        v.storageClassName\n\t\t\t\t\t}\n\n\t\t\t\t\tif v.resources.requests.storage
        == _|_ {\n\t\t\t\t\t\tresources: requests: storage: \"8Gi\"\n\t\t\t\t\t}\n\t\t\t\t\tif
        v.resources.requests.storage != _|_ {\n\t\t\t\t\t\tresources: requests: storage:
        v.resources.requests.storage\n\t\t\t\t\t}\n\t\t\t\t\tif v.resources.limits.storage
        != _|_ {\n\t\t\t\t\t\tresources: limits: storage: v.resources.limits.storage\n\t\t\t\t\t}\n\t\t\t\t\tif
        v.dataSourceRef != _|_ {\n\t\t\t\t\t\tdataSourceRef: v.dataSourceRef\n\t\t\t\t\t}\n\t\t\t\t\tif
        v.dataSource != _|_ {\n\t\t\t\t\t\tdataSource: v.dataSource\n\t\t\t\t\t}\n\t\t\t\t\tif
        v.selector != _|_ {\n\t\t\t\t\t\tdataSource: v.selector\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor
        v in parameter.configMap {\n\t\tif v.mountOnly == false {\n\t\t\t\"configmap-\\(v.name)\":
        {\n\t\t\t\tapiVersion: \"v1\"\n\t\t\t\tkind:       \"ConfigMap\"\n\t\t\t\tmetadata:
        name: v.name\n\t\t\t\tif v.data != _|_ {\n\t\t\t\t\tdata: v.data\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor
        v in parameter.secret {\n\t\tif v.mountOnly == false {\n\t\t\t\"secret-\\(v.name)\":
        {\n\t\t\t\tapiVersion: \"v1\"\n\t\t\t\tkind:       \"Secret\"\n\t\t\t\tmetadata:
        name: v.name\n\t\t\t\tif v.data != _|_ {\n\t\t\t\t\tdata: v.data\n\t\t\t\t}\n\t\t\t\tif
        v.stringData != _|_ {\n\t\t\t\t\tstringData: v.stringData\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n}\nparameter:
        {\n\t// +usage=Declare pvc type storage\n\tpvc?: [...{\n\t\tname:              string\n\t\tmountOnly:
        \        *false | bool\n\t\tmountPath:         string\n\t\tsubPath?:          string\n\t\tvolumeMode:
        \       *\"Filesystem\" | string\n\t\tvolumeName?:       string\n\t\taccessModes:
        \      *[\"ReadWriteOnce\"] | [...string]\n\t\tstorageClassName?: string\n\t\tresources?:
        {\n\t\t\trequests: storage: =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t\t\tlimits?:
        storage:  =~\"^([1-9][0-9]{0,63})(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$\"\n\t\t}\n\t\tdataSourceRef?:
        {\n\t\t\tname:     string\n\t\t\tkind:     string\n\t\t\tapiGroup: string\n\t\t}\n\t\tdataSource?:
        {\n\t\t\tname:     string\n\t\t\tkind:     string\n\t\t\tapiGroup: string\n\t\t}\n\t\tselector?:
        {\n\t\t\tmatchLabels?: [string]: string\n\t\t\tmatchExpressions?: {\n\t\t\t\tkey:
        string\n\t\t\t\tvalues: [...string]\n\t\t\t\toperator: string\n\t\t\t}\n\t\t}\n\t}]\n\n\t//
        +usage=Declare config map type storage\n\tconfigMap?: [...{\n\t\tname:      string\n\t\tmountOnly:
        *false | bool\n\t\tmountToEnv?: {\n\t\t\tenvName:      string\n\t\t\tconfigMapKey:
        string\n\t\t}\n\t\tmountToEnvs?: [...{\n\t\t\tenvName:      string\n\t\t\tconfigMapKey:
        string\n\t\t}]\n\t\tmountPath?:  string\n\t\tsubPath?:    string\n\t\tdefaultMode:
        *420 | int\n\t\treadOnly:    *false | bool\n\t\tdata?: {...}\n\t\titems?:
        [...{\n\t\t\tkey:  string\n\t\t\tpath: string\n\t\t\tmode: *511 | int\n\t\t}]\n\t}]\n\n\t//
        +usage=Declare secret type storage\n\tsecret?: [...{\n\t\tname:      string\n\t\tmountOnly:
        *false | bool\n\t\tmountToEnv?: {\n\t\t\tenvName:   string\n\t\t\tsecretKey:
        string\n\t\t}\n\t\tmountToEnvs?: [...{\n\t\t\tenvName:   string\n\t\t\tsecretKey:
        string\n\t\t}]\n\t\tmountPath?:  string\n\t\tsubPath?:    string\n\t\tdefaultMode:
        *420 | int\n\t\treadOnly:    *false | bool\n\t\tstringData?: {...}\n\t\tdata?:
        {...}\n\t\titems?: [...{\n\t\t\tkey:  string\n\t\t\tpath: string\n\t\t\tmode:
        *511 | int\n\t\t}]\n\t}]\n\n\t// +usage=Declare empty dir type storage\n\temptyDir?:
        [...{\n\t\tname:      string\n\t\tmountPath: string\n\t\tsubPath?:  string\n\t\tmedium:
        \   *\"\" | \"Memory\"\n\t}]\n\n\t// +usage=Declare host path type storage\n\thostPath?:
        [...{\n\t\tname:      string\n\t\tpath:      string\n\t\tmountPath: string\n\t\ttype:
        \     *\"Directory\" | \"DirectoryOrCreate\" | \"FileOrCreate\" | \"File\"
        | \"Socket\" | \"CharDevice\" | \"BlockDevice\"\n\t}]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add topology spread constraints hooks for every
      container of K8s pod for your workload which follows the pod spec in path 'spec.template'.
  name: topologyspreadconstraints
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  - statefulsets.apps
  - daemonsets.apps
  - jobs.batch
  podDisruptive: true
  schematic:
    cue:
      template: "constraintsArray: [\n\tfor v in parameter.constraints {\n\t\tmaxSkew:
        \          v.maxSkew\n\t\ttopologyKey:       v.topologyKey\n\t\twhenUnsatisfiable:
        v.whenUnsatisfiable\n\t\tlabelSelector:     v.labelSelector\n\t\tif v.nodeAffinityPolicy
        != _|_ {\n\t\t\tnodeAffinityPolicy: v.nodeAffinityPolicy\n\t\t}\n\t\tif v.nodeTaintsPolicy
        != _|_ {\n\t\t\tnodeTaintsPolicy: v.nodeTaintsPolicy\n\t\t}\n\t\tif v.minDomains
        != _|_ {\n\t\t\tminDomains: v.minDomains\n\t\t}\n\t\tif v.matchLabelKeys !=
        _|_ {\n\t\t\tmatchLabelKeys: v.matchLabelKeys\n\t\t}\n\t},\n]\npatch: spec:
        template: spec: topologySpreadConstraints: constraintsArray\n#labSelector:
        {\n\tmatchLabels?: [string]: string\n\tmatchExpressions?: [...{\n\t\tkey:
        \     string\n\t\toperator: *\"In\" | \"NotIn\" | \"Exists\" | \"DoesNotExist\"\n\t\tvalues?:
        [...string]\n\t}]\n}\nparameter: constraints: [...{\n\t// +usage=Describe
        the degree to which Pods may be unevenly distributed\n\tmaxSkew: int\n\t//
        +usage=Specify the key of node labels\n\ttopologyKey: string\n\t// +usage=Indicate
        how to deal with a Pod if it doesn't satisfy the spread constraint\n\twhenUnsatisfiable:
        *\"DoNotSchedule\" | \"ScheduleAnyway\"\n\t// +usage: labelSelector to find
        matching Pods\n\tlabelSelector: #labSelector\n\t// +usage=Indicate a minimum
        number of eligible domains\n\tminDomains?: int\n\t// +usage=A list of pod
        label keys to select the pods over which spreading will be calculated\n\tmatchLabelKeys?:
        [...string]\n\t// +usage=Indicate how we will treat Pod's nodeAffinity/nodeSelector
        when calculating pod topology spread skew\n\tnodeAffinityPolicy?: *\"Honor\"
        | \"Ignore\"\n\t// +usage=Indicate how we will treat node taints when calculating
        pod topology spread skew\n\tnodeTaintsPolicy?: *\"Honor\" | \"Ignore\"\n}]\n"
---
apiVersion: core.oam.dev/v1beta1
kind: TraitDefinition
metadata:
  annotations:
    definition.oam.dev/description: Add volumes on K8s pod for your workload which
      follows the pod spec in path 'spec.template'. This definition is DEPRECATED,
      please use 'storage' instead.
  labels:
    custom.definition.oam.dev/deprecated: "true"
  name: volumes
  namespace: vela-system
spec:
  appliesToWorkloads:
  - deployments.apps
  podDisruptive: true
  schematic:
    cue:
      template: "patch: {\n\t// +patchKey=name\n\tspec: template: spec: volumes: [\n\t\tif
        parameter.volumes != _|_ for v in parameter.volumes {\n\t\t\t{\n\t\t\t\tname:
        v.name\n\t\t\t\tif v.type == \"pvc\" {\n\t\t\t\t\tpersistentVolumeClaim: claimName:
        v.claimName\n\t\t\t\t}\n\t\t\t\tif v.type == \"configMap\" {\n\t\t\t\t\tconfigMap:
        {\n\t\t\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\t\t\tname:        v.cmName\n\t\t\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif
        v.type == \"secret\" {\n\t\t\t\t\tsecret: {\n\t\t\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\t\t\tsecretName:
        \ v.secretName\n\t\t\t\t\t\tif v.items != _|_ {\n\t\t\t\t\t\t\titems: v.items\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif
        v.type == \"emptyDir\" {\n\t\t\t\t\temptyDir: medium: v.medium\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t]\n}\nparameter:
        {\n\t// +usage=Declare volumes and volumeMounts\n\tvolumes?: [...{\n\t\tname:
        string\n\t\t// +usage=Specify volume type, options: \"pvc\",\"configMap\",\"secret\",\"emptyDir\",
        default to emptyDir\n\t\ttype: *\"emptyDir\" | \"pvc\" | \"configMap\" | \"secret\"\n\t\tif
        type == \"pvc\" {\n\t\t\tclaimName: string\n\t\t}\n\t\tif type == \"configMap\"
        {\n\t\t\tdefaultMode: *420 | int\n\t\t\tcmName:      string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"secret\" {\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:  string\n\t\t\titems?:
        [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode: *511 | int\n\t\t\t}]\n\t\t}\n\t\tif
        type == \"emptyDir\" {\n\t\t\tmedium: *\"\" | \"Memory\"\n\t\t}\n\t}]\n}\n"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: kubevela-vela-core-admission
  namespace: vela-system
webhooks:
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /validating-core-oam-dev-v1beta1-approllout
  failurePolicy: Ignore
  name: validating.core.oam.dev.v1beta1.approllouts
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    resources:
    - approllouts
    scope: Namespaced
  sideEffects: None
  timeoutSeconds: 5
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /validating-core-oam-dev-v1alpha2-traitdefinitions
  failurePolicy: Ignore
  name: validating.core.oam.dev.v1alpha2.traitdefinitions
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1alpha2
    operations:
    - CREATE
    - UPDATE
    resources:
    - traitdefinitions
    scope: Cluster
  sideEffects: None
  timeoutSeconds: 5
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /validate-standard-oam-dev-v1alpha1-podspecworkload
  failurePolicy: Fail
  name: vcontainerized.kb.io
  rules:
  - apiGroups:
    - standard.oam.dev
    apiVersions:
    - v1alpha1
    operations:
    - CREATE
    - UPDATE
    resources:
    - podspecworkloads
  sideEffects: None
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /validating-core-oam-dev-v1beta1-applications
  failurePolicy: Ignore
  name: validating.core.oam.dev.v1beta1.applications
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    resources:
    - applications
  sideEffects: None
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /validating-core-oam-dev-v1beta1-componentdefinitions
  failurePolicy: Ignore
  name: validating.core.oam-dev.v1beta1.componentdefinitions
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    resources:
    - componentdefinitions
  sideEffects: None
- admissionReviewVersions:
  - v1beta1
  - v1
  clientConfig:
    caBundle: Cg==
    service:
      name: vela-core-webhook
      namespace: vela-system
      path: /validating-core-oam-dev-v1beta1-initializers
  failurePolicy: Ignore
  name: validating.core.oam-dev.v1beta1.initializers
  rules:
  - apiGroups:
    - core.oam.dev
    apiVersions:
    - v1beta1
    operations:
    - CREATE
    - UPDATE
    - DELETE
    resources:
    - initializers
  sideEffects: None
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply components of an application in parallel
      for your workflow steps
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-application-in-parallel
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\noutput: op.#ApplyApplicationInParallel
        & {}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply application for your workflow steps, it
      has no arguments, should be used for custom steps before or after application
      applied.
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-application
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\n// apply application\noutput: op.#ApplyApplication
        & {}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply a specific component and its corresponding
      traits in application
  labels:
    custom.definition.oam.dev/scope: Application
  name: apply-component
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "parameter: {\n\t// +usage=Specify the component name to apply\n\tcomponent:
        string\n\t// +usage=Specify the cluster\n\tcluster: *\"\" | string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Apply deployment with specified image and cmd.
  name: apply-deployment
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"strconv\"\n\t\"strings\"\n\t\"vela/op\"\n)\n\noutput:
        op.#Apply & {\n\tvalue: {\n\t\tapiVersion: \"apps/v1\"\n\t\tkind:       \"Deployment\"\n\t\tmetadata:
        {\n\t\t\tname:      context.stepName\n\t\t\tnamespace: context.namespace\n\t\t}\n\t\tspec:
        {\n\t\t\tselector: matchLabels: \"workflow.oam.dev/step-name\": \"\\(context.name)-\\(context.stepName)\"\n\t\t\ttemplate:
        {\n\t\t\t\tmetadata: labels: \"workflow.oam.dev/step-name\": \"\\(context.name)-\\(context.stepName)\"\n\t\t\t\tspec:
        containers: [{\n\t\t\t\t\tname:  context.stepName\n\t\t\t\t\timage: parameter.image\n\t\t\t\t\tif
        parameter[\"cmd\"] != _|_ {\n\t\t\t\t\t\tcommand: parameter.cmd\n\t\t\t\t\t}\n\t\t\t\t}]\n\t\t\t}\n\t\t}\n\t}\n}\nwait:
        op.#ConditionalWait & {\n\tcontinue: output.value.status.readyReplicas ==
        1\n}\nparameter: {\n\timage: string\n\tcmd?: [...string]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply raw kubernetes objects for your workflow
      steps
  labels:
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-object
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napply: op.#Apply & {\n\tvalue:   parameter.value\n\tcluster:
        parameter.cluster\n}\nparameter: {\n\t// +usage=Specify Kubernetes native
        resource object to be applied\n\tvalue: {...}\n\t// +usage=The cluster you
        want to apply the resource to, default is the current control plane cluster\n\tcluster:
        *\"\" | string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Apply remaining components and traits
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: apply-remaining
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\n// apply remaining components and traits\napply:
        op.#ApplyRemaining & {\n\tparameter\n}\nparameter: {\n\t// +usage=Declare
        the name of the component\n\texceptions?: [...string]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Apply terraform configuration in the step
    definition.oam.dev/example-url: https://raw.githubusercontent.com/kubevela/workflow/main/examples/workflow-run/apply-terraform-resource.yaml
  name: apply-terraform-config
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napply: op.#Apply & {\n\tvalue: {\n\t\tapiVersion:
        \"terraform.core.oam.dev/v1beta2\"\n\t\tkind:       \"Configuration\"\n\t\tmetadata:
        {\n\t\t\tname:      \"\\(context.name)-\\(context.stepName)\"\n\t\t\tnamespace:
        context.namespace\n\t\t}\n\t\tspec: {\n\t\t\tdeleteResource: parameter.deleteResource\n\t\t\tvariable:
        \      parameter.variable\n\t\t\tforceDelete:    parameter.forceDelete\n\t\t\tif
        parameter.source.path != _|_ {\n\t\t\t\tpath: parameter.source.path\n\t\t\t}\n\t\t\tif
        parameter.source.remote != _|_ {\n\t\t\t\tremote: parameter.source.remote\n\t\t\t}\n\t\t\tif
        parameter.source.hcl != _|_ {\n\t\t\t\thcl: parameter.source.hcl\n\t\t\t}\n\t\t\tif
        parameter.providerRef != _|_ {\n\t\t\t\tproviderRef: parameter.providerRef\n\t\t\t}\n\t\t\tif
        parameter.jobEnv != _|_ {\n\t\t\t\tjobEnv: parameter.jobEnv\n\t\t\t}\n\t\t\tif
        parameter.writeConnectionSecretToRef != _|_ {\n\t\t\t\twriteConnectionSecretToRef:
        parameter.writeConnectionSecretToRef\n\t\t\t}\n\t\t\tif parameter.region !=
        _|_ {\n\t\t\t\tregion: parameter.region\n\t\t\t}\n\t\t}\n\t}\n}\ncheck: op.#ConditionalWait
        & {\n\tcontinue: apply.value.status != _|_ && apply.value.status.apply !=
        _|_ && apply.value.status.apply.state == \"Available\"\n}\nparameter: {\n\t//
        +usage=specify the source of the terraform configuration\n\tsource: close({\n\t\t//
        +usage=directly specify the hcl of the terraform configuration\n\t\thcl: string\n\t})
        | close({\n\t\t// +usage=specify the remote url of the terraform configuration\n\t\tremote:
        *\"https://github.com/kubevela-contrib/terraform-modules.git\" | string\n\t\t//
        +usage=specify the path of the terraform configuration\n\t\tpath?: string\n\t})\n\t//
        +usage=whether to delete resource\n\tdeleteResource: *true | bool\n\t// +usage=the
        variable in the configuration\n\tvariable: {...}\n\t// +usage=this specifies
        the namespace and name of a secret to which any connection details for this
        managed resource should be written.\n\twriteConnectionSecretToRef?: {\n\t\tname:
        \     string\n\t\tnamespace: *context.namespace | string\n\t}\n\t// +usage=providerRef
        specifies the reference to Provider\n\tproviderRef?: {\n\t\tname:      string\n\t\tnamespace:
        *context.namespace | string\n\t}\n\t// +usage=region is cloud provider's region.
        It will override the region in the region field of providerRef\n\tregion?:
        string\n\t// +usage=the envs for job\n\tjobEnv?: {...}\n\t// +usae=forceDelete
        will force delete Configuration no matter which state it is or whether it
        has provisioned some resources\n\tforceDelete: *false | bool\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Apply terraform provider config
    definition.oam.dev/example-url: https://raw.githubusercontent.com/kubevela/workflow/main/examples/workflow-run/apply-terraform-resource.yaml
  name: apply-terraform-provider
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"strings\"\n)\n\nconfig: op.#CreateConfig
        & {\n\tname:      \"\\(context.name)-\\(context.stepName)\"\n\tnamespace:
        context.namespace\n\ttemplate:  \"terraform-\\(parameter.type)\"\n\tconfig:
        {\n\t\tname: parameter.name\n\t\tif parameter.type == \"alibaba\" {\n\t\t\tALICLOUD_ACCESS_KEY:
        parameter.accessKey\n\t\t\tALICLOUD_SECRET_KEY: parameter.secretKey\n\t\t\tALICLOUD_REGION:
        \    parameter.region\n\t\t}\n\t\tif parameter.type == \"aws\" {\n\t\t\tAWS_ACCESS_KEY_ID:
        \    parameter.accessKey\n\t\t\tAWS_SECRET_ACCESS_KEY: parameter.secretKey\n\t\t\tAWS_DEFAULT_REGION:
        \   parameter.region\n\t\t\tAWS_SESSION_TOKEN:     parameter.token\n\t\t}\n\t\tif
        parameter.type == \"azure\" {\n\t\t\tARM_CLIENT_ID:       parameter.clientID\n\t\t\tARM_CLIENT_SECRET:
        \  parameter.clientSecret\n\t\t\tARM_SUBSCRIPTION_ID: parameter.subscriptionID\n\t\t\tARM_TENANT_ID:
        \      parameter.tenantID\n\t\t}\n\t\tif parameter.type == \"baidu\" {\n\t\t\tBAIDUCLOUD_ACCESS_KEY:
        parameter.accessKey\n\t\t\tBAIDUCLOUD_SECRET_KEY: parameter.secretKey\n\t\t\tBAIDUCLOUD_REGION:
        \    parameter.region\n\t\t}\n\t\tif parameter.type == \"ec\" {\n\t\t\tEC_API_KEY:
        parameter.apiKey\n\t\t}\n\t\tif parameter.type == \"gcp\" {\n\t\t\tGOOGLE_CREDENTIALS:
        parameter.credentials\n\t\t\tGOOGLE_REGION:      parameter.region\n\t\t\tGOOGLE_PROJECT:
        \    parameter.project\n\t\t}\n\t\tif parameter.type == \"tencent\" {\n\t\t\tTENCENTCLOUD_SECRET_ID:
        \ parameter.secretID\n\t\t\tTENCENTCLOUD_SECRET_KEY: parameter.secretKey\n\t\t\tTENCENTCLOUD_REGION:
        \    parameter.region\n\t\t}\n\t\tif parameter.type == \"ucloud\" {\n\t\t\tUCLOUD_PRIVATE_KEY:
        parameter.privateKey\n\t\t\tUCLOUD_PUBLIC_KEY:  parameter.publicKey\n\t\t\tUCLOUD_PROJECT_ID:
        \ parameter.projectID\n\t\t\tUCLOUD_REGION:      parameter.region\n\t\t}\n\t}\n}\nread:
        op.#Read & {\n\tvalue: {\n\t\tapiVersion: \"terraform.core.oam.dev/v1beta1\"\n\t\tkind:
        \      \"Provider\"\n\t\tmetadata: {\n\t\t\tname:      parameter.name\n\t\t\tnamespace:
        context.namespace\n\t\t}\n\t}\n}\ncheck: op.#ConditionalWait & {\n\tif read.value.status
        != _|_ {\n\t\tcontinue: read.value.status.state == \"ready\"\n\t}\n\tif read.value.status
        == _|_ {\n\t\tcontinue: false\n\t}\n}\nproviderBasic: {\n\taccessKey: string\n\tsecretKey:
        string\n\tregion:    string\n}\n#AlibabaProvider: {\n\tproviderBasic\n\ttype:
        \"alibaba\"\n\tname: *\"alibaba-provider\" | string\n}\n#AWSProvider: {\n\tproviderBasic\n\ttoken:
        *\"\" | string\n\ttype:  \"aws\"\n\tname:  *\"aws-provider\" | string\n}\n#AzureProvider:
        {\n\tsubscriptionID: string\n\ttenantID:       string\n\tclientID:       string\n\tclientSecret:
        \  string\n\tname:           *\"azure-provider\" | string\n}\n#BaiduProvider:
        {\n\tproviderBasic\n\ttype: \"baidu\"\n\tname: *\"baidu-provider\" | string\n}\n#ECProvider:
        {\n\ttype:   \"ec\"\n\tapiKey: *\"\" | string\n\tname:   *\"ec-provider\"
        | string\n}\n#GCPProvider: {\n\tcredentials: string\n\tregion:      string\n\tproject:
        \    string\n\ttype:        \"gcp\"\n\tname:        *\"gcp-provider\" | string\n}\n#TencentProvider:
        {\n\tsecretID:  string\n\tsecretKey: string\n\tregion:    string\n\ttype:
        \     \"tencent\"\n\tname:      *\"tencent-provider\" | string\n}\n#UCloudProvider:
        {\n\tpublicKey:  string\n\tprivateKey: string\n\tprojectID:  string\n\tregion:
        \    string\n\ttype:       \"ucloud\"\n\tname:       *\"ucloud-provider\"
        | string\n}\nparameter: *#AlibabaProvider | #AWSProvider | #AzureProvider
        | #BaiduProvider | #ECProvider | #GCPProvider | #TencentProvider | #UCloudProvider\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Build and push image from git url
    definition.oam.dev/example-url: https://raw.githubusercontent.com/kubevela/workflow/main/examples/workflow-run/built-push-image.yaml
  name: build-push-image
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/json\"\n\t\"strings\"\n)\n\nurl:
        {\n\tif parameter.context.git != _|_ {\n\t\taddress: strings.TrimPrefix(parameter.context.git,
        \"git://\")\n\t\tvalue:   \"git://\\(address)#refs/heads/\\(parameter.context.branch)\"\n\t}\n\tif
        parameter.context.git == _|_ {\n\t\tvalue: parameter.context\n\t}\n}\nkaniko:
        op.#Apply & {\n\tvalue: {\n\t\tapiVersion: \"v1\"\n\t\tkind:       \"Pod\"\n\t\tmetadata:
        {\n\t\t\tname:      \"\\(context.name)-\\(context.stepSessionID)-kaniko\"\n\t\t\tnamespace:
        context.namespace\n\t\t}\n\t\tspec: {\n\t\t\tcontainers: [\n\t\t\t\t{\n\t\t\t\t\targs:
        [\n\t\t\t\t\t\t\"--dockerfile=\\(parameter.dockerfile)\",\n\t\t\t\t\t\t\"--context=\\(url.value)\",\n\t\t\t\t\t\t\"--destination=\\(parameter.image)\",\n\t\t\t\t\t\t\"--verbosity=\\(parameter.verbosity)\",\n\t\t\t\t\t\tif
        parameter.platform != _|_ {\n\t\t\t\t\t\t\t\"--customPlatform=\\(parameter.platform)\"\n\t\t\t\t\t\t},\n\t\t\t\t\t\tif
        parameter.buildArgs != _|_ for arg in parameter.buildArgs {\n\t\t\t\t\t\t\t\"--build-arg=\\(arg)\"\n\t\t\t\t\t\t},\n\t\t\t\t\t]\n\t\t\t\t\timage:
        parameter.kanikoExecutor\n\t\t\t\t\tname:  \"kaniko\"\n\t\t\t\t\tif parameter.credentials
        != _|_ && parameter.credentials.image != _|_ {\n\t\t\t\t\t\tvolumeMounts:
        [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tmountPath: \"/kaniko/.docker/\"\n\t\t\t\t\t\t\t\tname:
        \     parameter.credentials.image.name\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t\tif
        parameter.credentials != _|_ && parameter.credentials.git != _|_ {\n\t\t\t\t\t\tenv:
        [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tname: \"GIT_TOKEN\"\n\t\t\t\t\t\t\t\tvalueFrom:
        secretKeyRef: {\n\t\t\t\t\t\t\t\t\tkey:  parameter.credentials.git.key\n\t\t\t\t\t\t\t\t\tname:
        parameter.credentials.git.name\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t]\n\t\t\tif
        parameter.credentials != _|_ && parameter.credentials.image != _|_ {\n\t\t\t\tvolumes:
        [\n\t\t\t\t\t{\n\t\t\t\t\t\tname: parameter.credentials.image.name\n\t\t\t\t\t\tsecret:
        {\n\t\t\t\t\t\t\tdefaultMode: 420\n\t\t\t\t\t\t\titems: [\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tkey:
        \ parameter.credentials.image.key\n\t\t\t\t\t\t\t\t\tpath: \"config.json\"\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t\tsecretName:
        parameter.credentials.image.name\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t]\n\t\t\t}\n\t\t\trestartPolicy:
        \"Never\"\n\t\t}\n\t}\n}\nlog: op.#Log & {\n\tsource: resources: [{\n\t\tname:
        \     \"\\(context.name)-\\(context.stepSessionID)-kaniko\"\n\t\tnamespace:
        context.namespace\n\t}]\n}\nread: op.#Read & {\n\tvalue: {\n\t\tapiVersion:
        \"v1\"\n\t\tkind:       \"Pod\"\n\t\tmetadata: {\n\t\t\tname:      \"\\(context.name)-\\(context.stepSessionID)-kaniko\"\n\t\t\tnamespace:
        context.namespace\n\t\t}\n\t}\n}\nwait: op.#ConditionalWait & {\n\tcontinue:
        read.value.status != _|_ && read.value.status.phase == \"Succeeded\"\n}\n#secret:
        {\n\tname: string\n\tkey:  string\n}\n#git: {\n\tgit:    string\n\tbranch:
        *\"master\" | string\n}\nparameter: {\n\t// +usage=Specify the kaniko executor
        image, default to oamdev/kaniko-executor:v1.9.1\n\tkanikoExecutor: *\"oamdev/kaniko-executor:v1.9.1\"
        | string\n\t// +usage=Specify the context to build image, you can use context
        with git and branch or directly specify the context, please refer to https://github.com/GoogleContainerTools/kaniko#kaniko-build-contexts\n\tcontext:
        #git | string\n\t// +usage=Specify the dockerfile\n\tdockerfile: *\"./Dockerfile\"
        | string\n\t// +usage=Specify the image\n\timage: string\n\t// +usage=Specify
        the platform to build\n\tplatform?: string\n\t// +usage=Specify the build
        args\n\tbuildArgs?: [...string]\n\t// +usage=Specify the credentials to access
        git and image registry\n\tcredentials?: {\n\t\t// +usage=Specify the credentials
        to access git\n\t\tgit?: {\n\t\t\t// +usage=Specify the secret name\n\t\t\tname:
        string\n\t\t\t// +usage=Specify the secret key\n\t\t\tkey: string\n\t\t}\n\t\t//
        +usage=Specify the credentials to access image registry\n\t\timage?: {\n\t\t\t//
        +usage=Specify the secret name\n\t\t\tname: string\n\t\t\t// +usage=Specify
        the secret key\n\t\t\tkey: *\".dockerconfigjson\" | string\n\t\t}\n\t}\n\t//
        +usage=Specify the verbosity level\n\tverbosity: *\"info\" | \"panic\" | \"fatal\"
        | \"error\" | \"warn\" | \"debug\" | \"trace\"\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: clean applied jobs in the cluster
  name: clean-jobs
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\nparameter: {\n\tlabelselector?: {...}\n\tnamespace:
        *context.namespace | string\n}\ncleanJobs: op.#Delete & {\n\tvalue: {\n\t\tapiVersion:
        \"batch/v1\"\n\t\tkind:       \"Job\"\n\t\tmetadata: {\n\t\t\tname:      context.name\n\t\t\tnamespace:
        parameter.namespace\n\t\t}\n\t}\n\tfilter: {\n\t\tnamespace: parameter.namespace\n\t\tif
        parameter.labelselector != _|_ {\n\t\t\tmatchingLabels: parameter.labelselector\n\t\t}\n\t\tif
        parameter.labelselector == _|_ {\n\t\t\tmatchingLabels: \"workflow.oam.dev/name\":
        context.name\n\t\t}\n\t}\n}\ncleanPods: op.#Delete & {\n\tvalue: {\n\t\tapiVersion:
        \"v1\"\n\t\tkind:       \"pod\"\n\t\tmetadata: {\n\t\t\tname:      context.name\n\t\t\tnamespace:
        parameter.namespace\n\t\t}\n\t}\n\tfilter: {\n\t\tnamespace: parameter.namespace\n\t\tif
        parameter.labelselector != _|_ {\n\t\t\tmatchingLabels: parameter.labelselector\n\t\t}\n\t\tif
        parameter.labelselector == _|_ {\n\t\t\tmatchingLabels: \"workflow.oam.dev/name\":
        context.name\n\t\t}\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Collect service endpoints for the application.
  name: collect-service-endpoints
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"vela/ql\"\n\t\"strconv\"\n)\n\ncollect:
        ql.#CollectServiceEndpoints & {\n\tapp: {\n\t\tname:      *context.name |
        string\n\t\tnamespace: *context.namespace | string\n\t\tif parameter.name
        != _|_ {\n\t\t\tname: parameter.name\n\t\t}\n\t\tif parameter.namespace !=
        _|_ {\n\t\t\tnamespace: parameter.namespace\n\t\t}\n\t\tfilter: {\n\t\t\tif
        parameter.components != _|_ {\n\t\t\t\tcomponents: parameter.components\n\t\t\t}\n\t\t}\n\t}\n}
        @step(1)\noutputs: {\n\teps_port_name_filtered: *[] | [...]\n\tif parameter.portName
        == _|_ {\n\t\teps_port_name_filtered: collect.list\n\t}\n\tif parameter.portName
        != _|_ {\n\t\teps_port_name_filtered: [ for ep in collect.list if parameter.portName
        == ep.endpoint.portName {ep}]\n\t}\n\n\teps_port_filtered: *[] | [...]\n\tif
        parameter.port == _|_ {\n\t\teps_port_filtered: eps_port_name_filtered\n\t}\n\tif
        parameter.port != _|_ {\n\t\teps_port_filtered: [ for ep in eps_port_name_filtered
        if parameter.port == ep.endpoint.port {ep}]\n\t}\n\teps:       eps_port_filtered\n\tendpoints:
        *[] | [...]\n\tif parameter.outer != _|_ {\n\t\ttmps: [ for ep in eps {\n\t\t\tep\n\t\t\tif
        ep.endpoint.inner == _|_ {\n\t\t\t\touter: true\n\t\t\t}\n\t\t\tif ep.endpoint.inner
        != _|_ {\n\t\t\t\touter: !ep.endpoint.inner\n\t\t\t}\n\t\t}]\n\t\tendpoints:
        [ for ep in tmps if (!parameter.outer || ep.outer) {ep}]\n\t}\n\tif parameter.outer
        == _|_ {\n\t\tendpoints: eps_port_filtered\n\t}\n}\nwait: op.#ConditionalWait
        & {\n\tcontinue: len(outputs.endpoints) > 0\n} @step(2)\nvalue: {\n\tif len(outputs.endpoints)
        > 0 {\n\t\tendpoint: outputs.endpoints[0].endpoint\n\t\t_portStr: strconv.FormatInt(endpoint.port,
        10)\n\t\turl:      \"\\(parameter.protocal)://\\(endpoint.host):\\(_portStr)\"\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the name of the application\n\tname?: string\n\t//
        +usage=Specify the namespace of the application\n\tnamespace?: string\n\t//
        +usage=Filter the component of the endpoints\n\tcomponents?: [...string]\n\t//
        +usage=Filter the port of the endpoints\n\tport?: int\n\t// +usage=Filter
        the port name of the endpoints\n\tportName?: string\n\t// +usage=Filter the
        endpoint that are only outer\n\touter?: bool\n\t// +usage=The protocal of
        endpoint url\n\tprotocal: *\"http\" | \"https\"\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Create or update a config
  name: create-config
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\ndeploy: op.#CreateConfig & {\n\tname:
        parameter.name\n\tif parameter.namespace != _|_ {\n\t\tnamespace: parameter.namespace\n\t}\n\tif
        parameter.namespace == _|_ {\n\t\tnamespace: context.namespace\n\t}\n\tif
        parameter.template != _|_ {\n\t\ttemplate: parameter.template\n\t}\n\tconfig:
        parameter.config\n}\nparameter: {\n\t//+usage=Specify the name of the config.\n\tname:
        string\n\n\t//+usage=Specify the namespace of the config.\n\tnamespace?: string\n\n\t//+usage=Specify
        the template of the config.\n\ttemplate?: string\n\n\t//+usage=Specify the
        content of the config.\n\tconfig: {...}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Delete a config
  name: delete-config
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\ndeploy: op.#DeleteConfig & {\n\tname:
        parameter.name\n\tif parameter.namespace != _|_ {\n\t\tnamespace: parameter.namespace\n\t}\n\tif
        parameter.namespace == _|_ {\n\t\tnamespace: context.namespace\n\t}\n}\nparameter:
        {\n\t//+usage=Specify the name of the config.\n\tname: string\n\n\t//+usage=Specify
        the namespace of the config.\n\tnamespace?: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Wait for the specified Application to complete.
  name: depends-on-app
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/yaml\"\n)\n\ndependsOn: op.#Read
        & {\n\tvalue: {\n\t\tapiVersion: \"core.oam.dev/v1beta1\"\n\t\tkind:       \"Application\"\n\t\tmetadata:
        {\n\t\t\tname:      parameter.name\n\t\t\tnamespace: parameter.namespace\n\t\t}\n\t}\n}\nload:
        op.#Steps & {\n\tif dependsOn.err != _|_ {\n\t\tconfigMap: op.#Read & {\n\t\t\tvalue:
        {\n\t\t\t\tapiVersion: \"v1\"\n\t\t\t\tkind:       \"ConfigMap\"\n\t\t\t\tmetadata:
        {\n\t\t\t\t\tname:      parameter.name\n\t\t\t\t\tnamespace: parameter.namespace\n\t\t\t\t}\n\t\t\t}\n\t\t}
        \        @step(1)\n\t\ttemplate: configMap.value.data[\"application\"]\n\t\tapply:
        \   op.#Apply & {\n\t\t\tvalue: yaml.Unmarshal(template)\n\t\t}     @step(2)\n\t\twait:
        op.#ConditionalWait & {\n\t\t\tcontinue: apply.value.status.status == \"running\"\n\t\t}
        @step(3)\n\t}\n\n\tif dependsOn.err == _|_ {\n\t\twait: op.#ConditionalWait
        & {\n\t\t\tcontinue: dependsOn.value.status.status == \"running\"\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Specify the name of the dependent Application\n\tname: string\n\t//
        +usage=Specify the namespace of the dependent Application\n\tnamespace: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Deploy cloud resource and deliver secret to multi
      clusters.
  labels:
    custom.definition.oam.dev/scope: Application
  name: deploy-cloud-resource
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napp: op.#DeployCloudResource & {\n\tenv:
        \   parameter.env\n\tpolicy: parameter.policy\n\t// context.namespace indicates
        the namespace of the app\n\tnamespace: context.namespace\n\t// context.namespace
        indicates the name of the app\n\tname: context.name\n}\nparameter: {\n\t//
        +usage=Declare the name of the env-binding policy, if empty, the first env-binding
        policy will be used\n\tpolicy: *\"\" | string\n\t// +usage=Declare the name
        of the env in policy\n\tenv: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: A powerful and unified deploy step for components
      multi-cluster delivery with policies.
  labels:
    custom.definition.oam.dev/scope: Application
  name: deploy
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\ndeploy: op.#Deploy & {\n\tpolicies:
        \                parameter.policies\n\tparallelism:              parameter.parallelism\n\tignoreTerraformComponent:
        parameter.ignoreTerraformComponent\n}\nparameter: {\n\t//+usage=If set to
        false, the workflow will suspend automatically before this step, default to
        be true.\n\tauto: *true | bool\n\t//+usage=Declare the policies that used
        for this deployment. If not specified, the components will be deployed to
        the hub cluster.\n\tpolicies?: [...string]\n\t//+usage=Maximum number of concurrent
        delivered components.\n\tparallelism: *5 | int\n\t//+usage=If set false, this
        step will apply the components with the terraform workload.\n\tignoreTerraformComponent:
        *true | bool\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Deploy env binding component to target env
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: deploy2env
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napp: op.#ApplyEnvBindApp & {\n\tenv:
        \     parameter.env\n\tpolicy:   parameter.policy\n\tparallel: parameter.parallel\n\tapp:
        \     context.name\n\t// context.namespace indicates the namespace of the
        app\n\tnamespace: context.namespace\n}\nparameter: {\n\t// +usage=Declare
        the name of the env-binding policy, if empty, the first env-binding policy
        will be used\n\tpolicy: *\"\" | string\n\t// +usage=Declare the name of the
        env in policy\n\tenv: string\n\t// +usage=components are applied in parallel\n\tparallel:
        *false | bool\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Deploy application to runtime clusters
  labels:
    custom.definition.oam.dev/deprecated: "true"
    custom.definition.oam.dev/scope: Application
    custom.definition.oam.dev/ui-hidden: "true"
  name: deploy2runtime
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napp: op.#Steps & {\n\tload: op.#Load\n\tclusters:
        [...string]\n\tif parameter.clusters == _|_ {\n\t\tlistClusters: op.#ListClusters\n\t\tclusters:
        \    listClusters.outputs.clusters\n\t}\n\tif parameter.clusters != _|_ {\n\t\tclusters:
        parameter.clusters\n\t}\n\n\tapply: op.#Steps & {\n\t\tfor _, cluster_ in
        clusters {\n\t\t\tfor name, c in load.value {\n\t\t\t\t\"\\(cluster_)-\\(name)\":
        op.#ApplyComponent & {\n\t\t\t\t\tvalue:   c\n\t\t\t\t\tcluster: cluster_\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nparameter:
        {\n\t// +usage=Declare the runtime clusters to apply, if empty, all runtime
        clusters will be used\n\tclusters?: [...string]\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Export data to clusters specified by topology.
  name: export-data
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\nobject: {\n\tapiVersion: \"v1\"\n\tkind:
        \      parameter.kind\n\tmetadata: {\n\t\tname:      *context.name | string\n\t\tnamespace:
        *context.namespace | string\n\t\tif parameter.name != _|_ {\n\t\t\tname: parameter.name\n\t\t}\n\t\tif
        parameter.namespace != _|_ {\n\t\t\tnamespace: parameter.namespace\n\t\t}\n\t}\n\tif
        parameter.kind == \"ConfigMap\" {\n\t\tdata: parameter.data\n\t}\n\tif parameter.kind
        == \"Secret\" {\n\t\tstringData: parameter.data\n\t}\n} @step(1)\ngetPlacements:
        op.#GetPlacementsFromTopologyPolicies & {\n\tpolicies: *[] | [...string]\n\tif
        parameter.topology != _|_ {\n\t\tpolicies: [parameter.topology]\n\t}\n}      @step(2)\napply:
        op.#Steps & {\n\tfor p in getPlacements.placements {\n\t\t(p.cluster): op.#Apply
        & {\n\t\t\tvalue:   object\n\t\t\tcluster: p.cluster\n\t\t}\n\t}\n} @step(3)\nparameter:
        {\n\t// +usage=Specify the name of the export destination\n\tname?: string\n\t//
        +usage=Specify the namespace of the export destination\n\tnamespace?: string\n\t//
        +usage=Specify the kind of the export destination\n\tkind: *\"ConfigMap\"
        | \"Secret\"\n\t// +usage=Specify the data to export\n\tdata: {}\n\t// +usage=Specify
        the topology to export\n\ttopology?: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Export service to clusters specified by topology.
  name: export-service
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\nmeta: {\n\tname:      *context.name
        | string\n\tnamespace: *context.namespace | string\n\tif parameter.name !=
        _|_ {\n\t\tname: parameter.name\n\t}\n\tif parameter.namespace != _|_ {\n\t\tnamespace:
        parameter.namespace\n\t}\n}\nobjects: [{\n\tapiVersion: \"v1\"\n\tkind:       \"Service\"\n\tmetadata:
        \  meta\n\tspec: {\n\t\ttype: \"ClusterIP\"\n\t\tports: [{\n\t\t\tprotocol:
        \  \"TCP\"\n\t\t\tport:       parameter.port\n\t\t\ttargetPort: parameter.targetPort\n\t\t}]\n\t}\n},
        {\n\tapiVersion: \"v1\"\n\tkind:       \"Endpoints\"\n\tmetadata:   meta\n\tsubsets:
        [{\n\t\taddresses: [{ip: parameter.ip}]\n\t\tports: [{port: parameter.targetPort}]\n\t}]\n}]
        @step(1)\ngetPlacements: op.#GetPlacementsFromTopologyPolicies & {\n\tpolicies:
        *[] | [...string]\n\tif parameter.topology != _|_ {\n\t\tpolicies: [parameter.topology]\n\t}\n}
        \     @step(2)\napply: op.#Steps & {\n\tfor p in getPlacements.placements
        {\n\t\tfor o in objects {\n\t\t\t\"\\(p.cluster)-\\(o.kind)\": op.#Apply &
        {\n\t\t\t\tvalue:   o\n\t\t\t\tcluster: p.cluster\n\t\t\t}\n\t\t}\n\t}\n}
        @step(3)\nparameter: {\n\t// +usage=Specify the name of the export destination\n\tname?:
        string\n\t// +usage=Specify the namespace of the export destination\n\tnamespace?:
        string\n\t// +usage=Specify the ip to be export\n\tip: string\n\t// +usage=Specify
        the port to be used in service\n\tport: int\n\t// +usage=Specify the port
        to be export\n\ttargetPort: int\n\t// +usage=Specify the topology to export\n\ttopology?:
        string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Export data to specified Kubernetes ConfigMap
      in your workflow.
  name: export2config
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napply: op.#Apply & {\n\tvalue: {\n\t\tapiVersion:
        \"v1\"\n\t\tkind:       \"ConfigMap\"\n\t\tmetadata: {\n\t\t\tname: parameter.configName\n\t\t\tif
        parameter.namespace != _|_ {\n\t\t\t\tnamespace: parameter.namespace\n\t\t\t}\n\t\t\tif
        parameter.namespace == _|_ {\n\t\t\t\tnamespace: context.namespace\n\t\t\t}\n\t\t}\n\t\tdata:
        parameter.data\n\t}\n\tcluster: parameter.cluster\n}\nparameter: {\n\t// +usage=Specify
        the name of the config map\n\tconfigName: string\n\t// +usage=Specify the
        namespace of the config map\n\tnamespace?: string\n\t// +usage=Specify the
        data of config map\n\tdata: {}\n\t// +usage=Specify the cluster of the config
        map\n\tcluster: *\"\" | string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Export data to Kubernetes Secret in your workflow.
  name: export2secret
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n)\n\nsecret:
        op.#Steps & {\n\tdata: *parameter.data | {}\n\tif parameter.kind == \"docker-registry\"
        && parameter.dockerRegistry != _|_ {\n\t\tregistryData: auths: \"\\(parameter.dockerRegistry.server)\":
        {\n\t\t\tusername: parameter.dockerRegistry.username\n\t\t\tpassword: parameter.dockerRegistry.password\n\t\t\tauth:
        \    base64.Encode(null, \"\\(parameter.dockerRegistry.username):\\(parameter.dockerRegistry.password)\")\n\t\t}\n\t\tdata:
        \".dockerconfigjson\": json.Marshal(registryData)\n\t}\n\tapply: op.#Apply
        & {\n\t\tvalue: {\n\t\t\tapiVersion: \"v1\"\n\t\t\tkind:       \"Secret\"\n\t\t\tif
        parameter.type == _|_ && parameter.kind == \"docker-registry\" {\n\t\t\t\ttype:
        \"kubernetes.io/dockerconfigjson\"\n\t\t\t}\n\t\t\tif parameter.type != _|_
        {\n\t\t\t\ttype: parameter.type\n\t\t\t}\n\t\t\tmetadata: {\n\t\t\t\tname:
        parameter.secretName\n\t\t\t\tif parameter.namespace != _|_ {\n\t\t\t\t\tnamespace:
        parameter.namespace\n\t\t\t\t}\n\t\t\t\tif parameter.namespace == _|_ {\n\t\t\t\t\tnamespace:
        context.namespace\n\t\t\t\t}\n\t\t\t}\n\t\t\tstringData: data\n\t\t}\n\t\tcluster:
        parameter.cluster\n\t}\n}\nparameter: {\n\t// +usage=Specify the name of the
        secret\n\tsecretName: string\n\t// +usage=Specify the namespace of the secret\n\tnamespace?:
        string\n\t// +usage=Specify the type of the secret\n\ttype?: string\n\t//
        +usage=Specify the data of secret\n\tdata: {}\n\t// +usage=Specify the cluster
        of the secret\n\tcluster: *\"\" | string\n\t// +usage=Specify the kind of
        the secret\n\tkind: *\"generic\" | \"docker-registry\"\n\t// +usage=Specify
        the docker data\n\tdockerRegistry?: {\n\t\t// +usage=Specify the username
        of the docker registry\n\t\tusername: string\n\t\t// +usage=Specify the password
        of the docker registry\n\t\tpassword: string\n\t\t// +usage=Specify the server
        of the docker registry\n\t\tserver: *\"https://index.docker.io/v1/\" | string\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Generate a JDBC connection based on Component
      of alibaba-rds
  name: generate-jdbc-connection
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/base64\"\n)\n\noutput: op.#Read
        & {\n\tvalue: {\n\t\tapiVersion: \"v1\"\n\t\tkind:       \"Secret\"\n\t\tmetadata:
        {\n\t\t\tname: parameter.name\n\t\t\tif parameter.namespace != _|_ {\n\t\t\t\tnamespace:
        parameter.namespace\n\t\t\t}\n\t\t}\n\t}\n}\ndbHost:   op.#ConvertString &
        {bt: base64.Decode(null, output.value.data[\"DB_HOST\"])}\ndbPort:   op.#ConvertString
        & {bt: base64.Decode(null, output.value.data[\"DB_PORT\"])}\ndbName:   op.#ConvertString
        & {bt: base64.Decode(null, output.value.data[\"DB_NAME\"])}\nusername: op.#ConvertString
        & {bt: base64.Decode(null, output.value.data[\"DB_USER\"])}\npassword: op.#ConvertString
        & {bt: base64.Decode(null, output.value.data[\"DB_PASSWORD\"])}\nenv: [\n\t{name:
        \"url\", value:      \"jdbc://\" + dbHost.str + \":\" + dbPort.str + \"/\"
        + dbName.str + \"?characterEncoding=utf8&useSSL=false\"},\n\t{name: \"username\",
        value: username.str},\n\t{name: \"password\", value: password.str},\n]\nparameter:
        {\n\t// +usage=Specify the name of the secret generated by database component\n\tname:
        string\n\t// +usage=Specify the namespace of the secret generated by database
        component\n\tnamespace?: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: List the configs
  name: list-config
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\noutput: op.#ListConfig & {\n\tif parameter.namespace
        != _|_ {\n\t\tnamespace: parameter.namespace\n\t}\n\tif parameter.namespace
        == _|_ {\n\t\tnamespace: context.namespace\n\t}\n\ttemplate: parameter.template\n}\nparameter:
        {\n\t//+usage=Specify the template of the config.\n\ttemplate: string\n\t//+usage=Specify
        the namespace of the config.\n\tnamespace?: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Send notifications to Email, DingTalk, Slack,
      Lark or webhook in your workflow.
  name: notification
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/base64\"\n)\n\nparameter: {\n\t//
        +usage=Please fulfill its url and message if you want to send Lark messages\n\tlark?:
        {\n\t\t// +usage=Specify the the lark url, you can either sepcify it in value
        or use secretRef\n\t\turl: close({\n\t\t\t// +usage=the url address content
        in string\n\t\t\tvalue: string\n\t\t}) | close({\n\t\t\tsecretRef: {\n\t\t\t\t//
        +usage=name is the name of the secret\n\t\t\t\tname: string\n\t\t\t\t// +usage=key
        is the key in the secret\n\t\t\t\tkey: string\n\t\t\t}\n\t\t})\n\t\t// +usage=Specify
        the message that you want to sent, refer to [Lark messaging](https://open.feishu.cn/document/ukTMukTMukTM/ucTM5YjL3ETO24yNxkjN#8b0f2a1b).\n\t\tmessage:
        {\n\t\t\t// +usage=msg_type can be text, post, image, interactive, share_chat,
        share_user, audio, media, file, sticker\n\t\t\tmsg_type: string\n\t\t\t//
        +usage=content should be json encode string\n\t\t\tcontent: string\n\t\t}\n\t}\n\t//
        +usage=Please fulfill its url and message if you want to send DingTalk messages\n\tdingding?:
        {\n\t\t// +usage=Specify the the dingding url, you can either sepcify it in
        value or use secretRef\n\t\turl: close({\n\t\t\t// +usage=the url address
        content in string\n\t\t\tvalue: string\n\t\t}) | close({\n\t\t\tsecretRef:
        {\n\t\t\t\t// +usage=name is the name of the secret\n\t\t\t\tname: string\n\t\t\t\t//
        +usage=key is the key in the secret\n\t\t\t\tkey: string\n\t\t\t}\n\t\t})\n\t\t//
        +usage=Specify the message that you want to sent, refer to [dingtalk messaging](https://developers.dingtalk.com/document/robots/custom-robot-access/title-72m-8ag-pqw)\n\t\tmessage:
        {\n\t\t\t// +usage=Specify the message content of dingtalk notification\n\t\t\ttext?:
        close({\n\t\t\t\tcontent: string\n\t\t\t})\n\t\t\t// +usage=msgType can be
        text, link, mardown, actionCard, feedCard\n\t\t\tmsgtype: *\"text\" | \"link\"
        | \"markdown\" | \"actionCard\" | \"feedCard\"\n\t\t\t#link: {\n\t\t\t\ttext?:
        \      string\n\t\t\t\ttitle?:      string\n\t\t\t\tmessageUrl?: string\n\t\t\t\tpicUrl?:
        \    string\n\t\t\t}\n\n\t\t\tlink?:     #link\n\t\t\tmarkdown?: close({\n\t\t\t\ttext:
        \ string\n\t\t\t\ttitle: string\n\t\t\t})\n\t\t\tat?: close({\n\t\t\t\tatMobiles?:
        [...string]\n\t\t\t\tisAtAll?: bool\n\t\t\t})\n\t\t\tactionCard?: close({\n\t\t\t\ttext:
        \          string\n\t\t\t\ttitle:          string\n\t\t\t\thideAvatar:     string\n\t\t\t\tbtnOrientation:
        string\n\t\t\t\tsingleTitle:    string\n\t\t\t\tsingleURL:      string\n\t\t\t\tbtns?:
        [...close({\n\t\t\t\t\ttitle:     string\n\t\t\t\t\tactionURL: string\n\t\t\t\t})]\n\t\t\t})\n\t\t\tfeedCard?:
        close({\n\t\t\t\tlinks: [...#link]\n\t\t\t})\n\t\t}\n\t}\n\t// +usage=Please
        fulfill its url and message if you want to send Slack messages\n\tslack?:
        {\n\t\t// +usage=Specify the the slack url, you can either sepcify it in value
        or use secretRef\n\t\turl: close({\n\t\t\t// +usage=the url address content
        in string\n\t\t\tvalue: string\n\t\t}) | close({\n\t\t\tsecretRef: {\n\t\t\t\t//
        +usage=name is the name of the secret\n\t\t\t\tname: string\n\t\t\t\t// +usage=key
        is the key in the secret\n\t\t\t\tkey: string\n\t\t\t}\n\t\t})\n\t\t// +usage=Specify
        the message that you want to sent, refer to [slack messaging](https://api.slack.com/reference/messaging/payload)\n\t\tmessage:
        {\n\t\t\t// +usage=Specify the message text for slack notification\n\t\t\ttext:
        string\n\t\t\tblocks?: [...block]\n\t\t\tattachments?: close({\n\t\t\t\tblocks?:
        [...block]\n\t\t\t\tcolor?: string\n\t\t\t})\n\t\t\tthread_ts?: string\n\t\t\t//
        +usage=Specify the message text format in markdown for slack notification\n\t\t\tmrkdwn?:
        *true | bool\n\t\t}\n\t}\n\t// +usage=Please fulfill its from, to and content
        if you want to send email\n\temail?: {\n\t\t// +usage=Specify the email info
        that you want to send from\n\t\tfrom: {\n\t\t\t// +usage=Specify the email
        address that you want to send from\n\t\t\taddress: string\n\t\t\t// +usage=The
        alias is the email alias to show after sending the email\n\t\t\talias?: string\n\t\t\t//
        +usage=Specify the password of the email, you can either sepcify it in value
        or use secretRef\n\t\t\tpassword: close({\n\t\t\t\t// +usage=the password
        content in string\n\t\t\t\tvalue: string\n\t\t\t}) | close({\n\t\t\t\tsecretRef:
        {\n\t\t\t\t\t// +usage=name is the name of the secret\n\t\t\t\t\tname: string\n\t\t\t\t\t//
        +usage=key is the key in the secret\n\t\t\t\t\tkey: string\n\t\t\t\t}\n\t\t\t})\n\t\t\t//
        +usage=Specify the host of your email\n\t\t\thost: string\n\t\t\t// +usage=Specify
        the port of the email host, default to 587\n\t\t\tport: *587 | int\n\t\t}\n\t\t//
        +usage=Specify the email address that you want to send to\n\t\tto: [...string]\n\t\t//
        +usage=Specify the content of the email\n\t\tcontent: {\n\t\t\t// +usage=Specify
        the subject of the email\n\t\t\tsubject: string\n\t\t\t// +usage=Specify the
        context body of the email\n\t\t\tbody: string\n\t\t}\n\t}\n}\nblock: {\n\ttype:
        \     string\n\tblock_id?: string\n\telements?: [...{\n\t\ttype:       string\n\t\taction_id?:
        string\n\t\turl?:       string\n\t\tvalue?:     string\n\t\tstyle?:     string\n\t\ttext?:
        \     textType\n\t\tconfirm?: {\n\t\t\ttitle:   textType\n\t\t\ttext:    textType\n\t\t\tconfirm:
        textType\n\t\t\tdeny:    textType\n\t\t\tstyle?:  string\n\t\t}\n\t\toptions?:
        [...option]\n\t\tinitial_options?: [...option]\n\t\tplaceholder?:  textType\n\t\tinitial_date?:
        string\n\t\timage_url?:    string\n\t\talt_text?:     string\n\t\toption_groups?:
        [...option]\n\t\tmax_selected_items?: int\n\t\tinitial_value?:      string\n\t\tmultiline?:
        \         bool\n\t\tmin_length?:         int\n\t\tmax_length?:         int\n\t\tdispatch_action_config?:
        trigger_actions_on?: [...string]\n\t\tinitial_time?: string\n\t}]\n}\ntextType:
        {\n\ttype:      string\n\ttext:      string\n\temoji?:    bool\n\tverbatim?:
        bool\n}\noption: {\n\ttext:         textType\n\tvalue:        string\n\tdescription?:
        textType\n\turl?:         string\n}\n// send webhook notification\nding: op.#Steps
        & {\n\tif parameter.dingding != _|_ {\n\t\tif parameter.dingding.url.value
        != _|_ {\n\t\t\tding1: op.#DingTalk & {\n\t\t\t\tmessage: parameter.dingding.message\n\t\t\t\tdingUrl:
        parameter.dingding.url.value\n\t\t\t}\n\t\t}\n\t\tif parameter.dingding.url.secretRef
        != _|_ && parameter.dingding.url.value == _|_ {\n\t\t\tread: op.#Read & {\n\t\t\t\tvalue:
        {\n\t\t\t\t\tapiVersion: \"v1\"\n\t\t\t\t\tkind:       \"Secret\"\n\t\t\t\t\tmetadata:
        {\n\t\t\t\t\t\tname:      parameter.dingding.url.secretRef.name\n\t\t\t\t\t\tnamespace:
        context.namespace\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstringValue: op.#ConvertString
        & {bt: base64.Decode(null, read.value.data[parameter.dingding.url.secretRef.key])}\n\t\t\tding2:
        \      op.#DingTalk & {\n\t\t\t\tmessage: parameter.dingding.message\n\t\t\t\tdingUrl:
        stringValue.str\n\t\t\t}\n\t\t}\n\t}\n}\nlark: op.#Steps & {\n\tif parameter.lark
        != _|_ {\n\t\tif parameter.lark.url.value != _|_ {\n\t\t\tlark1: op.#Lark
        & {\n\t\t\t\tmessage: parameter.lark.message\n\t\t\t\tlarkUrl: parameter.lark.url.value\n\t\t\t}\n\t\t}\n\t\tif
        parameter.lark.url.secretRef != _|_ && parameter.lark.url.value == _|_ {\n\t\t\tread:
        op.#Read & {\n\t\t\t\tvalue: {\n\t\t\t\t\tapiVersion: \"v1\"\n\t\t\t\t\tkind:
        \      \"Secret\"\n\t\t\t\t\tmetadata: {\n\t\t\t\t\t\tname:      parameter.lark.url.secretRef.name\n\t\t\t\t\t\tnamespace:
        context.namespace\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstringValue: op.#ConvertString
        & {bt: base64.Decode(null, read.value.data[parameter.lark.url.secretRef.key])}\n\t\t\tlark2:
        \      op.#Lark & {\n\t\t\t\tmessage: parameter.lark.message\n\t\t\t\tlarkUrl:
        stringValue.str\n\t\t\t}\n\t\t}\n\t}\n}\nslack: op.#Steps & {\n\tif parameter.slack
        != _|_ {\n\t\tif parameter.slack.url.value != _|_ {\n\t\t\tslack1: op.#Slack
        & {\n\t\t\t\tmessage:  parameter.slack.message\n\t\t\t\tslackUrl: parameter.slack.url.value\n\t\t\t}\n\t\t}\n\t\tif
        parameter.slack.url.secretRef != _|_ && parameter.slack.url.value == _|_ {\n\t\t\tread:
        op.#Read & {\n\t\t\t\tvalue: {\n\t\t\t\t\tkind:       \"Secret\"\n\t\t\t\t\tapiVersion:
        \"v1\"\n\t\t\t\t\tmetadata: {\n\t\t\t\t\t\tname:      parameter.slack.url.secretRef.name\n\t\t\t\t\t\tnamespace:
        context.namespace\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstringValue: op.#ConvertString
        & {bt: base64.Decode(null, read.value.data[parameter.slack.url.secretRef.key])}\n\t\t\tslack2:
        \     op.#Slack & {\n\t\t\t\tmessage:  parameter.slack.message\n\t\t\t\tslackUrl:
        stringValue.str\n\t\t\t}\n\t\t}\n\t}\n}\nemail: op.#Steps & {\n\tif parameter.email
        != _|_ {\n\t\tif parameter.email.from.password.value != _|_ {\n\t\t\temail1:
        op.#SendEmail & {\n\t\t\t\tfrom: {\n\t\t\t\t\taddress: parameter.email.from.address\n\t\t\t\t\tif
        parameter.email.from.alias != _|_ {\n\t\t\t\t\t\talias: parameter.email.from.alias\n\t\t\t\t\t}\n\t\t\t\t\tpassword:
        parameter.email.from.password.value\n\t\t\t\t\thost:     parameter.email.from.host\n\t\t\t\t\tport:
        \    parameter.email.from.port\n\t\t\t\t}\n\t\t\t\tto:      parameter.email.to\n\t\t\t\tcontent:
        parameter.email.content\n\t\t\t}\n\t\t}\n\n\t\tif parameter.email.from.password.secretRef
        != _|_ && parameter.email.from.password.value == _|_ {\n\t\t\tread: op.#Read
        & {\n\t\t\t\tvalue: {\n\t\t\t\t\tkind:       \"Secret\"\n\t\t\t\t\tapiVersion:
        \"v1\"\n\t\t\t\t\tmetadata: {\n\t\t\t\t\t\tname:      parameter.email.from.password.secretRef.name\n\t\t\t\t\t\tnamespace:
        context.namespace\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tstringValue: op.#ConvertString
        & {bt: base64.Decode(null, read.value.data[parameter.email.from.password.secretRef.key])}\n\t\t\temail2:
        \     op.#SendEmail & {\n\t\t\t\tfrom: {\n\t\t\t\t\taddress: parameter.email.from.address\n\t\t\t\t\tif
        parameter.email.from.alias != _|_ {\n\t\t\t\t\t\talias: parameter.email.from.alias\n\t\t\t\t\t}\n\t\t\t\t\tpassword:
        stringValue.str\n\t\t\t\t\thost:     parameter.email.from.host\n\t\t\t\t\tport:
        \    parameter.email.from.port\n\t\t\t\t}\n\t\t\t\tto:      parameter.email.to\n\t\t\t\tcontent:
        parameter.email.content\n\t\t\t}\n\t\t}\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: print message in workflow step status
  name: print-message-in-status
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\nparameter: message: string\nmsg: op.#Message
        & {\n\tmessage: parameter.message\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Read a config
  name: read-config
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\noutput: op.#ReadConfig & {\n\tname:
        parameter.name\n\tif parameter.namespace != _|_ {\n\t\tnamespace: parameter.namespace\n\t}\n\tif
        parameter.namespace == _|_ {\n\t\tnamespace: context.namespace\n\t}\n}\nparameter:
        {\n\t//+usage=Specify the name of the config.\n\tname: string\n\n\t//+usage=Specify
        the namespace of the config.\n\tnamespace?: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Read Kubernetes objects from cluster for your
      workflow steps
  name: read-object
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\noutput: {\n\tif parameter.apiVersion
        == _|_ && parameter.kind == _|_ {\n\t\top.#Read & {\n\t\t\tvalue: {\n\t\t\t\tapiVersion:
        \"core.oam.dev/v1beta1\"\n\t\t\t\tkind:       \"Application\"\n\t\t\t\tmetadata:
        {\n\t\t\t\t\tname: parameter.name\n\t\t\t\t\tif parameter.namespace != _|_
        {\n\t\t\t\t\t\tnamespace: parameter.namespace\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcluster:
        parameter.cluster\n\t\t}\n\t}\n\tif parameter.apiVersion != _|_ || parameter.kind
        != _|_ {\n\t\top.#Read & {\n\t\t\tvalue: {\n\t\t\t\tapiVersion: parameter.apiVersion\n\t\t\t\tkind:
        \      parameter.kind\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: parameter.name\n\t\t\t\t\tif
        parameter.namespace != _|_ {\n\t\t\t\t\t\tnamespace: parameter.namespace\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tcluster:
        parameter.cluster\n\t\t}\n\t}\n}\nparameter: {\n\t// +usage=Specify the apiVersion
        of the object, defaults to 'core.oam.dev/v1beta1'\n\tapiVersion?: string\n\t//
        +usage=Specify the kind of the object, defaults to Application\n\tkind?: string\n\t//
        +usage=Specify the name of the object\n\tname: string\n\t// +usage=The namespace
        of the resource you want to read\n\tnamespace?: *\"default\" | string\n\t//
        +usage=The cluster you want to apply the resource to, default is the current
        control plane cluster\n\tcluster: *\"\" | string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/alias: ""
    definition.oam.dev/description: Send request to the url
    definition.oam.dev/example-url: https://raw.githubusercontent.com/kubevela/workflow/main/examples/workflow-run/request.yaml
  name: request
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/json\"\n)\n\nhttp: op.#HTTPDo
        & {\n\tmethod: parameter.method\n\turl:    parameter.url\n\trequest: {\n\t\tif
        parameter.body != _|_ {\n\t\t\tbody: json.Marshal(parameter.body)\n\t\t}\n\t\tif
        parameter.header != _|_ {\n\t\t\theader: parameter.header\n\t\t}\n\t}\n}\nfail:
        op.#Steps & {\n\tif http.response.statusCode > 400 {\n\t\trequestFail: op.#Fail
        & {\n\t\t\tmessage: \"request of \\(parameter.url) is fail: \\(http.response.statusCode)\"\n\t\t}\n\t}\n}\nresponse:
        json.Unmarshal(http.response.body)\nparameter: {\n\turl:    string\n\tmethod:
        *\"GET\" | \"POST\" | \"PUT\" | \"DELETE\"\n\tbody?: {...}\n\theader?: [string]:
        string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Sync secrets created by terraform component to
      runtime clusters so that runtime clusters can share the created cloud resource.
  labels:
    custom.definition.oam.dev/scope: Application
  name: share-cloud-resource
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\napp: op.#ShareCloudResource & {\n\tenv:
        \       parameter.env\n\tpolicy:     parameter.policy\n\tplacements: parameter.placements\n\t//
        context.namespace indicates the namespace of the app\n\tnamespace: context.namespace\n\t//
        context.namespace indicates the name of the app\n\tname: context.name\n}\nparameter:
        {\n\t// +usage=Declare the location to bind\n\tplacements: [...{\n\t\tnamespace?:
        string\n\t\tcluster?:   string\n\t}]\n\t// +usage=Declare the name of the
        env-binding policy, if empty, the first env-binding policy will be used\n\tpolicy:
        *\"\" | string\n\t// +usage=Declare the name of the env in policy\n\tenv:
        string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: A special step that you can declare 'subSteps'
      in it, 'subSteps' is an array containing any step type whose valid parameters
      do not include the `step-group` step type itself. The sub steps were executed
      in parallel.
  name: step-group
  namespace: vela-system
spec:
  schematic:
    cue:
      template: |
        // no parameters, the nop only to make the template not empty or it's invalid
        nop: {}
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Suspend the current workflow, it can be resumed
      by 'vela workflow resume' command.
  name: suspend
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "parameter: {\n\t// +usage=Specify the wait duration time to resume
        workflow such as \"30s\", \"1min\" or \"2m15s\"\n\tduration?: string\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Run a vela command
    definition.oam.dev/example-url: https://raw.githubusercontent.com/kubevela/workflow/main/examples/workflow-run/apply-terraform-resource.yaml
  name: vela-cli
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n)\n\nmountsArray: [\n\tif parameter.storage
        != _|_ && parameter.storage.secret != _|_ for v in parameter.storage.secret
        {\n\t\t{\n\t\t\tname:      \"secret-\" + v.name\n\t\t\tmountPath: v.mountPath\n\t\t\tif
        v.subPath != _|_ {\n\t\t\t\tsubPath: v.subPath\n\t\t\t}\n\t\t}\n\t},\n\tif
        parameter.storage != _|_ && parameter.storage.hostPath != _|_ for v in parameter.storage.hostPath
        {\n\t\t{\n\t\t\tname:      \"hostpath-\" + v.name\n\t\t\tmountPath: v.mountPath\n\t\t}\n\t},\n]\nvolumesList:
        [\n\tif parameter.storage != _|_ && parameter.storage.secret != _|_ for v
        in parameter.storage.secret {\n\t\t{\n\t\t\tname: \"secret-\" + v.name\n\t\t\tsecret:
        {\n\t\t\t\tdefaultMode: v.defaultMode\n\t\t\t\tsecretName:  v.secretName\n\t\t\t\tif
        v.items != _|_ {\n\t\t\t\t\titems: v.items\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif
        parameter.storage != _|_ && parameter.storage.hostPath != _|_ for v in parameter.storage.hostPath
        {\n\t\t\t{\n\t\t\t\tname: \"hostpath-\" + v.name\n\t\t\t\tpath: v.path\n\t\t\t}\n\t\t}\n\t},\n]\ndeDupVolumesArray:
        [\n\tfor val in [\n\t\tfor i, vi in volumesList {\n\t\t\tfor j, vj in volumesList
        if j < i && vi.name == vj.name {\n\t\t\t\t_ignore: true\n\t\t\t}\n\t\t\tvi\n\t\t},\n\t]
        if val._ignore == _|_ {\n\t\tval\n\t},\n]\njob: op.#Apply & {\n\tvalue: {\n\t\tapiVersion:
        \"batch/v1\"\n\t\tkind:       \"Job\"\n\t\tmetadata: {\n\t\t\tname: \"\\(context.name)-\\(context.stepName)-\\(context.stepSessionID)\"\n\t\t\tif
        parameter.serviceAccountName == \"kubevela-vela-core\" {\n\t\t\t\tnamespace:
        \"vela-system\"\n\t\t\t}\n\t\t\tif parameter.serviceAccountName != \"kubevela-vela-core\"
        {\n\t\t\t\tnamespace: context.namespace\n\t\t\t}\n\t\t}\n\t\tspec: {\n\t\t\tbackoffLimit:
        3\n\t\t\ttemplate: {\n\t\t\t\tmetadata: labels: \"workflow.oam.dev/step-name\":
        \"\\(context.name)-\\(context.stepName)\"\n\t\t\t\tspec: {\n\t\t\t\t\tcontainers:
        [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tname:         \"\\(context.name)-\\(context.stepName)-\\(context.stepSessionID)-job\"\n\t\t\t\t\t\t\timage:
        \       parameter.image\n\t\t\t\t\t\t\tcommand:      parameter.command\n\t\t\t\t\t\t\tvolumeMounts:
        mountsArray\n\t\t\t\t\t\t},\n\t\t\t\t\t]\n\t\t\t\t\trestartPolicy:  \"Never\"\n\t\t\t\t\tserviceAccount:
        parameter.serviceAccountName\n\t\t\t\t\tvolumes:        deDupVolumesArray\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nlog:
        op.#Log & {\n\tsource: resources: [{labelSelector: \"workflow.oam.dev/step-name\":
        \"\\(context.name)-\\(context.stepName)\"}]\n}\nfail: op.#Steps & {\n\tif
        job.value.status.failed != _|_ {\n\t\tif job.value.status.failed > 2 {\n\t\t\tbreakWorkflow:
        op.#Fail & {\n\t\t\t\tmessage: \"failed to execute vela command\"\n\t\t\t}\n\t\t}\n\t}\n}\nwait:
        op.#ConditionalWait & {\n\tcontinue: job.value.status.succeeded != _|_ &&
        job.value.status.succeeded > 0\n}\nparameter: {\n\t// +usage=Specify the name
        of the addon.\n\taddonName: string\n\t// +usage=Specify the vela command\n\tcommand:
        [...string]\n\t// +usage=Specify the image\n\timage: *\"oamdev/vela-cli:v1.6.4\"
        | string\n\t// +usage=specify serviceAccountName want to use\n\tserviceAccountName:
        *\"kubevela-vela-core\" | string\n\tstorage?: {\n\t\t// +usage=Mount Secret
        type storage\n\t\tsecret?: [...{\n\t\t\tname:        string\n\t\t\tmountPath:
        \  string\n\t\t\tsubPath?:    string\n\t\t\tdefaultMode: *420 | int\n\t\t\tsecretName:
        \ string\n\t\t\titems?: [...{\n\t\t\t\tkey:  string\n\t\t\t\tpath: string\n\t\t\t\tmode:
        *511 | int\n\t\t\t}]\n\t\t}]\n\t\t// +usage=Declare host path type storage\n\t\thostPath?:
        [...{\n\t\t\tname:      string\n\t\t\tpath:      string\n\t\t\tmountPath:
        string\n\t\t\ttype:      *\"Directory\" | \"DirectoryOrCreate\" | \"FileOrCreate\"
        | \"File\" | \"Socket\" | \"CharDevice\" | \"BlockDevice\"\n\t\t}]\n\t}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkflowStepDefinition
metadata:
  annotations:
    definition.oam.dev/description: Send a request to the specified Webhook URL. If
      no request body is specified, the current Application body will be sent by default.
  name: webhook
  namespace: vela-system
spec:
  schematic:
    cue:
      template: "import (\n\t\"vela/op\"\n\t\"encoding/json\"\n\t\"encoding/base64\"\n)\n\ndata:
        op.#Steps & {\n\tif parameter.data == _|_ {\n\t\tread: op.#Read & {\n\t\t\tvalue:
        {\n\t\t\t\tapiVersion: \"core.oam.dev/v1beta1\"\n\t\t\t\tkind:       \"Application\"\n\t\t\t\tmetadata:
        {\n\t\t\t\t\tname:      context.name\n\t\t\t\t\tnamespace: context.namespace\n\t\t\t\t}\n\t\t\t}\n\t\t}
        \     @step(1)\n\t\tvalue: json.Marshal(read.value) @step(2)\n\t}\n\tif parameter.data
        != _|_ {\n\t\tvalue: json.Marshal(parameter.data) @step(3)\n\t}\n}\nwebhook:
        op.#Steps & {\n\tif parameter.url.value != _|_ {\n\t\thttp: op.#HTTPPost &
        {\n\t\t\turl: parameter.url.value\n\t\t\trequest: {\n\t\t\t\tbody: data.value\n\t\t\t\theader:
        \"Content-Type\": \"application/json\"\n\t\t\t}\n\t\t} @step(4)\n\t}\n\tif
        parameter.url.secretRef != _|_ && parameter.url.value == _|_ {\n\t\tread:
        op.#Read & {\n\t\t\tvalue: {\n\t\t\t\tapiVersion: \"v1\"\n\t\t\t\tkind:       \"Secret\"\n\t\t\t\tmetadata:
        {\n\t\t\t\t\tname:      parameter.url.secretRef.name\n\t\t\t\t\tnamespace:
        context.namespace\n\t\t\t\t}\n\t\t\t}\n\t\t} @step(5)\n\n\t\tstringValue:
        op.#ConvertString & {bt: base64.Decode(null, read.value.data[parameter.url.secretRef.key])}
        @step(6)\n\t\thttp:        op.#HTTPPost & {\n\t\t\turl: stringValue.str\n\t\t\trequest:
        {\n\t\t\t\tbody: data.value\n\t\t\t\theader: \"Content-Type\": \"application/json\"\n\t\t\t}\n\t\t}
        @step(7)\n\t}\n}\nparameter: {\n\t// +usage=Specify the webhook url\n\turl:
        close({\n\t\tvalue: string\n\t}) | close({\n\t\tsecretRef: {\n\t\t\t// +usage=name
        is the name of the secret\n\t\t\tname: string\n\t\t\t// +usage=key is the
        key in the secret\n\t\t\tkey: string\n\t\t}\n\t})\n\t// +usage=Specify the
        data you want to send\n\tdata?: {...}\n}\n"
---
apiVersion: core.oam.dev/v1beta1
kind: WorkloadDefinition
metadata:
  annotations:
    definition.oam.dev/description: autodetects.core.oam.dev is the default workload
      type of ComponentDefinition
  name: autodetects.core.oam.dev
  namespace: vela-system
spec:
  definitionRef:
    name: autodetects.core.oam.dev
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission
  namespace: vela-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-cluster-gateway-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-cluster-gateway-admission
  namespace: vela-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  - mutatingwebhookconfigurations
  verbs:
  - get
  - update
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubevela-vela-core-admission
subjects:
- kind: ServiceAccount
  name: kubevela-vela-core-admission
  namespace: vela-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission
  namespace: vela-system
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-cluster-gateway-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-cluster-gateway-admission
  namespace: vela-system
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission
  namespace: vela-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubevela-vela-core-admission
subjects:
- kind: ServiceAccount
  name: kubevela-vela-core-admission
  namespace: vela-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-cluster-gateway-admission
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-cluster-gateway-admission
  namespace: vela-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubevela-vela-core-cluster-gateway-admission
subjects:
- kind: ServiceAccount
  name: kubevela-vela-core-cluster-gateway-admission
  namespace: vela-system
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: hook-succeeded
  name: kubevela-application-test
  namespace: vela-system
spec:
  containers:
  - command:
    - /bin/bash
    - -ec
    - |2

      set -e

      echo "Waiting application is ready..."

      echo "waiting for application being Ready"
      kubectl -n vela-system wait --for=condition=Ready applications.core.oam.dev helm-test-vela-app --timeout=3m
      echo "application is Ready"

      # wait for deploy being created
      echo "waiting for deployment being available"
      kubectl -n vela-system wait --for=condition=available deployments helm-test-express-server --timeout 3m
      echo "deployment being available"

      echo "Application and its components are created"
    image: oamdev/alpine-k8s:1.18.2
    imagePullPolicy: IfNotPresent
    name: kubevela-application-test
  restartPolicy: Never
  serviceAccountName: kubevela-vela-core
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission-create
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission-create
  namespace: vela-system
spec:
  template:
    metadata:
      labels:
        app: vela-core-admission-create
        app.kubernetes.io/instance: kubevela
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/version: 1.7.4
        helm.sh/chart: vela-core-1.7.4
      name: kubevela-vela-core-admission-create
    spec:
      containers:
      - args:
        - create
        - --host=vela-core-webhook,vela-core-webhook.vela-system.svc
        - --namespace=vela-system
        - --secret-name=kubevela-vela-core-admission
        - --key-name=tls.key
        - --cert-name=tls.crt
        image: oamdev/kube-webhook-certgen:v2.4.1
        imagePullPolicy: IfNotPresent
        name: create
      restartPolicy: OnFailure
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: kubevela-vela-core-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: vela-core-admission-patch
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-admission-patch
  namespace: vela-system
spec:
  template:
    metadata:
      labels:
        app: vela-core-admission-patch
        app.kubernetes.io/instance: kubevela
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/version: 1.7.4
        helm.sh/chart: vela-core-1.7.4
      name: kubevela-vela-core-admission-patch
    spec:
      containers:
      - args:
        - patch
        - --webhook-name=kubevela-vela-core-admission
        - --namespace=vela-system
        - --secret-name=kubevela-vela-core-admission
        - --patch-failure-policy=Fail
        image: oamdev/kube-webhook-certgen:v2.4.1
        imagePullPolicy: IfNotPresent
        name: patch
      restartPolicy: OnFailure
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: kubevela-vela-core-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: kubevela-vela-core-cluster-gateway-tls-secret-create
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-cluster-gateway-tls-secret-create
  namespace: vela-system
spec:
  template:
    metadata:
      labels:
        app: kubevela-vela-core-cluster-gateway-tls-secret-create
        app.kubernetes.io/instance: kubevela
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/version: 1.7.4
        helm.sh/chart: vela-core-1.7.4
      name: kubevela-vela-core-cluster-gateway-tls-secret-create
    spec:
      containers:
      - args:
        - create
        - --host=kubevela-cluster-gateway-service,kubevela-cluster-gateway-service.vela-system.svc
        - --namespace=vela-system
        - --secret-name=kubevela-vela-core-cluster-gateway-tls-v2
        - --cert-name=tls.crt
        - --key-name=tls.key
        image: oamdev/kube-webhook-certgen:v2.4.1
        imagePullPolicy: IfNotPresent
        name: create
      restartPolicy: OnFailure
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: kubevela-vela-core-cluster-gateway-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: kubevela-vela-core-cluster-gateway-tls-secret-patch
    app.kubernetes.io/instance: kubevela
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: vela-core
    app.kubernetes.io/version: 1.7.4
    helm.sh/chart: vela-core-1.7.4
  name: kubevela-vela-core-cluster-gateway-tls-secret-patch
  namespace: vela-system
spec:
  template:
    metadata:
      labels:
        app: kubevela-vela-core-cluster-gateway-tls-secret-patch
        app.kubernetes.io/instance: kubevela
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: vela-core
        app.kubernetes.io/version: 1.7.4
        helm.sh/chart: vela-core-1.7.4
      name: kubevela-vela-core-cluster-gateway-tls-secret-patch
    spec:
      containers:
      - args:
        - --secret-namespace=vela-system
        - --secret-name=kubevela-vela-core-cluster-gateway-tls-v2
        command:
        - /patch
        image: oamdev/cluster-gateway:v1.7.0
        imagePullPolicy: IfNotPresent
        name: patch
      restartPolicy: OnFailure
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: kubevela-vela-core
---
apiVersion: core.oam.dev/v1beta1
kind: Application
metadata:
  annotations:
    helm.sh/hook: test-success
    helm.sh/hook-delete-policy: hook-succeeded
  name: helm-test-vela-app
  namespace: vela-system
spec:
  components:
  - name: helm-test-express-server
    properties:
      image: oamdev/hello-world:v1
      port: 8000
    type: webservice
